[{"path":"index.html","id":"welcome-to-the-ns5108-quantiatitve-methods-and-analysis-handbook","chapter":"1 Welcome to the NS5108 quantiatitve methods and analysis handbook","heading":"1 Welcome to the NS5108 quantiatitve methods and analysis handbook","text":"year tried make half NS5108 structured accessible possible. Part plan place necessary content together one place: book. find explanation concepts cover semester, depth explanations concepts won’t time cover class, relevant video clips lectures, demonstrations perform various statistical analyses.guide designed work alongside NS5108 lectures, replace entirely. Personally, find topic need several explanations worded slightly differently concept starts click. Please use book way consolidate class learning.Now, get started. Yes, going maths, yes going formulas, sorry can’t teach statistics without aspects. However, different depths can learn statistics. handbook give basics, also allows delve little deeper theory behind various statistical analysis work.tried make terrifying statistical maths hidden away behind blue boxes (like one ) can click reveal information. blue boxes also contain extra explanation around graphs, tangents didn’t want get bogged main text.feeling anxious statistics, know alone; many people feel way.Think learning statistics like learning drive ride bike. Initially, full painful stops bumps, lessons, things eventually start make sense. struggled lot first encountered material, realised incredibly powerful can applied topics care , motivation skyrocketed, got hump now ’m hooked life. Just remember learning marathon, perfectly fine move pace. goal make journey smooth possible clear explanations practical exercises.find stuck, hesitate ask help. graduate teaching assistants willing explain concepts repeatedly various ways; fact, welcome —keeps us toes! Asking questions integral part learning process, support every step way. , take deep breath, patient kind . skills acquire serve well academic journey hopefully lead meaningful impact future work.","code":""},{"path":"index.html","id":"exercises-and-statistical-software","chapter":"1 Welcome to the NS5108 quantiatitve methods and analysis handbook","heading":"1.1 Exercises and statistical software","text":"Throughout workbook exercises suggest download data conduct analysis use JASP. JASP simple open source (.e. free) program can used run statistical tests. link website can download JASP.Link download JASPData files can found weekly materials NS5108 Moodle page.","code":""},{"path":"index.html","id":"note-for-msc-health-psychology-and-msc-forensic-psychology-students","chapter":"1 Welcome to the NS5108 quantiatitve methods and analysis handbook","heading":"1.2 Note for MSc Health Psychology and MSc Forensic Psychology students","text":"speed running module part NS7004 NS7151 research methods modules may want conduct analysis using SPSS R. absolutely can , theoretical content still apply, might just need search around additional click guides. Feel free email (rclarke8@glos.ac.uk) want point direction good online content help aspect.","code":""},{"path":"index.html","id":"this-book-is-a-work-in-progress","chapter":"1 Welcome to the NS5108 quantiatitve methods and analysis handbook","heading":"1.3 This book is a work in progress","text":"semester, \"gromiting\" way course.plan, content, building parts book semester. , come book early expect watch video walkthrough guides techniques later course, apologies, maybe check back weeks.Also, spot glaring errors please contact rclarke8@glos.ac.uk correct go.","code":""},{"path":"introduction-to-difference-tests.html","id":"introduction-to-difference-tests","chapter":"2 Introduction to difference tests","heading":"2 Introduction to difference tests","text":"","code":""},{"path":"introduction-to-difference-tests.html","id":"why-do-we-compare-groups","chapter":"2 Introduction to difference tests","heading":"2.1 Why Do We Compare Groups?","text":"One goals quantitative psychological research quest understand differences: differences individuals, groups, conditions, time points. pursuit rooted scientific method, relies empirical evidence support refute hypotheses. compare groups study, essentially asking whether differences observe meaningful merely result random variation.underlying philosophy systematically comparing groups, can uncover patterns, relationships, causal effects contribute understanding human behaviour, cognition, emotions. comparisons help us test theories, evaluate interventions, make predictions future outcomes. essence, difference tests tools allow us determine whether observed variations significant worth consideration, can attributed chance.help clarify can compare groups different research scenarios, explore various statistical tests available, suited specific types comparisons:","code":""},{"path":"introduction-to-difference-tests.html","id":"comparing-two-groups-the-t-test","chapter":"2 Introduction to difference tests","heading":"2.1.1 Comparing Two Groups: The T-Test","text":"t-test one commonly used statistical methods comparing means two groups (.e. independent variable two levels). helps determine whether significant difference two groups, considering variability within group.Independent Samples T-Test: Used comparing two independent groups (e.g., treatment vs. control).Paired Samples T-Test: Used comparing two related groups (e.g., pre-test vs. post-test scores individuals).","code":""},{"path":"introduction-to-difference-tests.html","id":"comparing-three-or-more-groups-one-way-anova","chapter":"2 Introduction to difference tests","heading":"2.1.2 Comparing Three or More Groups: One-Way ANOVA","text":"two groups compare (.e., independent variable two levels), one-way Analysis Variance (ANOVA) used determine whether statistically significant differences means three independent groups. example, comparing effectiveness three different therapies anxiety levels, one-way ANOVA appropriate test. use ANOVA can avoid performing multiple t-tests, increases risk Type errors (see chapter power). ANOVA provides single test controls risk, offering reliable way assess differences across multiple groups.","code":""},{"path":"introduction-to-difference-tests.html","id":"comparing-multiple-factors-factorial-anova","chapter":"2 Introduction to difference tests","heading":"2.1.3 Comparing Multiple Factors: Factorial ANOVA","text":"Factorial ANOVA extends one-way ANOVA allowing additional independent variables. type ANOVA examines differences within independent variables, also investigates possible interactions independent variables.example, examine effects therapy type (CBT vs. psychotherapy) session frequency (weekly vs. biweekly) anxiety reduction. performing factorial ANOVA can determine main effect therapy type, main effect frequency, effectiveness therapy type differs based frequency (vice versa).","code":""},{"path":"introduction-to-difference-tests.html","id":"controlling-for-covariates-ancova","chapter":"2 Introduction to difference tests","heading":"2.1.4 Controlling for Covariates: ANCOVA","text":"Analysis Covariance (ANCOVA) combines ANOVA regression. allows comparison group means controlling influence one continuous covariates. helps isolating effect independent variable accounting variance explained covariates.example, Comparing effectiveness different teaching methods exam performance controlling prior knowledge (measured pre-test score).","code":""},{"path":"introduction-to-difference-tests.html","id":"extending-to-multiple-dependent-variables-manova","chapter":"2 Introduction to difference tests","heading":"2.1.5 Extending to Multiple Dependent Variables: MANOVA","text":"Multivariate Analysis Variance (MANOVA) used multiple dependent variables, tests whether mean differences among groups combination dependent variables significant.example, studying effect training program job satisfaction productivity simultaneously.","code":""},{"path":"introduction-to-difference-tests.html","id":"choosing-the-appropriate-test","chapter":"2 Introduction to difference tests","heading":"2.2 Choosing the appropriate test","text":"module, focus ANOVA factorial ANOVA, also valuable aware ANCOVA MANOVA, especially consider using techniques dissertation. tests parametric, meaning rely certain statistical assumptions. using , essential conduct checks ensure assumptions met. assumptions violated, non-parametric alternatives used. cover parametric tests module, non-parametric alternatives listed reference case need explore dissertation.","code":""},{"path":"t-tests-theory-visualisation-and-calculation-by-hand.html","id":"t-tests-theory-visualisation-and-calculation-by-hand","chapter":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","heading":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","text":"mentioned previous chapter, t-tests help us answer key questions whether new treatment effective existing one two groups differ significantly psychological measure. theory behind t-tests rooted hypothesis testing, specifically Null Hypothesis Significance Testing (NHST), helps us determine whether observed differences data likely due chance reflect true differences population.comparing groups, goal determine whether differences observe statistically significant—, whether unlikely occurred chance. T-tests allow us test hypotheses differences group means, providing way infer sample data larger population. essence, conduct t-test, assessing whether difference means two groups large enough considered meaningful given variability data.two main types t-tests commonly used research, suited different study designs:Independent Samples (-subjects) T-Test:test compares means two independent groups determine statistically significant difference . used groups distinct related.example, researcher compare average reaction times two different groups participants, one receiving caffeine receiving water.Paired Samples (within-subjects / repeated measures) T-Test:test compares means two related groups, participants tested two different conditions (e.g., intervention). paired samples t-test often used subjects measured , pairs subjects matched way.experiment independent samples design redesigned paired samples design. Meaning participant takes part condition experiment.designing experiment, likely choosing independent measures design (-subjects) repeated measures design (within-subjects) IVs. choice distinct advantages disadvantages can influence study's feasibility data quality. See blue box detail advantages disadvantages.independent measures design (-subjects), participant assigned one condition, avoids risk order effects practice, fatigue, carryover effects can confound results. design simpler analyse since participant provides one data point, making setup straightforward. Additionally, participants exposed one condition, less risk participant fatigue boredom affecting data. However, independent measures design typically requires larger sample size, increases cost time needed recruitment testing. also greater -group variability since different participants group, can introduce differences unrelated independent variable may complicate interpretation results.repeated measures design (within-subjects), participants used across conditions, significantly reduces number participants needed , therefore, overall cost time required study. design also reduces variability due individual differences, participant serves control, leading increased statistical power. However, repeated measures designs prone order effects, can confound results. mitigate issues, researchers often need use counterbalancing, adds complexity design analysis. Additionally, participants may become fatigued lose focus completing multiple conditions, can affect performance potentially skew results.following video extract one lectures explain design aspects t-tests:","code":""},{"path":"t-tests-theory-visualisation-and-calculation-by-hand.html","id":"what-does-statistical-difference-look-like","chapter":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","heading":"3.1 What does statistical difference look like?","text":"diving mechanics t-tests, important understand statistical difference looks like. Even two groups different mean values, difference might meaningful unless exceeds expected chance. inferential statistics come , allowing us determine whether difference observed sample data reflects true difference population.example reaction time (milliseconds) data N=8 independent samples design IV=coffee/water DV=reaction time study .data can say , average, reaction time coffee condition 7.25ms faster participants water condition.Well, plot data see looks. data line graph:points represent mean reaction time condition, vertical lines indicating 95% confidence intervals (CIs) around means. CIs provide range true mean RT likely fall 95% confidence, based mean variability within data. ’s great concept use help determine significance within data. case, overlap CIs suggests difference mean RT Caffeine Water conditions might small potentially statistically significant, although formal statistical test needed confirm ., , just looking data way, ’m overly convinced. Especially due sample size N=8 (four condition). aspect likely mean study particularly low statistical power, concept cover depth later handbook.Say instead, run study N=100 participants.Another way visualise data looking distribution rather just mean 95% CI. following histograms based running experiment 100 times, 50 participants condition.histograms shown , bar represents \"bin\" covers range reaction times (RT) within 10-unit interval. height bar corresponds number data points (\"count\") fall within range. example, histogram Caffeine condition, bar 200 x-axis indicates eight participants reaction times 200 210 milliseconds. water condition, participants reaction time 170 180 milliseconds.red dashed line histogram marks mean reaction time condition. looking distribution bars around line, can get sense data spread (previously 95% CI).example can see difference mean RT . caffeine condition participants , average, 15.1ms faster participants water conditions.add smooth line histograms overlap , difference becomes apparent. bars removed type data visualisation known density plot.density plot can see distribution data caffeine condition shifted left water condition. won’t see type visualisation papers limits comparison two conditions (gets confusing three overlapping colours). However, might see combination box plot. choose best visually represent data:box plot summarises distribution, central tendency, variability data. box represents interquartile range (IQR), contains middle 50% data, line inside box marking median (note: mean). whiskers extend box smallest largest values considered outliers, providing clear view spread data. Outliers, values fall outside 1.5 times IQR quartiles, displayed individual points beyond whiskers (outliers within data might find , see assumption checks topic).visualisation, box plot combined violin plot, essentially mirrored density plot shows distribution data. wider sections violin indicate areas data points concentrated, narrower sections represent less frequent values. additional layer helps understand just central values lie, also data distributed across range reaction times.Finally, individual data points plotted using technique called jittering, spreads points horizontally avoid overlap, making easier see distribution values. ’s important note jittering alter actual values data points; merely adjusts horizontal position slightly clarity. Together, box plot, violin plot, jittered points provide comprehensive view differences reaction times Caffeine Water conditions.Now ’s good-looking graph! still don’t know difference meaningful, ’s immediately noticeable image. , move conducting statistical test.","code":""},{"path":"t-tests-theory-visualisation-and-calculation-by-hand.html","id":"sidenote-on-assumption-checks","chapter":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","heading":"3.1.1 Sidenote on assumption checks","text":"running statistical test, including t-test, crucial perform assumption checks. Statistical assumption checks procedures verify whether data meet specific requirements test plan use. t-tests, assumptions typically include level measurement, normality data, homogeneity variances. Ensuring assumptions met essential validity test results.However, checks fundamental, can conceptually intuitive first understand t-test works aims achieve. grasping basic mechanics t-test purpose determining whether differences groups statistically significant, better equipped appreciate assumptions matter. walk process conducting t-test, revisit assumption checks detail next chapter.","code":""},{"path":"t-tests-theory-visualisation-and-calculation-by-hand.html","id":"what-actually-is-a-t-test","chapter":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","heading":"3.2 What actually is a t-test?","text":"t-test statistical method used determine whether significant difference means two groups. compares observed difference group means variation within group, taking account sample sizes. result comparison t-value, tells us likely observed difference occurred chance.formula used determine t-statistic (don’t worry ’s scary looks):formula t-statistic :\\[\nt = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}}\n\\]:\\(\\bar{X}\\) sample mean,\\(\\bar{X}\\) sample mean,\\(\\mu\\) population mean,\\(\\mu\\) population mean,\\(s\\) sample standard deviation,\\(s\\) sample standard deviation,\\(n\\) sample size.\\(n\\) sample size., actually everything need work t-statistics basic group descriptives data:\\[\nt = \\frac{183.460 - 198.560}{\\sqrt{\\frac{27.637^2}{50} + \\frac{31.022^2}{50}}}\n\\]example data gives us t-statistic 2.57.calculating t-value, need determine whether value large enough indicate statistically significant difference. done comparing t-value critical value known t-distribution. case, critical value (based desired level significance, commonly 0.05) degrees freedom (depend sample sizes) guide us number t-statistics needs higher finding judged significant. calculated t-value exceeds critical value, reject null hypothesis conclude statistically significant difference groups. (Note: value differ depending one-tailed two-tailed hypothesis)case, take usual alpha value 0.05, find t-distribution table stretches degrees freedom, df = (n1 – 1) + (n2 - 1) = 98The distribution says need t-value around 1.98 finding significant 0.05 level 2.62 significant 0.01 level. , t-value 2.57, ’re little 0.01 level. fact, run JASP see exact p-value comparison 0.012.’s certainly nerdy question ask, try best answer !t-distribution probability distribution similar shape normal distribution (bell curve), heavier tails, meaning spread .number t-distribution table corresponds specific critical value t-distribution given confidence level (like 95%) degrees freedom (df). Degrees freedom calculated total number observations groups minus number groups.look critical value table, finding t-value cuts extreme 5% (95% confidence level) t-distribution. calculated t-value exceeds critical value, falls within extreme region, leading reject null hypothesis.good explainer video topic: link","code":""},{"path":"t-tests-theory-visualisation-and-calculation-by-hand.html","id":"is-all-this-maths-stressing-you-out","chapter":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","heading":"3.3 Is all this maths stressing you out?","text":"understanding formula valuable, important note modern statistical software, JASP, can perform calculations automatically. practice, need manually compute t-value; instead, simply input data JASP, clicks, software provide t-value, degrees freedom, corresponding p-value. makes conducting t-tests accessible, even underlying mathematics may seem complex first glance.","code":""},{"path":"t-tests-theory-visualisation-and-calculation-by-hand.html","id":"writing-up-findings-in-apa-style","chapter":"3 T-Tests: Theory, Visualisation, and Calculation (by hand)","heading":"3.4 Writing up findings in APA style","text":"put findings together, can reliably report (minus assumption checks):conducted independent samples t-test compare mean reaction time across levels caffeine. results showed statistically significant difference means two groups (t(98) = 2.57, p = 0.012). mean reaction time participants caffeine condition (Mean = 183.5, SD = 27.64) significantly lower mean reaction time participants water condition (Mean = 198.6, SD = 31.02), indicating quicker reaction time.add graph just make easier participants interpret:next chapter talk assumption checks t-tests important.","code":""},{"path":"assumption-checks-of-t-tests-and-other-difference-tests.html","id":"assumption-checks-of-t-tests-and-other-difference-tests","chapter":"4 Assumption checks of t-tests and other difference tests","heading":"4 Assumption checks of t-tests and other difference tests","text":"","code":""},{"path":"assumption-checks-of-t-tests-and-other-difference-tests.html","id":"why-we-run-assumption-checks-for-t-tests","chapter":"4 Assumption checks of t-tests and other difference tests","heading":"4.1 Why We Run Assumption Checks for T-Tests","text":"conducting t-test, essential check data meets specific assumptions required test produce valid reliable results. primary reason running assumption checks t-tests rely certain mathematical properties, normality data equal variances groups, accurately determine whether differences group means statistically significant. assumptions violated, t-test may yield misleading results, increasing risk Type Type II errors (see section statistical power detail type errors).illustrate assumptions critical, consider t-tests use mean measure central tendency. However, data normally distributed variance groups unequal, mean may accurately represent central tendency data (see histograms mean different levels normality). lead incorrect conclusions differences groups.following video extract one lectures explain concept assumption checks t-tests","code":""},{"path":"assumption-checks-of-t-tests-and-other-difference-tests.html","id":"assumption-1-type-of-data","chapter":"4 Assumption checks of t-tests and other difference tests","heading":"4.2 Assumption 1: Type of Data","text":"T-tests designed use interval ratio data, differences values meaningful consistent across scale. types data allow precise measurement differences means groups. contrast, ordinal data, ranks data without assuming equal intervals ranks, suitable t-tests test assumes dependent variable measured continuous scale.Using ordinal data t-test can lead inaccurate results distances data points consistent quantifiable way interval ratio data. example, rank participants' satisfaction levels 1 5, difference 1 2 might difference 4 5, making mean less reliable measure central tendency.","code":""},{"path":"assumption-checks-of-t-tests-and-other-difference-tests.html","id":"assumption-2-normal-distribution","chapter":"4 Assumption checks of t-tests and other difference tests","heading":"4.3 Assumption 2: Normal Distribution","text":"One critical assumptions t-test dependent variable normally distributed within group. Normality essential t-test relies distribution sample means approximately normal, especially sample sizes small. assumption allows test accurately assess whether observed difference group means occurred chance.check normality, can use visual methods like histograms Q-Q plots, statistical tests Shapiro-Wilk test (see JASP example read test). sample size large (typically 50 observations per group), visual inspection histograms using skewness kurtosis statistics may suffice. smaller samples, Shapiro-Wilk test recommended. test indicates significant deviation normality, assumption violated, may need consider non-parametric alternative.Small Samples (n < 50): Calculate skewness kurtosis value divide standard error. resulting value falls within range -1.96 1.96, data likely normal.Small Samples (n < 50): Calculate skewness kurtosis value divide standard error. resulting value falls within range -1.96 1.96, data likely normal.Larger Samples (50 < n < 300): Use wider range -3.29 3.29 skewness kurtosis value divided standard error assess normality.Larger Samples (50 < n < 300): Use wider range -3.29 3.29 skewness kurtosis value divided standard error assess normality.skewness kurtosis values fall within respective ranges, data likely meets normality assumption. fall outside ranges, may need consider transforming data using non-parametric test instead.table show values two variables presented figure .","code":""},{"path":"assumption-checks-of-t-tests-and-other-difference-tests.html","id":"assumption-3-homogeneity-of-variance","chapter":"4 Assumption checks of t-tests and other difference tests","heading":"4.4 Assumption 3: Homogeneity of Variance","text":"assumption homogeneity variance states variance (square root standard deviation) within groups compared roughly equal. assumption crucial t-test ensures groups comparable, allowing test accurately assess differences means.images illustrate violation assumption homogeneity variance. demonstrate purple variable much larger variance red variable, however, still normally distributed.statistically test homogeneity variance, can use Levene’s test, compares variances groups. Levene’s test significant, indicates variances equal, assumption homogeneity variance violated. cases, might need consider alternative non-parametric test.next chapter work two examples t-test analyis using data worksheets can find Moodle.","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"jasp-workshop---conducting-t-tests-independent-and-paired","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","text":"JASP free, open-source statistical software package designed user-friendly, point--click interface, ideal research psychology. offers wide range statistical analyses, including t-tests, ANOVA, regression, . guide walk steps conducting independent paired samples t-tests JASP, highlighting critical points check report analysis.JASP available university computers, recommend installing personal laptop desktop continue learning outside class. can download JASP : www.jasp-stats.org.following video shows install JASP computer, talks open data JASP, gives overview use program.","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"t-test-video-walkthrough","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5.1 T-Test Video Walkthrough","text":"video, work example analyses independent paired samples t-tests. want follow along, please find corresponding question sheets datasets available NS5108 module Moodle.Independent samples t-test JASP:\nPaired samples t-tests JASP:\n","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"where-to-click-guide---conducting-t-tests-in-jasp","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5.2 \"Where to Click\" Guide - Conducting T-Tests in JASP","text":"Sometimes just want know click run test. step--step guide performing independent paired samples t-tests JASP. Refer video context regarding steps.","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"conducting-an-independent-samples-t-test","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5.2.1 Conducting an Independent Samples T-Test","text":"Open JASP load data: Click File tab top left, select Open, navigate folder containing data file.Visualise Data: Click Descriptives -> Descriptive Statistics move variables interest Variables box. visualise distribution, use histograms boxplots check spread identify potential outliers.Check Assumptions:Normality: Check normality clicking Plots -> Q-Q plots. Look data points lie approximately along diagonal line.Homogeneity Variance: Conduct Levene’s test clicking T-Tests -> Independent Samples T-Test. Move grouping variable Grouping Variable dependent variable Dependent Variable. Ensure option Equality variances test (Levene’s) checked.Run T-Test: T-Tests -> Independent Samples T-Test, input dependent grouping variables (’ll already done ’ve check homogeneity variance). Extract t-value, degrees freedom (df), p-value output.Interpret Results: Determine statistically significant difference groups based p-value (p < .05 typically indicates significance).","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"conducting-a-paired-samples-t-test","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5.2.2 Conducting a Paired Samples T-Test","text":"Open JASP load data .Visualise Data: Identify paired variables use histograms boxplots visualise distribution. helps assessing normality checking potential outliers.Check Assumptions:Normality: Use Q-Q plots visually check paired differences normally distributed.Homogeneity Variance: paired samples t-test, homogeneity variance assumed. Due participants providing data level IV.Run Paired Samples T-Test: Click T-Tests -> Paired Samples T-Test. Move paired variables Paired Variables box. Extract t-value, degrees freedom, p-value results.Interpret Results: Check p-value determine difference paired groups statistically significant.","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"apa-style-guide-for-reporting-t-tests","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5.3 APA Style Guide for Reporting T-Tests","text":"’s format hypotheses report results t-tests APA style.Hypothesis Independent Samples T-Test (two-tail/non-directional):\n- Null Hypothesis (H0): significant difference [dependent variable] [two groups].\n- Alternative Hypothesis (H1): significant difference [dependent variable] [two groups].Hypothesis Independent Samples T-Test (One-Tail/Directional):\n- Null Hypothesis (H0): significant difference [Group 1] higher/lower [dependent variable] [Group 2].\n- Alternative Hypothesis (H1): [Group 1] significantly higher/lower [dependent variable] [Group 2].Example APA Report Independent Samples T-Test:\n> “independent samples t-test conducted compare reaction times [variable name] group [variable name] group. Assumption checks confirmed normality equal variances. results showed significant difference groups, t(df) = t-statistic, p = p-value, indicating [dependent variable] [greater/lower] [condition 1/2] group compared [condition 1/2].”Hypothesis Paired Samples T-Test (Two-Tail/Non-Directional):\n- Null Hypothesis (H0): significant difference [dependent variable] [Condition 1] [Condition 2].\n- Alternative Hypothesis (H1): significant difference [dependent variable] [Condition 1] [Condition 2].Hypothesis Paired Samples T-Test (One-Tail/Directional):\n- Null Hypothesis (H0): significant difference, [Condition 1] result higher/lower [dependent variable] [Condition 2].\n- Alternative Hypothesis (H1): [Condition 1] results significantly higher/lower [dependent variable] [Condition 2].Example APA Report Paired Samples T-Test:\n> “paired samples t-test conducted compare test scores intervention. Assumption checks confirmed normality dependent variable within condition. results indicated significant improvement scores intervention, t(df) = t-statistic, p = p-value, suggesting intervention effective.”","code":""},{"path":"jasp-workshop---conducting-t-tests-independent-and-paired.html","id":"dr-clarkes-automatic-t-test-generator","chapter":"5 JASP Workshop - Conducting T-Tests (Independent and Paired)","heading":"5.4 Dr Clarke’s Automatic t-test Generator","text":"link takes web application automatically generated data independent samples t-test, performs analysis reports findings.moving sliders around able see data different means SD look affect assumption checks, choice test, statistical significance (observed power, effect size. explained detail next chapter).","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"understanding-effect-size-and-power-in-psychological-research","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6 Understanding Effect Size and Power in Psychological Research","text":"psychological research, understanding just whether difference exists, also large difference , crucial. chapter explore two key concepts help us : effect size statistical power. Effect size measures magnitude relationship difference groups, providing insight practical significance findings. Statistical power, hand, refers likelihood study detect effect one. Together, concepts allow us draw meaningful conclusions data, moving beyond simple yes--answers provided statistical significance.end chapter associated exercise, understand calculate interpret effect sizes, consider power study data collection begins, factors just important——p-value traditionally used hypothesis testing.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"real-world-example-of-effect-size-and-power","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.1 Real-World Example of Effect Size and Power","text":"Imagine scenario psychologist comparing effectiveness two types therapy reducing anxiety: Cognitive Behavioural Therapy (CBT) Mindfulness-Based Stress Reduction (MBSR). conducting study, find statistically significant difference two therapies terms impact anxiety levels.understanding effect size comes . Effect size tell us big difference CBT MBSR . effect size large, suggests one therapy meaningfully better , difference important implications treatment decisions. However, effect size small, even though difference statistically significant, might practically important.Now, consider statistical power. study small sample size, power might low, meaning higher chance detecting difference even one existed. contrast, study high power reduces risk missing true effect, giving confidence results.following sections ’ll look concepts detail, calculated can use calculations build existing statistical findings.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"what-is-effect-size","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.2 What is Effect Size?","text":"p-values tell us whether evidence effect, effect size represents magnitude strength effect. provides way understand practical significance research findings indicating size difference groups strength relationship variables. essence, effect size helps us move beyond asking, \"difference?\" answering, \"big difference?\"Effect size can measured several ways, depending type data nature analysis. commonly used effect size measures psychological research.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"cohens-d","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.2.1 Cohen’s d","text":"Cohen’s d one widely used measures effect size. quantifies difference means two groups terms standard deviations. instance, comparing average anxiety levels participants receiving CBT versus receiving MBSR, Cohen’s d tell us much groups differ, relative variability within group.\n- Small Effect (d ~0.2): small effect size indicates modest difference groups, often noticeable certain conditions large samples.\n- Medium Effect (d ~ 0.5): medium effect size suggests moderate difference, generally visible meaningful contexts.\n- Large Effect (d ~ 0.8): large effect size indicates substantial difference, easily noticeable likely practical importance.illustration different effect sizes:three graphs represents data group spread . Think smoothed version histogram (graph shows data points ). x-axis (horizontal) shows possible values participants score, y-axis (vertical) shows likely participants score score.graph, green area represents \"Group 1,\" red area represents \"Group 2.\" dotted lines show average (mean) value group.less two areas overlap greater difference two groups greater effect size.interactive version image can seen automatic t-test generator shared previous chapter: Automatic t-test generator","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"how-is-cohens-d-calculated","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.2.2 How is Cohen’s d calculated?","text":"’ll likely calculate Cohen’s d single click box JASP statistical software, ’s helpful understand number comes represents. , let’s break Cohen’s d calculated.Cohen’s d measures difference means two groups, relative variability within groups. ’s essentially way standardising difference possible compare findings across different studies, even used different scales measurements.’s basic formula:\\[\nd = \\frac{M_1 - M_2}{SD_{pooled}}\n\\]:\\(M_1\\) \\(M_2\\) means (averages) two groups.\\(M_1\\) \\(M_2\\) means (averages) two groups.\\(SD_{pooled}\\) pooled standard deviation two groups, measure spread variability data.\\(SD_{pooled}\\) pooled standard deviation two groups, measure spread variability data.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"breaking-it-down","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.2.3 Breaking it Down:","text":"Difference Means\nFirst, calculate difference mean values two groups. tells us far apart groups whatever measure ’re using (e.g. anxiety scores).\nFirst, calculate difference mean values two groups. tells us far apart groups whatever measure ’re using (e.g. anxiety scores).Pooled Standard Deviation\nPooled SD bit complex, simple terms, ’s average standard deviations two groups. gives us sense spread scores group.\nformula pooled standard deviation :\nPooled SD bit complex, simple terms, ’s average standard deviations two groups. gives us sense spread scores group.formula pooled standard deviation :\\[\nSD_{pooled} = \\sqrt{\\frac{(n_1 - 1) SD_1^2 + (n_2 - 1) SD_2^2}{n_1 + n_2 - 2}}\n\\]:\\(SD_1\\) \\(SD_2\\) standard deviations group.\\(SD_1\\) \\(SD_2\\) standard deviations group.\\(n_1\\) \\(n_2\\) sample sizes group.\\(n_1\\) \\(n_2\\) sample sizes group.part formula adjusts fact groups might different variabilities different numbers participants.Finally, divide difference means pooled standard deviation. gives us Cohen’s d, tells us big difference standard deviation units., mentioned previously, Cohen’s d can interpreted :Small effect d roughly 0.2 lower. minimum value 0.Medium effect d roughly 0.5Large effect d roughly 0.8 higher. upper bound, however, effect size around 2.0, ’d take indication ’d done something wrong analysis data entry/cleaning meaningful, well-designed experiment difference big.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"other-measures-of-effect-size","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.3 Other Measures of Effect Size","text":"Cohen’s d measure effect size. fact, Cohen’s d mostly just used t-test comparisons.Later book talk Eta-Squared (η²) Partial-Eta Squared (η²p). used primarily ANOVA (Analysis Variance), principle much (just slightly different interpretation values).also looking r R-squared regression section book, correlational equivalent effect size measurement.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"effect-size-meaning-recap","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.4 Effect Size Meaning Recap","text":"Understanding effect size critical goes beyond statistical significance address real-world implications research findings. p-value might tell us difference groups unlikely due chance, tell us difference large enough meaningful.example, large study, even tiny difference might yield significant p-value. However, effect size small, difference might important practical terms. Conversely, large effect size smaller study indicate meaningful difference, even yield statistical significance due small sample size.psychological research, reporting effect size allows better understanding magnitude findings facilitates comparison across studies. also aids meta-analyses, effect sizes different studies combined assess overall strength phenomenon. considering statistical significance effect size, researchers can provide fuller, nuanced interpretation results, leading informed decisions research practice.deeper understanding effect size see Lakens, D. (2022). Improving Statistical Inferences. Section 6.1 – 6.6","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"overview-of-statistical-power","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.5 Overview of Statistical Power","text":"Statistical power crucial concept psychological research closely related effect size. effect size tells us magnitude difference, statistical power tells us likely detect magnitude difference difference actually exists.Understanding statistical power essential planning design stages research, directly influences decision many participants needed study. Given participant recruitment can costly time-consuming, assuring adequate power makes sure study sensitive enough detect meaningful differences, reducing risk overlooking important effects.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"recap-of-type-i-and-type-ii-error","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.5.1 Recap of type I and type II error","text":"conduct study psychology, evidence collect test hypotheses almost guaranteed perfect representation reality. measures won’t perfect, manipulations might fallible, participants unlikely truly representative population, /countless confounding factors., ’s best think terms four possible outcomes research:find evidence difference, difference reliable reflection difference found reality (True Positive)find evidence difference, difference reliable reflection difference found reality (True Negative)find evidence difference, however, reality difference (false positive, Type error)find evidence difference, however, reality difference (false negative, Type II error)good way think concept pretend judge court case. role decide guilt innocents based evidence presented . Underlying decision knowledge evidence unreliable, maybe convicting innocent person letting guilty person go free.job science, just legal system, minimize likelihood Type Type II errors increasing chance correctly identifying true nature reality.Imagine now grid full set possible outcomes experiment. Conducting experiment much like throwing dart grid. start simplistic view box equal area, ’d 25% chance hitting one boxes—meaning 50% chance experiment working designed (landing \"correct decision\" boxes) 50% chance giving answer representative reality (landing one error boxes). scenario illustrates poorly designed experiment mechanisms place improve odds.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"reducing-type-i-errors","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.5.2 Reducing Type I Errors:","text":"improve chances, need adjust likelihood outcome. ’ve already encountered one key way reduce Type errors: setting alpha level. psychology, typically accept false positive rate (Type error) 5%. means willing risk concluding effect (isn’t) 5% time. practice, controlled p-value, tells us probability observing data (something extreme) null hypothesis true. p-value falls threshold 0.05 (represents 5%, 1 20), reject null hypothesis, accepting small chance making Type error.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"addressing-type-ii-errors","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.5.3 Addressing Type II Errors:","text":"Now, consider Type II errors—fail detect true effect. Unlike Type errors, controlled setting alpha level, Type II errors influenced power study. Power probability correctly rejecting null hypothesis false (.e., avoiding Type II error). higher power, less likely miss true effect.psychology, generally aim power 0.80, meaning 80% chance study detect true effect one exists, leaving 20% chance making Type II error.Together hypothetical grid typically look psychology study criteria.psychological research, generally stricter Type errors Type II errors consequences false positive (Type error) often considered serious. Type error means concluding effect difference exists actually , can lead false theories, wasted resources, potentially harmful applications practice, adoption ineffective treatments. setting low alpha level (commonly 0.05), aim minimize risk making incorrect claims. Type II errors (false negatives) also important, often viewed less critical imply missed existing effect, can corrected research. contrast, propagation false positive can far-reaching lasting impacts science practice. , think terms convicting innocent person compared convicting guilty person.","code":""},{"path":"understanding-effect-size-and-power-in-psychological-research.html","id":"power-analysis","chapter":"6 Understanding Effect Size and Power in Psychological Research","heading":"6.6 Power Analysis","text":"Power analysis helps us determine necessary sample size study achieve desired level power. conducting power analysis collecting data, can estimate many participants need reliably detect effect given size. analysis takes account alpha level, effect size expect, desired power.example, expect small effect size, need larger sample achieve 80% power expected large effect size. Without sufficient number participants, study might underpowered, increasing risk Type II error.next chapter walk conduct -priori power analysis independent samples t-test. later book ’ll return concept illustrate can plan intended sample size complex studies.deeper understanding effect size see Lakens, D. (2022). Improving Statistical Inferences. Section 8In next chapter run though conduct power analysis study design.","code":""},{"path":"how-to-conduct-a-power-analysis.html","id":"how-to-conduct-a-power-analysis","chapter":"7 How to Conduct a Power Analysis","heading":"7 How to Conduct a Power Analysis","text":"conduct -priori power analysis (working statistical power study conducting study), difference test, need number pieces information:\n1. design study, many IVs, many levels IVs\n2. likely effect size expect designed study\n3. chosen values alpha power (1-beta)mathematical way can calculate sample size needed study purposes undergraduate studies two simpler ways can calculate sample size studies. Firstly, Cohen’s power primer, simple limited way, secondly G*Power application allows customisable power calculations.","code":""},{"path":"how-to-conduct-a-power-analysis.html","id":"cohens-power-primer","chapter":"7 How to Conduct a Power Analysis","heading":"7.1 Cohen’s Power Primer","text":"Imagine conducting study examine effect new relaxation technique reducing stress levels. plan compare stress scores two groups: one group practicing relaxation technique control group practicing . goal determine required sample size detect significant difference stress scores groups.conduct -priori power calculation using Cohen’s Power Primer, start decide expected effect size, often read literature hopefully found previous research, studying similar topic, base calculation . often assumed medium medium effect size (Cohen’s d = 0.5).Next, ’d choose alpha level (α), significance threshold, commonly set .05 (5%), .01 (1%), .10 (10%); example, use α = .05., ’d determine desired power (1 - β), probability correctly rejecting null hypothesis effect exists; typical target .80 (80%).defining parameters, locate appropriate section Cohen’s Power Primer table (see ), specifically section titled \"1. Mean dif\" (stands difference means, .e. t-test). Find column corresponding chosen alpha level (.05) locate row medium effect size (Med). table, read sample size required adequate power; medium effect size α = .05, table indicates 64 participants per group. determine total sample size study, multiply number participants per group 2, independent samples t-test comparing two groups: 64 participants Group 1 plus 64 participants Group 2 equals total 128 participants. calculation therefore means detect medium effect size 80% power alpha level .05, need total 128 participants (64 per group).different rows grid can used different statistical tests different number conditions different number variables.However, comprehensive (instance table used factorial ANOVA). complex power calculations best use program G*Power","code":""},{"path":"how-to-conduct-a-power-analysis.html","id":"gpower","chapter":"7 How to Conduct a Power Analysis","heading":"7.2 G*power","text":"","code":""},{"path":"how-to-conduct-a-power-analysis.html","id":"installation-and-opening-the-application","chapter":"7 How to Conduct a Power Analysis","heading":"7.2.1 Installation and opening the application","text":"First step download application, university computer already installed able access desktop search bar.computer ’ll need download following webpage: https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpowerYes, right place, hosted university University Düsseldorf web page German. instructions download app English, won’t issue .","code":""},{"path":"how-to-conduct-a-power-analysis.html","id":"conducting-an-a-priori-power-calculation-for-a-t-test","chapter":"7 How to Conduct a Power Analysis","heading":"7.2.2 Conducting an a-priori power calculation for a t-test","text":"Select t-test test family drop menu.select case like power analysis independent samples difference two means.input expected effect size, alpha level, powerThe allocation ratio just wanted unequal groups reason (.e. intervention expensive control), can just leave one.click calculateThe graph top shows two curves: red curve represents null hypothesis distribution, blue dashed curve represents alternative hypothesis distribution, shaded areas highlighting probabilities Type error (α) Type II error (β).Cohen’s power primer, results analysis indicate recruit 64 participants per group, totalling 128 participants, achieve target power. case give us actual power 0.801, assuming effect size study (often big assumption)[check back video power analysis demo later semester]","code":""},{"path":"introduction-to-anova.html","id":"introduction-to-anova","chapter":"8 Introduction to ANOVA","heading":"8 Introduction to ANOVA","text":"ANOVA, stands Analysis Variance, statistical test used comparing continuous dependent variable across three groups. might seem similar t-test already learnt , fundamental issue needs overcome move two groups two groups.chapter learn basic one-way ANOVA, design single independent variable 3 levels, moving factorial ANOVA, design one independent variable, next chapter.","code":""},{"path":"introduction-to-anova.html","id":"example-of-a-one-way-anova-design","chapter":"8 Introduction to ANOVA","heading":"8.1 Example of a one-way ANOVA design","text":"Imagine study designed investigate best frame information campaign prepare residence local area extreme weather event. experiment, participants randomly assigned one four levels independent variable (.e. subjects design).Messaging (independent variable):\n- Fear-Based Messaging: Messages emphasise severe consequences prepared.\n- Efficacy-Based Messaging: Messages focus practical steps individuals can take highlight ability effectively prepare.\n- Community-Focused Messaging: Messages highlight importance collective preparedness working together can improve community resilience.\n- Control condition: messagePreparedness (Dependent variable):\n- Measured using scale developed study. Scored 0 100, higher scores indicating greater intentions engage preparedness behaviours, creating emergency kit making family plan.Hypothesis:H1: hypothesised type preparedness messaging significantly affect participants' intentions prepare disasters.\nH2: Efficacy-based messaging expected generate highest preparedness intentions, followed community-focused messaging, fear-based messaging generating lowest intentions.\nH3: forms messaging lead higher preparedness control, messaging, condition.Procedure:\npower analysis using G*Power (see ), N= 180 participants randomly assigned receive one three types preparedness messages, message. reading message, participants complete questionnaire assessing intentions engage preparedness behaviours. one-way ANOVA used compare mean preparedness intentions across three messaging conditions.(simulated) data study design:graph shows preparedness intentions across four messaging conditions: Community-Focused, Control, Efficacy-Based, Fear-Based (N=45 per condition). Violin plots display distribution scores, wider sections indicating frequent scores. Boxplots within violins show median interquartile range, highlighting central 50% scores. Jittered grey dots represent individual scores, red dots mark outliers. Efficacy-Based messaging shows highest median range, followed Community-Focused, Fear-Based, Control conditions show lower scores.","code":""},{"path":"introduction-to-anova.html","id":"the-omnibus-test","chapter":"8 Introduction to ANOVA","heading":"8.2 The Omnibus Test","text":"ANOVA omnibus test first step determining whether significant differences four groups.key idea behind omnibus test assess whether least one group mean differs significantly others. study, ANOVA omnibus test evaluates whether mean preparedness scores differ across four messaging conditions. However, specify groups different; tells us significant difference exists somewhere among groups. omnibus test significant, post hoc tests used pinpoint differences lie.ANOVA calculates F-statistic comparing two sources variability data: 1. variance group means (-group variance) 2. variance within group (within-group variance). t-test, ’s possible work F-statistic hand, course ’ll using stats program (JASP/SPSS/R) perform us (see blue box don’t make hand).","code":""},{"path":"introduction-to-anova.html","id":"basic-maths-behind-the-anova-omnibus-test","chapter":"8 Introduction to ANOVA","heading":"8.2.1 Basic maths behind the ANOVA omnibus test","text":"-Group Variance - measures variability group means around overall mean groups combined. captures much different messaging conditions (e.g., Efficacy-Based vs. Fear-Based) affect preparedness intentions. larger -group variance indicates group means spread far apart, suggesting potential effect messaging type.Within-Group Variance - measures variability within group around group mean. reflects individual differences preparedness intentions explained messaging condition. High within-group variance suggests participants' scores vary widely within messaging condition.Calculation F-statistic - F-ratio calculated dividing -group variance degrees freedom dividing within-group variance degrees freedom within dividing products . Mathematically, represented :Let’s generate F-statistics experiment.following formular used calculate -group variance. simple terms tells us take group’s mean compared overall mean (grand mean) participants. difference group mean overall mean taken squared, weighted number participants group (45 conditions), summed across groups.\\[\nSSB = \\sum_{=1}^{k} n_i \\left(\\bar{X}_i - \\bar{X}_{\\text{overall}}\\right)^2\n\\]:\\(n_i\\) number participants group \\(\\).\\(n_i\\) number participants group \\(\\).\\(\\bar{X}_i\\) mean group \\(\\).\\(\\bar{X}_i\\) mean group \\(\\).\\(\\bar{X}_{\\text{overall}}\\) overall mean participants.\\(\\bar{X}_{\\text{overall}}\\) overall mean participants.data get following:Within group, participant’s score compared group’s mean. difference squared, squared differences summed group combined across groups.\\[\nSSW = \\sum_{=1}^{k} \\sum_{j=1}^{n_i} \\left( X_{ij} - \\bar{X}_i \\right)^2\n\\]:\\(X_{ij}\\) score participant \\(j\\) group \\(\\).\\(X_{ij}\\) score participant \\(j\\) group \\(\\).\\(\\bar{X}_i\\) mean group \\(\\).\\(\\bar{X}_i\\) mean group \\(\\).\\(n_i\\) number participants group \\(\\).\\(n_i\\) number participants group \\(\\)., words, part need find difference participant groups mean, square sum difference group, sum group totals. snippet works first two participants group, going hand 180 participants (life time!), totals code R instead.groups value = 4468/ 3 = 1489.3Within groups value = 16017 /176 = 91.01And divide groups value within groups valueT-statistics = 1489.3 / 91.01 = 16.36","code":""},{"path":"introduction-to-anova.html","id":"interpreting-the-results-of-an-anova-omnibus-test","chapter":"8 Introduction to ANOVA","heading":"8.3 Interpreting the results of an ANOVA omnibus test","text":"study, F-ratio significantly greater 1 (, F = 16.365), suggests differences messaging conditions larger expected chance. significant F-value (p<0.05, , p<0.001) indicates least one messaging condition different effect preparedness intentions compared others.note: Number table differ slightly hand calculation due rounding numbers step.","code":""},{"path":"introduction-to-anova.html","id":"post-hoc-testing","chapter":"8 Introduction to ANOVA","heading":"8.4 Post-Hoc Testing","text":"significant omnibus result tells us significant difference somewhere among group means, specify differences lie. identify groups differ , need perform additional “post-hoc” tests, make comparisons individual group. way turns back number 2 group comparisons, tempting just resort back running number t-tests . ’s almost , just running t-test t-test cause issue Type error rate.","code":""},{"path":"introduction-to-anova.html","id":"why-we-cant-just-run-multiple-t-tests","chapter":"8 Introduction to ANOVA","heading":"8.5 Why we can’t just run multiple t-tests","text":"likely remember previous chapter, conducting statistical tests, t-tests correlations, test associated specific error rate, typically set alpha level 0.05. means single test, 5% chance making Type error (falsely rejecting null hypothesis). error rate known “test-wise error rate.” However, performing multiple tests dataset, probability making least one Type error increases additional test. instance, perform 20 comparisons, likelihood high* least one results statistically significant chance alone, even null hypothesis true. accumulation error across multiple tests known multiple comparisons problem.ANOVA therefore split two separate tests. omnibus test, test determines significant difference across levels independent variable, post-hoc test, test looks difference individual level independent variable controlling multiple comparisons., got distracted took bit tangent wrote section. Probability hard counterintuitive, originally wanted give actual probability type 1 error study run 20 times. didn’t want get bogged value (’s main take away) ’ve relegated maths another nerdy blue box.Question: 5% probability false positive study run study 20 times probability one study results false positive?Probability false positive: p=0.05Probability false positive: 1 – p = 0.95N = 20Probability false positives: P(x=0)P(x=0) = (20 | 0)x (0.05)^0 x (0.95)^20P(x=0) = 0.95^20 = 0.3585Probability one false positive: P(x=1)P(x=1) = 1 - P(x=0)= 1- 0.358 = 0.6415This gives us 64.15% probability least one 20 studies produce false positive result. say another way, ran study 20 times found significant result, point across 20 studies, likely false positive. .e. good science!","code":""},{"path":"introduction-to-anova.html","id":"interpretation-of-post-hoc-comparisons","chapter":"8 Introduction to ANOVA","heading":"8.6 Interpretation of post-hoc comparisons","text":", statistics program heavy statistical lifting accounting multiple tests give table something like .table can see group paired group, p-value less 0.05 can say difference two groups. can take bit effort get head around table form, graph , time significant differences added.","code":""},{"path":"introduction-to-anova.html","id":"test-yourself","chapter":"8 Introduction to ANOVA","heading":"8.6.1 Test yourself","text":"Look back hypotheses earlier. confirmed? See can explain results real world terms, actually found? technique(s) suggest use ?next chapter work analysis different types one-way ANOVA","code":""},{"path":"jasp-workshop-one-way-anova.html","id":"jasp-workshop-one-way-anova","chapter":"9 JASP workshop – one way ANOVA","heading":"9 JASP workshop – one way ANOVA","text":"video , work example analyses independent repeated measures one-way ANOVAs. want follow along, please find corresponding question sheets datasets available NS5108 module Moodle.[check back later semester video content]","code":""},{"path":"jasp-workshop-one-way-anova.html","id":"where-to-click-guide---conducting-one-way-anovas-in-jasp","chapter":"9 JASP workshop – one way ANOVA","heading":"9.1 “Where to Click” Guide - Conducting One-Way ANOVAs in JASP","text":"Sometimes just want know click run test. step--step guide performing independent repeated measures one-way ANOVAs JASP. Refer video context regarding steps.Conducting Independent Samples One-Way ANOVA\n### Conducting Independent Samples One-Way ANOVAOpen JASP Load Data:\nClick File tab top left.\nSelect Open, navigate folder containing data file.\nClick File tab top left.Select Open, navigate folder containing data file.Visualise Data:\nGo Descriptives -> Descriptive Statistics.\nMove dependent variable Variables box independent variable (factor) Split box.\nUse Plots create histograms boxplots group check distribution identify potential outliers.\nGo Descriptives -> Descriptive Statistics.Move dependent variable Variables box independent variable (factor) Split box.Use Plots create histograms boxplots group check distribution identify potential outliers.Check Assumptions:\nNormality:\nSkewness Kurtosis:\nDescriptive Statistics window, Statistics, check Skewness Kurtosis.\n\nShapiro-Wilk Test:\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test normality group.\nShapiro-Wilk test assesses whether data group normally distributed.\n\nUse:\nUse Shapiro-Wilk test sample size relatively small (typically less 50 per group). larger samples, test becomes overly sensitive, minor deviations normality can lead significant results.\n\n\nHomogeneity Variance:\nNavigate ANOVA -> ANOVA.\nMove dependent variable Dependent Variable independent variable Fixed Factors.\nAssumption Checks, check Homogeneity tests perform Levene's Test.\nnon-significant p-value (p > .05) Levene's Test indicates assumption homogeneity variances met.\n\nNormality:\nSkewness Kurtosis:\nDescriptive Statistics window, Statistics, check Skewness Kurtosis.\n\nShapiro-Wilk Test:\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test normality group.\nShapiro-Wilk test assesses whether data group normally distributed.\n\nUse:\nUse Shapiro-Wilk test sample size relatively small (typically less 50 per group). larger samples, test becomes overly sensitive, minor deviations normality can lead significant results.\n\nSkewness Kurtosis:\nDescriptive Statistics window, Statistics, check Skewness Kurtosis.\nDescriptive Statistics window, Statistics, check Skewness Kurtosis.Shapiro-Wilk Test:\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test normality group.\nShapiro-Wilk test assesses whether data group normally distributed.\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test normality group.Shapiro-Wilk test assesses whether data group normally distributed.Use:\nUse Shapiro-Wilk test sample size relatively small (typically less 50 per group). larger samples, test becomes overly sensitive, minor deviations normality can lead significant results.\nUse Shapiro-Wilk test sample size relatively small (typically less 50 per group). larger samples, test becomes overly sensitive, minor deviations normality can lead significant results.Homogeneity Variance:\nNavigate ANOVA -> ANOVA.\nMove dependent variable Dependent Variable independent variable Fixed Factors.\nAssumption Checks, check Homogeneity tests perform Levene's Test.\nnon-significant p-value (p > .05) Levene's Test indicates assumption homogeneity variances met.\nNavigate ANOVA -> ANOVA.Move dependent variable Dependent Variable independent variable Fixed Factors.Assumption Checks, check Homogeneity tests perform Levene's Test.non-significant p-value (p > .05) Levene's Test indicates assumption homogeneity variances met.Post Hoc Tests:\nGo Post Hoc Tests tab.\nMove independent variable Post Hoc Tests box.\nSelect correction method (e.g. Bonferroni) multiple comparisons.\nGo Post Hoc Tests tab.Move independent variable Post Hoc Tests box.Select correction method (e.g. Bonferroni) multiple comparisons.","code":""},{"path":"jasp-workshop-one-way-anova.html","id":"conducting-a-repeated-measures-one-way-anova","chapter":"9 JASP workshop – one way ANOVA","heading":"9.1.1 Conducting a Repeated Measures One-Way ANOVA","text":"Open JASP Load Data:\n, open data file JASP.\n, open data file JASP.Visualise Data:\nGo Descriptives -> Descriptive Statistics.\nMove levels repeated measure Variables box.\nUse plots visualise distributions condition.\nGo Descriptives -> Descriptive Statistics.Move levels repeated measure Variables box.Use plots visualise distributions condition.Check Assumptions:\nNormality:\nSkewness Kurtosis:\nDescriptive Statistics window, Statistics, check Skewness Kurtosis condition.\n\nShapiro-Wilk Test:\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test condition.\n\nUse:\nUse Shapiro-Wilk test sample sizes per condition small (less 50). larger samples, rely skewness, kurtosis, graphical assessments.\n\n\nSphericity:\nNavigate ANOVA -> Repeated Measures ANOVA.\nAssumption Checks, check Sphericity tests perform Mauchly's Test.\n\nNormality:\nSkewness Kurtosis:\nDescriptive Statistics window, Statistics, check Skewness Kurtosis condition.\n\nShapiro-Wilk Test:\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test condition.\n\nUse:\nUse Shapiro-Wilk test sample sizes per condition small (less 50). larger samples, rely skewness, kurtosis, graphical assessments.\n\nSkewness Kurtosis:\nDescriptive Statistics window, Statistics, check Skewness Kurtosis condition.\nDescriptive Statistics window, Statistics, check Skewness Kurtosis condition.Shapiro-Wilk Test:\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test condition.\nStatistics tab, check Shapiro-Wilk perform Shapiro-Wilk test condition.Use:\nUse Shapiro-Wilk test sample sizes per condition small (less 50). larger samples, rely skewness, kurtosis, graphical assessments.\nUse Shapiro-Wilk test sample sizes per condition small (less 50). larger samples, rely skewness, kurtosis, graphical assessments.Sphericity:\nNavigate ANOVA -> Repeated Measures ANOVA.\nAssumption Checks, check Sphericity tests perform Mauchly's Test.\nNavigate ANOVA -> Repeated Measures ANOVA.Assumption Checks, check Sphericity tests perform Mauchly's Test.Run Repeated Measures ANOVA:\nDefine repeated measures factor Repeated Measures Factors box (e.g., Time levels Time1, Time2, Time3).\nMove corresponding variables Repeated Measures Cells.\nClick OK run analysis.\nDefine repeated measures factor Repeated Measures Factors box (e.g., Time levels Time1, Time2, Time3).Move corresponding variables Repeated Measures Cells.Click OK run analysis.Post Hoc Tests:\nSelect repeated measures factor pairwise comparisons.\nChoose appropriate correction method, Bonferroni.\nSelect repeated measures factor pairwise comparisons.Choose appropriate correction method, Bonferroni.","code":""},{"path":"jasp-workshop-one-way-anova.html","id":"jasp-lab-exercises","chapter":"9 JASP workshop – one way ANOVA","heading":"9.2 JASP Lab Exercises","text":"practice additional exercises can found Moodle course area. exercise comes dataset answer sheet. Work exercises solidify understanding compare results provided answers.","code":""},{"path":"jasp-workshop-one-way-anova.html","id":"apa-style-guide-for-reporting-one-way-anovas","chapter":"9 JASP workshop – one way ANOVA","heading":"9.3 APA Style Guide for Reporting One-Way ANOVAs","text":"format hypotheses report results one-way ANOVAs APA style.Hypotheses Independent Samples One-Way ANOVA (Two-Tailed/Non-Directional)Null Hypothesis (H₀): significant difference [dependent variable] among [different groups].Alternative Hypothesis (H₁): significant difference [dependent variable] among [different groups].Continue specify specific group difference predictionsExample APA Report Independent Samples One-Way ANOVA\"independent samples one-way ANOVA conducted compare effect [independent variable] [dependent variable] across [number] groups. Assumption checks confirmed normality equal variances. results showed significant effect [independent variable] [dependent variable], F(df_between, df_within) = F-value, p = p-value, η² = effect size. Post hoc analyses, using Bonferroni correction, indicated [specific group differences], suggesting [interpretation results].\"Hypotheses Repeated Measures One-Way ANOVA (Two-Tailed/Non-Directional)Null Hypothesis (H₀): significant difference [dependent variable] across [different conditions].Alternative Hypothesis (H₁): significant difference [dependent variable] across [different conditions].Continue specify specific group difference predictionsExample APA Report Repeated Measures One-Way ANOVA\"repeated measures one-way ANOVA conducted examine effect [independent variable] [dependent variable] [number] conditions. Mauchly's Test indicated assumption sphericity met/violated (χ²(df) = value, p = p-value). results revealed significant effect [independent variable] [dependent variable], F(df_between, df_error) = F-value, p = p-value, η² = effect size. Using Greenhouse-Geisser correction (sphericity violated), effect remained significant/non-significant. Post hoc pairwise comparisons showed [specific condition differences], indicating [interpretation results].\"Note: Remember replace placeholders (e.g., [dependent variable], [independent variable], df values, F-values, p-values) actual data reporting results.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"factorial-anova-important-for-your-assignment","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10 Factorial ANOVA (Important for your assignment)","text":"previous chapter, explored one-way ANOVA, statistical method\nused compare means across three groups single\nindependent variable. sometimes want fit one\nindependent variables research. factorial ANOVA\ncomes play.Factorial ANOVA allows us examine effects multiple independent\nvariables dependent variable, well interactions \nfactors. chapter, work basics \nfactorial ANOVA using 2x2 (pronounced two--two) design \nexample. explore interpret main effects interaction\neffects, understand contribute overall\nunderstanding data.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"what-is-a-factorial-anova","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.1 What is a Factorial ANOVA?","text":"factorial ANOVA extension one-way ANOVA includes two\nindependent variables. independent variable, factor, can\nmultiple levels. combined, factors create various\nconditions groups. \"factorial\" aspect refers fact \nevery level one factor combined every level \nfactors.example, 2x2 factorial design, two independent\nvariables, two levels, resulting four unique experimental\nconditions.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"example-of-a-factorial-anova-design","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.2 Example of a factorial ANOVA design","text":"consider study designed investigate presentation \nclimate change information across different psychological distances\naffects changes risk perception. experiment employs 2x2\n-subjects factorial design, manipulating temporal distance (now\nvs. future) physical distance (near vs. far). dependent variable\nchange risk perception, measured difference \npre- post-presentation scores.Research Question: temporal physical distances \nframing climate change information influence changes risk\nperception?Design: 2x2 -subjects factorial design.Independent Variables:Temporal Distance (-Subjects)\nNow: Information presented immediate threat.\nFuture: Information presented distant future threat.\nTemporal Distance (-Subjects)Now: Information presented immediate threat.Now: Information presented immediate threat.Future: Information presented distant future threat.Future: Information presented distant future threat.Physical Distance (-Subjects)\nNear: Information framed affecting geographically close\nlocation (e.g., within participant’s country).\nFar: Information framed affecting geographically\ndistant location (e.g., another continent).\nPhysical Distance (-Subjects)Near: Information framed affecting geographically close\nlocation (e.g., within participant’s country).Near: Information framed affecting geographically close\nlocation (e.g., within participant’s country).Far: Information framed affecting geographically\ndistant location (e.g., another continent).Far: Information framed affecting geographically\ndistant location (e.g., another continent).Dependent Variable:Change Risk Perception: Calculated difference \npost-presentation pre-presentation risk perception scores.\nHigher values indicate greater increase perceived risk.HypothesesH1: Temporal distance main effect change risk\nperception, information presented near-term threat (now)\nresulting greater increase risk perception compared \ninformation presented future threat.H2: Physical distance main effect change risk\nperception, information presented affecting near location\nresulting greater increase risk perception compared \ninformation presented affecting far location.H3: significant interaction temporal \nphysical distance change risk perception, combined\neffect near-term near-location presentations leading \ngreatest increase risk perception.ProcedureParticipant Recruitment: Recruit N = 128 participants (\nspecified -priori power analysis) randomly assigned\none four conditions, ensuring equal group sizes (n = 32\nper condition).Pre-Presentation Measure: participants complete baseline\nmeasure risk perception using standardised risk perception\nscale.Presentation Information:Now, Near Condition: Immediate threat affecting nearby\nlocation.Now, Far Condition: Immediate threat affecting distant\nlocation.Future, Near Condition: Future threat affecting nearby\nlocation.Future, Far Condition: Future threat affecting distant\nlocation.Post-Presentation Measure: presentation, participants\ncomplete risk perception scale .Calculation Change Scores: change risk perception \ncalculated subtracting pre-presentation scores \npost-presentation scores.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"visualizing-factorial-anova-data","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.3 Visualizing Factorial ANOVA Data","text":"four-condition setup, tempting visualise data \none-way ANOVA—treating combination factors \nseparate condition. approach allows us compare across \nfour conditions see differences lie., can already start see differences \nconditions. However, method somewhat misses core aspect \nstudy design. two separate independent variables, two\nlevels, factorial structure well presented \nprevious image. visualising data way, treat \ncombination isolated condition, neglecting interactions\nvariables.better capture relationships data, makes sense \nuse line graph 95% confidence intervals. type \nvisualisation reflects main effects interaction effect\ntwo variables. allows us see effect one\nindependent variable may depend level independent\nvariable. data presented line graph:line graph can now see risk perception increases \nclimate change presented immediate threat (\"Now\") compared \nfuture threat (\"Future\"), main effect Temporal\nDistance. Additionally, risk perception higher threat \nframed affecting nearby location (\"Near\") rather distant one\n(\"Far\"), main effect Physical Distance.Importantly, lines parallel, suggesting significant\ninteraction effect Temporal Physical Distance.\nSpecifically, combination presenting climate change \nimmediate nearby results greatest increase perceived risk,\neffect less pronounced either temporal physical\naspect framed distant.always, need check see differences \noccurring chance likely experimental\nmanipulation causing differences. need \nreturn statistical analysis.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"conducting-a-factorial-anova","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.4 Conducting a Factorial ANOVA","text":"","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"the-omnibus-test-1","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.4.1 The Omnibus Test","text":"factorial ANOVA, omnibus test first step assessing\nwhether statistically significant effects within data.\ntest evaluates main effects independent variable \nwell interaction effects . main effects tell us\nwhether independent variable (e.g., Temporal Distance Physical\nDistance) independently influences dependent variable. \ninteraction effect, hand, tests whether impact one\nindependent variable depends level variable.\nImportantly, omnibus test specify groups \nsignificantly different one another; simply tells us \nstatistically significant effect present data, either \nmain effects interaction.example can see significant main effect \ntemporal distance physical distance, significant\ninteraction effect. can look descriptive statistics \ndetermine direction main effects.mean dependent variable value (difference risk\nperception presented information) “Now”\ninformation framing significantly higher “Future”\ninformation framing mean dependent variable value “Near”\ninformation framing significantly higher “Far” information\nframing.interaction effect, however, takes little thought.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"interaction-effect","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.4.2 Interaction Effect","text":"interaction effect shows whether effect one independent\nvariable dependent variable changes depending level \nindependent variable. study, instance, interaction\neffect reveals impact Temporal Distance risk perception\nvaries depending whether information framed \"Near\" \n\"Far.\" crucial highlights combined influence \nfactors, providing deeper understanding looking main\neffects alone. interaction effect significant, suggests \nfactors operate independently rather\ninteract way influences outcome variable.’s possible determine breakdowns means.However, far easier determine line graph, see\npervious section.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"post-hoc-tests-in-factorial-anova","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.4.3 Post-Hoc Tests in Factorial ANOVA","text":"omnibus test reveals significant effects, particularly\ninteraction effects, post-hoc tests needed pinpoint exactly \ndifferences lie. Post-hoc tests factorial ANOVA used \nexplore specific group comparisons driving significant\nfindings observed interaction main effects. tests make\npairwise comparisons levels independent variables\ncontrolling increased risk Type errors comes\nmultiple testing. example, interaction effect \nsignificant, post-hoc tests help determine whether difference\n“Now” “Future” significant levels Physical\nDistance (“Near” “Far”), vice versa. Essentially, post-hoc tests\nbreak overall effects detected omnibus test \nspecific, interpretable comparisons, allowing detailed\nunderstanding nature effects data.","code":""},{"path":"factorial-anova-important-for-your-assignment.html","id":"assumption-checks","chapter":"10 Factorial ANOVA (Important for your assignment)","heading":"10.4.4 Assumption checks","text":"interpreting results factorial ANOVA, crucial \nensure data meet necessary assumptions type \nanalysis. assumptions similar discussed \nchapter t-tests, specifically assumptions homogeneity \nvariance normality. However, key differences \nassumptions apply conducting factorial ANOVA.Homogeneity Variance, refers assumption variance\nwithin group (condition) equal. factorial ANOVA, \nassumption must met just across levels single\nindependent variable (one-way ANOVA independent samples\nt-test), across combinations levels ()\nindependent variables. example, 2x2 design \nTemporal Distance Physical Distance independent variables, \nmust ensure variance four unique conditions\n(e.g., \"Now_Near\", \"Now_Far\", \"Future_Near\", \"Future_Far\") similar.\nViolations assumption can lead biased F-statistics, making \ndifficult trust results ANOVA. Levene’s Test \ntypically used check assumption, factorial ANOVA, \nassesses variance across conditions simultaneously.Normality refers assumption dependent variable \nnormally distributed within group. factorial designs, must\nensure normality holds within combination independent\nvariables. minor deviations normality unlikely impact\nresults significantly, severe violations can lead incorrect\ninferences. t-test, assumption can evaluated using\nvisual inspections histograms statistical tests like \nShapiro-Wilk test. factorial ANOVA, however, complexity increases\nadditional independent variable, normality must \nchecked within subgroup.Factorial ANOVA also assumes observations independent \n, just simpler designs. means \nsystematic relationship scores participants \ndifferent conditions. assumption may require statistical\ntesting, ensured design data collection\nphases.","code":""},{"path":"jasp-workship---factorial-anova.html","id":"jasp-workship---factorial-anova","chapter":"11 JASP workship - Factorial ANOVA","heading":"11 JASP workship - Factorial ANOVA","text":"video , walk examples analysing factorial ANOVAs. want follow along, please find corresponding question sheets datasets available NS5108 module Moodle.[Insert Video ]","code":""},{"path":"jasp-workship---factorial-anova.html","id":"where-to-click-guide---conducting-factorial-anovas-in-jasp","chapter":"11 JASP workship - Factorial ANOVA","heading":"11.1 “Where to Click” Guide - Conducting Factorial ANOVAs in JASP","text":"Sometimes just want know click run test. step--step guide performing factorial ANOVAs JASP. Refer video context regarding steps.","code":""},{"path":"jasp-workship---factorial-anova.html","id":"conducting-a-fully-independent-samples-factorial-anova","chapter":"11 JASP workship - Factorial ANOVA","heading":"11.1.1 Conducting a fully Independent Samples Factorial ANOVA","text":"Open JASP Load Data:Click File tab top left.Select Open, navigate folder containing data file.Explore data:Due factorial design, need filter data able split data one variable. filter funnel symbol click top right screen.filter box need bring one IVs, click “=” type one levels. click “apply pass filter” everything run just level variable.Go Descriptives -> Descriptive Statistics.Move dependent variable Variables box independent variables (factors) Split box.Use Plots create histograms boxplots combination factors check distribution identify potential outliers.Clear change filtered level level IV repeatOnce finished make sure clear filter entirely.Check Assumptions:Normality:Check normality within combination independent variables. need filter account design.Descriptive Statistics window, Statistics, check Skewness Kurtosis condition.Use Shapiro-Wilk Test assess normality within condition sample sizes small (typically less 50 per group).Homogeneity Variance:Navigate ANOVA -> ANOVA.Move dependent variable Dependent Variable independent variables Fixed Factors.Assumption Checks, check Homogeneity tests perform Levene's Test.non-significant p-value (p > .05) Levene's Test indicates assumption homogeneity variances met across conditions.Run Factorial ANOVA:Ensure dependent variable independent variables correctly specified.Select report effect sizesSelect create line graphs illustrate relationship variablesSelect run post hoc testsPost Hoc Tests:significant main effects interactions found, go Post Hoc Tests tab.Move independent variables interactions Post Hoc Tests box.Select correction method (e.g., Bonferroni Tukey) multiple comparisons.help identify significant differences lie among levels independent variables.","code":""},{"path":"jasp-workship---factorial-anova.html","id":"conducting-a-factorial-anova-with-a-repeated-measures-variable","chapter":"11 JASP workship - Factorial ANOVA","heading":"11.1.2 Conducting a Factorial ANOVA with a repeated measures variable","text":"Name repeated measures variable clicking says RM Factor 1 renaming .Bring levels repeated measures variable says level 1 level 2, adding levels needed.Add new repeated measure IVs needed repeatBring independent samples IV subject factors boxRepeat steps form independent samples factorial ANOVA","code":""},{"path":"jasp-workship---factorial-anova.html","id":"apa-style-guide-for-reporting-factorial-anovas","chapter":"11 JASP workship - Factorial ANOVA","heading":"11.2 APA Style Guide for Reporting Factorial ANOVAs","text":"format hypotheses report results factorial ANOVAs APA style.Hypotheses Independent Samples Factorial ANOVA:Null Hypothesis (H₀): wil significant main effect IV1], significant main effect [IV2], interaction effect [IV1] [IV2] [dependent variable].Null Hypothesis (H₀): wil significant main effect IV1], significant main effect [IV2], interaction effect [IV1] [IV2] [dependent variable].Alternative Hypothesis (H1): significant main effect [IV1] whereby ...Alternative Hypothesis (H1): significant main effect [IV1] whereby ...Alternative Hypothesis (H2): significant main effect [IV2] whereby ...Alternative Hypothesis (H2): significant main effect [IV2] whereby ...Alternative Hypothesis (H3): significant interaction effect whereby change IV1 ... compared change IV2Alternative Hypothesis (H3): significant interaction effect whereby change IV1 ... compared change IV2","code":""},{"path":"jasp-workship---factorial-anova.html","id":"example-apa-report-for-an-independent-samples-factorial-anova","chapter":"11 JASP workship - Factorial ANOVA","heading":"11.3 Example APA Report for an Independent Samples Factorial ANOVA","text":"\"independent samples factorial ANOVA conducted examine effects [IV1] [IV2] [dependent variable]. Assumption checks confirmed normality homogeneity variance. results revealed significant main effect [IV1], F(df_between, df_within) = F-value, p = p-value, η² = effect size, significant main effect [IV2], F(df_between, df_within) = F-value, p = p-value, η² = effect size. Additionally, significant interaction effect [IV1] [IV2], F(df_between, df_within) = F-value, p = p-value, η² = effect size. Post hoc analyses using Bonferroni correction indicated [specific group differences], suggesting [interpretation results].\"","code":""},{"path":"jasp-workship---factorial-anova.html","id":"example-apa-report-for-an-repeated-measures-factorial-anova","chapter":"11 JASP workship - Factorial ANOVA","heading":"11.4 Example APA Report for an Repeated Measures Factorial ANOVA","text":"\"repeated measures factorial ANOVA conducted examine effects [Repeated Measure IV] [-Subjects IV] [dependent variable]. Mauchly's Test indicated assumption sphericity met/violated (χ²(df) = value, p = p-value). results showed significant main effect [Repeated Measure IV], F(df_between, df_error) = F-value, p = p-value, η² = effect size, significant main effect [-Subjects IV], F(df_between, df_error) = F-value, p = p-value, η² = effect size. also significant interaction effect, F(df_between, df_error) = F-value, p = p-value, η² = effect size. Using Greenhouse-Geisser correction (sphericity violated), effect remained significant/non-significant. Post hoc pairwise comparisons showed [specific condition differences], indicating [interpretation results].\"","code":""},{"path":"advanced-anova-techniques.html","id":"advanced-anova-techniques","chapter":"12 Advanced ANOVA techniques","heading":"12 Advanced ANOVA techniques","text":"[check back content later semester]","code":""},{"path":"introduction-to-regression-analysis.html","id":"introduction-to-regression-analysis","chapter":"13 Introduction to Regression analysis","heading":"13 Introduction to Regression analysis","text":"Regression analysis powerful statistical tool investigates relationships variables. core, technique used understand quantify one variable (criterion variable) changes response another several others (predictor variables). merely determining singular correlation, regression analysis offers nuance, enabling researchers predict outcomes uncover sophisticated patterns embedded within datasets.","code":""},{"path":"introduction-to-regression-analysis.html","id":"data-driven-decision-making","chapter":"13 Introduction to Regression analysis","heading":"13.1 Data-driven decision making","text":"2011 film Moneyball based true story Oakland Athletics baseball team's 2002 season. general manager, Billy Beane (played Brad Pitt film), faced problem: limited budget put together winning team. Rather relying traditional baseball scouting methods, often depended heavily scouts' intuitions prone various biases, Beane employed skills young Yale economics graduate named Peter Brand (played Jonah Hill). Brand used statistical analysis evaluate players' values.traditional method valuing players subjective. Scouts often looked physique players, style, moved, even things like attractiveness girlfriends indicator confidence. Beane Brand shifted focus objective evidence, including statistics.core, analytics used \"Moneyball\" predicting runs, importantly, wins. Using regression analysis, determine statistics strongly correlated creating runs. understanding leads runs, use build model aid player acquisition decisions.film (also far nerdy book Michael Lewis) highlights tensions arise data-driven metrics clash entrenched traditional norms. Yet, time progresses, efficacy analytical techniques becomes increasingly evident. Today, rarity encounter professional sports team incorporate form statistical analysis strategy decision-making processes, showcasing undeniable impact relevance regression modern world.","code":""},{"path":"introduction-to-regression-analysis.html","id":"further-real-world-examples","chapter":"13 Introduction to Regression analysis","heading":"13.2 Further real world examples","text":"course, ’s just world sports taken advantage regression analysis inform decision making. examples everyday life regression analysis plays role.Streaming Services Recommendations: probably use platforms like Netflix, Spotify, YouTube. platforms utilise regression analysis predict shows, songs, videos might like based past behaviour behaviour others similar tastes.Streaming Services Recommendations: probably use platforms like Netflix, Spotify, YouTube. platforms utilise regression analysis predict shows, songs, videos might like based past behaviour behaviour others similar tastes.Predicting Box-office Income Different Forms Advertising: Film producers often use regression analyses gauge adverts, TV spots social media campaigns, influence cinema ticket sales, optimising advertising budgets upcoming films based insights.Predicting Box-office Income Different Forms Advertising: Film producers often use regression analyses gauge adverts, TV spots social media campaigns, influence cinema ticket sales, optimising advertising budgets upcoming films based insights.","code":""},{"path":"introduction-to-regression-analysis.html","id":"use-of-regression-analysis-in-psychology","chapter":"13 Introduction to Regression analysis","heading":"13.3 Use of regression analysis in psychology","text":"Regression analysis also cornerstone psychological research, allows psychologists simultaneously explore dissect influence numerous variables single outcome variable. comprehensive approach indispensable field like psychology, behaviours mental processes often outcome web interconnected variables.instance:Predictors Job Satisfaction: Organizational psychologists use regression determine factors (e.g., salary, working hours, team dynamics, leadership style) significant predictors job satisfaction among employees.Predictors Job Satisfaction: Organizational psychologists use regression determine factors (e.g., salary, working hours, team dynamics, leadership style) significant predictors job satisfaction among employees.Influence Social Behaviours: Social psychologists might employ regression analysis understand various factors like media exposure, peer influence, past experiences predict certain social behaviours attitudes, aggression altruism.Influence Social Behaviours: Social psychologists might employ regression analysis understand various factors like media exposure, peer influence, past experiences predict certain social behaviours attitudes, aggression altruism.Personally interested public health climate change. Regression indispensable studying areas like :Determinants Vaccine Hesitancy: used regression analysis predict likelihood individuals hesitant take vaccines. Predictors analysis demographic factors like age education level, psychological factors risk perception trust healthcare, well societal variables like exposure misinformation social media.Determinants Vaccine Hesitancy: used regression analysis predict likelihood individuals hesitant take vaccines. Predictors analysis demographic factors like age education level, psychological factors risk perception trust healthcare, well societal variables like exposure misinformation social media.Predictors Pro-Environmental Behaviours: currently using regression model predict pro-environmental behaviour varying levels capability, motivation, opportunity (.e. COM-B model).Predictors Pro-Environmental Behaviours: currently using regression model predict pro-environmental behaviour varying levels capability, motivation, opportunity (.e. COM-B model).Reflect psychological phenomenon behaviour piqued interest recently studied.Jot potential factors variables believe might influence phenomenon behaviour.Consider factors might integrated regression model. variable choose dependent variable (variable predicting)? variables might serve predictors dependent variable?","code":""},{"path":"introduction-to-regression-analysis.html","id":"overview-of-regression-techniques-covered-in-this-module","chapter":"13 Introduction to Regression analysis","heading":"13.4 Overview of regression techniques covered in this module","text":"handbook cover three main regression techniques:Simple linear regression (basically just fancy correlation)Multiple linear regression (technique required assignment)Binary logistic regression (multiple regression binary, instead continuous, criterion variable)Continue statistics psychology ’ll learn forms regression, regression like, analysis techniques :Ordinal regressionMediation moderationMulti-level modellingStructural equation modelling","code":""},{"path":"simple-linear-regression.html","id":"simple-linear-regression","chapter":"14 Simple linear regression","heading":"14 Simple linear regression","text":"","code":""},{"path":"simple-linear-regression.html","id":"recap-of-correlation","chapter":"14 Simple linear regression","heading":"14.1 Recap of correlation","text":"core, correlation provides measure data points two variables related one another.Positive Correlation: (top left figure) one variable increases, also increases.Negative Correlation: (top right figure) one variable increases, decreases.Correlation: (bottom figure) discernible pattern relationship two variables.strength direction correlation represented correlation coefficient, typically denoted \\(r\\). value \\(r\\) ranges -1 +1.\\(r\\) value closer +1 indicates stronger positive correlation.\\(r\\) value closer -1 indicates stronger negative correlation.\\(r\\) value closer 0 suggests little correlation.","code":""},{"path":"simple-linear-regression.html","id":"correlation-as-a-statistical-test","chapter":"14 Simple linear regression","heading":"14.1.1 Correlation as a Statistical Test","text":"Going beyond visualisation relationships, correlation can also used formal statistical test determine significant linear relationship two variables.conducting correlation test, p-value informs us significance observed correlation coefficient (\\(r\\)). p-value predetermined threshold (commonly 0.05), infer correlation sample likely due random chance, thus, statistically significant relationship two variables.Surface Level Explanation:p-value number 0 1 tells us result experiment likely due chance something going . small p-value (typically less 0.05) suggests result significant just random occurrence.Intermediate Explanation:p-value represents probability observing data (something extreme) given specific null hypothesis true. p-value less pre-decided threshold (like 0.05), reject null hypothesis favour alternative hypothesis. suggests observed data unlikely assumption null hypothesis. However, smaller p-value necessarily mean result \"meaningful\"; just indicates statistically significant.-Depth Explanation:Mathematically, p-value probability observing data extreme , extreme , observed data assumption null hypothesis true. direct measure probability either hypothesis true. Instead, measure extremity data relative specific model.Lower p-values suggest observed data less likely null hypothesis, leading us reject null favour alternative hypothesis. However, crucial understand p-value measure size effect practical significance result. Furthermore, threshold 0.05 common, arbitrary must chosen context caution.Lastly, essential remember p-value contingent correctness underlying statistical model level data meet statistical assumptions. Misunderstandings misuse p-values led various controversies scientific community (see replication crisis). See Lakens (2022) Improving Statistical Inferences comprehensive overview p-values.","code":""},{"path":"simple-linear-regression.html","id":"statistical-assumptions-of-correlation","chapter":"14 Simple linear regression","heading":"14.1.2 Statistical assumptions of correlation","text":"frequentist statistical analysis, often aim use parametric tests statistical assumptions underlying tests met, tests can sensitive smaller effect sizes. correlation analysis, Pearson's correlation commonly used parametric test Spearman's Rank correlation commonly used non-parametric test.experience teaching regression, assumption checks often part statistics confuse students . now, put safely away box, scare first week :-)fully refresh theory behind assumption check aspect next week cover multiple regression.correlation analyse following assumptions checked assessed:Linearity: variables linear relationship, means plot scatterplot, distribution data points roughly form straight line.Linearity: variables linear relationship, means plot scatterplot, distribution data points roughly form straight line.Normality Residuals: data pairs normally distributed, meaning variables analysed approximately normally distributed considered together. checking normality residuals.Normality Residuals: data pairs normally distributed, meaning variables analysed approximately normally distributed considered together. checking normality residuals.Homoscedasticity: spread (variance) residuals constant across values variable x-axis. .e. data points start clustered widen (like funnel) along correlation.Homoscedasticity: spread (variance) residuals constant across values variable x-axis. .e. data points start clustered widen (like funnel) along correlation.Absence Outliers: Outliers can disproportionately influence correlation coefficient. , important check consider impact outliers data. Outliers can checked use boxplot calculated manually.Absence Outliers: Outliers can disproportionately influence correlation coefficient. , important check consider impact outliers data. Outliers can checked use boxplot calculated manually.","code":""},{"path":"simple-linear-regression.html","id":"simple-linear-regression-1","chapter":"14 Simple linear regression","heading":"14.2 Simple linear regression","text":"content sections , likely reflects extent explored correlation first year us. Regression analysis simply extension knowledge. Central understanding concept \"line best fit\".might previously used line visual representation relationship two variables, linear regression, takes pivotal role.","code":""},{"path":"simple-linear-regression.html","id":"the-line-of-best-fit","chapter":"14 Simple linear regression","heading":"14.2.1 The line of best fit","text":"scatter plot , see data points representing hours sleep test performance various participants. Notice data points positively correlated? can capture trend drawing straight line data. line, call \"line best fit,\" gives us simplified representation relationship hours sleep test performance. optimal line best fit one minimises total distance individual data points plot.Even best possible straight line drawn data points, rare line pass exactly every point. vertical distance data point line called \"residual\". every data point, residual difference actual value line predicts value . line best fit job well, residuals quite small, indicating predictions close actual data. However, residuals large, suggests line might capturing relationship two variables adequately.talk residuals, context assumption checks, next chapter.purpose line best fit model relationship test performance number hours slept.model suggest performance test participant gets 7 hours sleep? 100110120130140Find point 7 hours x-axis.Draw finger blueline.Draw finger across y-axis.Take number.test score model suggest participant 7 hours sleep.","code":""},{"path":"simple-linear-regression.html","id":"the-equation-of-a-line","chapter":"14 Simple linear regression","heading":"14.2.2 The equation of a line","text":"Every straight line scatter plot can written simple formula:\\[ Y = mX + c \\]:\\(Y\\) dependent variable.\\(Y\\) dependent variable.\\(X\\) independent variable.\\(X\\) independent variable.\\(c\\) line intersects Y-axis, representing predicted test performance hours sleep .\\(c\\) line intersects Y-axis, representing predicted test performance hours sleep .\\(m\\) slope line, indicating predicted change test performance additional hour sleep.\\(m\\) slope line, indicating predicted change test performance additional hour sleep.make formula previous scatter plot need work much blue line \"goes \" far \"goes along\", line crosses y-axis.scatter plot data axes going zero time. allows us get \\(c\\) value, 50, taking two points line can work \\(m\\) value, case 10 (60/6).give us following equation line:\\[ y = 10x + 50 \\]Look back question . need now sub number 7 get predicted test score: (10*7) + 50 = 120","code":""},{"path":"simple-linear-regression.html","id":"is-this-a-good-model","chapter":"14 Simple linear regression","heading":"14.2.3 Is this a good model?","text":"oft-cited quote sums central truth statistics data modelling: model can capture full intricacy unpredictability real-world phenomena. However, diminish value models. model simplifies complex systems highlights important relationships, can offer invaluable insights guide decision-making.line sleep test score scatter plot () appear quite representative data therefore perhaps fairly good model (long sample representative population want use model future).However, try use relationship participants height test scores model, likely fairly poor model:model derived data, \\(y = -0.05x + 77.89\\), actually give answer close 70 matter individuals height. words, whether taller shorter, model essentially shrugs predicts something close 70 (mean dataset). describe useful model, , spend evening rack try make taller test.can see scatter plot, none numbers equations allow us gauge 'useful' fitting sleep model compared height model. understand model fit need final new concept week, shared variance","code":""},{"path":"simple-linear-regression.html","id":"shared-variance","chapter":"14 Simple linear regression","heading":"14.2.4 Shared Variance","text":"examining relationship two variables, proportion variation one variable can predicted explained variable known shared variance . correlational research, concept crucial tells us well one variable explains variation variable. word useful used scores one variable predict scores variable.good way think visually Venn diagram.value shared variance derived taking square \\(r\\), correlation coefficient, giving us \\(R^2\\) value model. Take instance, sleep model, run pearsons correlation test data find relationship correlation coefficient \\(r\\) = 0.94, squaring give us \\(R^2\\) 0.89 predictor (study hours) outcome (test scores).Another way talk \\(R^2\\) say 89% variance test scores can predicted explained number hours studied. remaining 11% variance due factors included model random error.say two variables share certain percentage variance, indication strength utility relationship. However, high shared variance can promising, essential remember correlation imply causation. underlying factors, third variables, even coincidences create correlations.practical terms, understanding shared variance critical researchers. significant shared variance exists, predictor variable becomes valuable understanding, predicting, even potentially influencing outcome variable. However, unexplained variance might also prompt researchers consider additional predictors factors initially model.Next week start looking multiple regression modelling, name suggest involves multiple variables sharing variance dependent variable.","code":""},{"path":"simple-linear-regression.html","id":"week-1---test-yourself-mcqs","chapter":"14 Simple linear regression","heading":"14.3 Week 1 - Test yourself mcq’s","text":"","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"jasp-workshop---simple-linear-regression","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15 JASP Workshop - Simple linear Regression","text":"JASP free, open-source statistical software package user-friendly, point--click interface suitable research psychology. offers wide range statistical analyses, basic descriptive statistics advanced methods like regression ANOVA.JASP available university computers, however, recommend also install personal version laptop desktop computer can continue learning outside class time.JASP can downloaded : www.jasp-stats.org/","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"simple-linear-regression---example-analysis","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15.1 Simple linear regression - example analysis","text":"video go analysis answers Simple Linear Regression Exercise 1. like follow along (go exercise first) question sheet data can found Week 1 module area NS5108.","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"where-to-click-guide---simple-linear-regression-analysis","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15.2 \"Where to click\" guide - simple linear regression analysis","text":"following step step guide performing correlation simple linear regression analysis JASP. Watch video context related steps.","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"correlation","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15.2.1 Correlation","text":"Open JASP.Open JASP.Load Data: click File tab top left select Open. navigate folder containing data file open .Load Data: click File tab top left select Open. navigate folder containing data file open .Visualise data: Identify two variables want run correlation . Click Descriptives move two variables Variables box. , Customizable plots drop tick Scatter plots. may want click none graphs right (make plot clearer). Also change regression line (line best fit) linear.Visualise data: Identify two variables want run correlation . Click Descriptives move two variables Variables box. , Customizable plots drop tick Scatter plots. may want click none graphs right (make plot clearer). Also change regression line (line best fit) linear.Identify outliers: Also customizable plots drop , select boxplots. Tick label outliers show dots outside whiskers box plots. Note interpret outliers.Identify outliers: Also customizable plots drop , select boxplots. Tick label outliers show dots outside whiskers box plots. Note interpret outliers.Calculate correlation coefficient: click Regression -> Correlation top bar. Pick variables interested running correlation . Extract interpret Pearson's \\(r\\) p-value.Calculate correlation coefficient: click Regression -> Correlation top bar. Pick variables interested running correlation . Extract interpret Pearson's \\(r\\) p-value.","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"simple-linear-regression-2","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15.2.2 Simple linear regression","text":"Check normality (residuals): Click Regression -> Linear Regression top bar. Pick criterion variable (variable aiming predict) place Dependent variable box. Pick predictor place Covariates box. Plots drop select Residuals histogram. Visually interpret histogram indication assumption check.Check normality (residuals): Click Regression -> Linear Regression top bar. Pick criterion variable (variable aiming predict) place Dependent variable box. Pick predictor place Covariates box. Plots drop select Residuals histogram. Visually interpret histogram indication assumption check.Check Homoscedasticity: Also Plots drop , select Residuals vs. predicted. Visually interpret plot assess Homoscedasticity.Check Homoscedasticity: Also Plots drop , select Residuals vs. predicted. Visually interpret plot assess Homoscedasticity.Extract \\(R^2\\) value: first table Linear Regression section.Extract \\(R^2\\) value: first table Linear Regression section.Determine significance model: can obtained though ANOVA table. F(df,df)=F-value, p=p-value.Determine significance model: can obtained though ANOVA table. F(df,df)=F-value, p=p-value.Extract values model regression equation: H1 intercept = \\(c\\), H1 variable = \\(m\\)Extract values model regression equation: H1 intercept = \\(c\\), H1 variable = \\(m\\)","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"jasp-lab-exercises-1","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15.3 JASP lab exercises","text":"two additional exercises Moodle. exercise question sheet comes dataset answers sheet. Work question check answers answer sheet.","code":""},{"path":"jasp-workshop---simple-linear-regression.html","id":"apa-style-guide","chapter":"15 JASP Workshop - Simple linear Regression","heading":"15.4 APA style guide","text":"example hypothesis correlational analysis:Null Hypothesis (H0): correlation participants' age scores cognitive ability test.Alternative Hypothesis (H1): correlation participants' age scores cognitive ability test.example correlation analysis reported APA style:Pearson correlation conducted assess relationship participants' age scores cognitive ability test. Assumptions checks performed ensure violation assumptions normality residuals, linearity, homoscedasticity. significant negative correlation age cognitive ability scores, r(98) = -.45, p < .001, older participants tending lower scores cognitive ability test. , null hypothesis can rejected.example hypothesis simple linear regression analysis:Null Hypothesis (H0): number study hours significant predictor test scores.Alternative Hypothesis (H1): number study hours significant predictor test scores.example simple linear regression analysis reported APA style:simple linear regression conducted predict test scores based number study hours. assumptions linearity, independence, normality checked met.results indicated significant relationship number study hours test scores, F(1, 98) = 34.5, p < .001. , null hypothesis can rejected R² value .26, indicating approximately 26% variance test scores can explained number study hours.regression equation found :Test Score = 50.2 + 6.7*Study Hours.additional hour study, increase 6.7 points test score. intercept value 50.2 indicates student study (0 hours) expected score 50.2 points test.","code":""},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"16 Multiple regression","heading":"16 Multiple regression","text":"Multiple regression method helps us predict outcome variable based several independent variables. technique tells us variables important predicting outcome, predictor variables interact. building prediction model, multiple regression allows us understand variables connected helps us make powerful data-informed decisions.","code":""},{"path":"multiple-regression.html","id":"shared-variance-with-multiple-variables","chapter":"16 Multiple regression","heading":"16.1 Shared variance with multiple variables","text":"previous chapter simple linear regression, suggested one predictor variable (exercise) shared variance another variable (well-). Visually, can represented using Venn diagram two circles: one exercise one well-.\noverlap two circles represents shared variance, extent variations exercise can explain variations well-. shared variance can represented numerically square correlation coefficient two variables. hypothetical example exercise correlated well-correlation coefficient r=0.307 (.e. moderate-positive correlation) , , can said exercise explains 9.4% variance well-(\\(R^2\\)=0.094)multiple regression analysis, use two independent variables predict value dependent variable. , independent variable may share variance dependent variable, also shares variance among independent variables. Understanding shared variance crucial interpreting results multiple regression analysis.instance, say using three independent variables predict well-: exercise, age, income. variables contribute variance well-.add variables one--one, overlap independent variable circle well-circle represents unique shared variance variable contributes well-.adding samples age data, help us explain well-, see significant correlations three different relationships.\n1. positive correlation exists exercise well-,\n2. negative correlations exist age well-\n3. negative correlation age exercise.\\(R^2\\) derived squaring r value, square negative value, loses negative sign.age exercise now explain variability well-, since also related , variance explain overlaps.Finally, add income, find variable shares variance age well-. Income therefore adds variance explained model also overlapping variance explained ageThe outcome variable add allowed us better explain concept well-. , know person's amount exercise, age, income predict well-greater degree just know amount exercise.important understand Venn diagrams guide offer simplified representation variance explained predictor variables regression model. reality, two predictor variables overlapping variance, shared variance can contribute model individual variables might suggest. occurs due interaction effects, combined influence two predictors can explain variance dependent variable sum individual effects. looking Venn diagrams, remember additional explained variance interactions explicitly depicted, crucial consider interpreting regression results., see later , final \\(R^2\\) greater sum simple (univariate) regressions.","code":""},{"path":"multiple-regression.html","id":"the-regression-equation","chapter":"16 Multiple regression","heading":"16.2 The Regression Equation","text":"probably realised, using Venn diagrams illustrate relationships among multiple variables can get quite confusing, especially numerous variables degree correlation . regression equation comes handy.regression equation mathematical representation relationships among variables multiple regression analysis. shows dependent variable predicted independent variables. abstract form, regression equation can written :\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_kx_k + \\varepsilon \\], \\(y\\) dependent variable, \\(x_1\\), \\(x_2\\),...,\\(x_k\\) independent variables (.e. amount exercise, age, income participants), \\(\\beta_0\\) intercept, \\(\\beta_1\\), \\(\\beta_2\\),...,\\(\\beta_k\\) coefficients independent variables, \\(\\varepsilon\\) error term, representing unexplained variability dependent variable (.e. left white space Venn diagram figure ).intercept \\(\\beta_0\\) represents value dependent variable independent variables zero. \\(\\beta_0\\) equivalent intercept, \\(c\\), value equation last week.coefficients (remaining \\(\\beta\\) values) represent average change dependent variable one-unit change respective independent variable, holding variables constant. Just like gradient line \\(m\\) value equation last week. \"...\" \\(k\\)'s just way saying \"add many beta values variables model\".Running regression analysis allowed us substituent numbers coefficients degree left \\(x\\)'s, can use sub data.","code":""},{"path":"multiple-regression.html","id":"interpreting-a-regression-model","chapter":"16 Multiple regression","heading":"16.3 Interpreting a regression model","text":"explain steps running regression model, full, weeks JASP workshop. now, want us just focus interpretation. JASP output model uses variables exercise, age income predict well-scores.break table.","code":""},{"path":"multiple-regression.html","id":"descriptive-statistics","chapter":"16 Multiple regression","heading":"16.3.1 Descriptive statistics","text":"descriptive statistics useful context. detail given can tell dataset (N=1000), well-score 100, exercise variable based number hours exercise week, income £GBP, age years.","code":""},{"path":"multiple-regression.html","id":"model-summary","chapter":"16 Multiple regression","heading":"16.3.2 Model Summary","text":"Model Summary table provides overview overall performance regression model. contains key statistics helps us assess well model fits data.table contains two rows \\(H_0\\) \\(H_1\\). \\(H_0\\) refers null model, model assumes none predictors model effect dependent variable, meaning coefficients predictors equal zero. \\(H_1\\) alternative model, model containing variables. line interests us.main details interest :Adjusted \\(R^2\\) Value: modified version \\(R^2\\) takes account number independent variables model. allows us say much variance dependent variable explained predictor variables.Adjusted \\(R^2\\) Value: modified version \\(R^2\\) takes account number independent variables model. allows us say much variance dependent variable explained predictor variables.RMSE: RMSE stands Root Mean Square Error. measure differences values predicted model values actually observed. RMSE measure spread residuals . words, tells concentrated data around line best fit. used \\(\\varepsilon\\) regression model.RMSE: RMSE stands Root Mean Square Error. measure differences values predicted model values actually observed. RMSE measure spread residuals . words, tells concentrated data around line best fit. used \\(\\varepsilon\\) regression model.often take Adjusted \\(R^2\\) multiple regression \\(R^2\\) simple regression (regression just one predictor variable). well-model, Adjusted \\(R^2\\) 0.469 tells us three variables together explain 46.9% variability well-. pretty good.","code":""},{"path":"multiple-regression.html","id":"anova","chapter":"16 Multiple regression","heading":"16.3.3 ANOVA","text":"ANOVA table provides information overall significance model. shows whether model better predicting dependent variable model independent variables (.e., null model).findings table need reported write-, however, interpret , actually need p-value. significant finding test tells us model significantly better null model.ANOVA stands ANalysis VAriance. come back ANOVA week 4. now, just think slightly fancier t-test, test compares across groups (rather correlates). case comparing null model alternative model seeing significantly different.","code":""},{"path":"multiple-regression.html","id":"coefficients","chapter":"16 Multiple regression","heading":"16.3.4 Coefficients","text":"Finally, understand role variable model need look coefficients table.table provides detailed information individual variables model. includes statistics independent variable intercept model.two columns key understanding model. Unstandardized column p column.Starting p column, probability obtaining t-value extreme one observed null hypothesis true. small p-value indicates variable plays statistically significant role model. case include variable related dependent variable variance explains accounted variables model. cases, p-value 0.05. p-value <.001 three variables indicates relevant include model.values Unstandardized column correspond \\(\\beta\\) values regression equation. play role gradient line best fit simple regression last week. represent absolute change dependent variable one-unit change independent variable, holding variables constant.model, table tells us :every 1 hour increase exercise week, well-increases 1.88 points.every 1 year ageing, well-decreases 1.09 points.every £1 income, well-increased 0.001 points.last finding intuitive , makes sense convert income values 1000's pounds (.e. someone earns £32500 year can said earn 32.5 thousand pounds year). need divide income values 1000 run model .gives us :every £1000 income, well-increases 1.35 points.","code":""},{"path":"multiple-regression.html","id":"building-our-regression-equation-from-the-model","chapter":"16 Multiple regression","heading":"16.4 Building our regression equation from the model","text":"can now start building regression equation. three variables predicting well-need fill \\(\\beta\\) values \\(\\varepsilon\\) following equation.\\[ Wellbeing = \\beta_0 + \\beta_1income + \\beta_2age + \\beta_3exercise + \\varepsilon \\]\\(\\beta_0\\) = unstandardized (beta) value intercept \\(H_1\\) = 35.25\\(\\beta_1\\) - \\(\\beta_3\\) = unstandardized (beta) value three variables = 1.35, -1.09 1.88\\(\\varepsilon\\) = Root Mean Square Error (RMSE) \\(H_1\\) = 15.7This give us following regression equation:\\[ Wellbeing = 35.25 + 1.35income -1.09age + 1.88exercise + 15.7\\]equation, knew individual's income, age much exercise engaged per week estimate well-fairly high degree accuracy.Alongside equation important note Adjusted \\(R^2\\) 0.469, indicate means half variability well-explained variables included model.","code":""},{"path":"multiple-regression.html","id":"so-what","chapter":"16 Multiple regression","heading":"16.5 So what?","text":"point work? Well, wanting increase well-local community finding might put putting money community exercise classes. likely benefits intervention, many hours week , based data, 1.88 point increase well-(scale 100) extra hour exercise might cost-effective way spending limited pool money., like can stop someone ageing, increase income. Perhaps research compares effects , example, exercise, mindfulness diet, controlling demographic factors, useful inform type intervention might cost-effective.","code":""},{"path":"multiple-regression.html","id":"week-2---test-yourself-mcqs","chapter":"16 Multiple regression","heading":"16.6 Week 2 - Test yourself mcq’s","text":"multiple regression?\n\nUsing single categorical variable predict value dependent variable.method helps predict outcome variable based one independent variable.method helps predict outcome variable based several independent variables.measure association two variables.\nregression equation?\n\ngraphical representation relationships among variables.mathematical calculation shared variance.mathematical representation relationships among variables multiple regression analysis.empirical observation relationships among variables.\nregression equation, β values represent?\n\ndependent variable.coefficients independent variables.error term.independent variables.\nvalues Unstandardized column coefficients table represent?\n\nshared variance variables.average change dependent variable one-unit change respective independent variable, holding variables constant.correlation coefficient variables.absolute value correlation coefficient variables.\nAdjusted \\(R^2\\) value tell us multiple regression analysis?\n\nstrength linear relationship dependent variable independent variable.shared variance among independent variables.percentage variability dependent variable explained predictor variables.effect size independent variables dependent variable.\n","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"statistical-assumption-checks-for-multiple-regression","chapter":"17 Statistical assumption checks for multiple regression","heading":"17 Statistical assumption checks for multiple regression","text":"point studies, important understand assess report statistical assumptions underlying regression analysis. However, advanced stages career learn address violations assumptions.now, just interpret report assumptions. results deviate greatly assumptions, important report deviations state findings \"taken caution\".key assumptions multiple regression check :","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"check-for-linearity","chapter":"17 Statistical assumption checks for multiple regression","heading":"17.1 Check for Linearity","text":"assumption linearity states relationship independent dependent variables linear. can check visually examining scatter plots independent variables dependent variable.","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"check-for-multicollinearity","chapter":"17 Statistical assumption checks for multiple regression","heading":"17.2 Check for Multicollinearity","text":"Multicollinearity occurs two independent variables regression model closely related, making difficult disentangle individual effects dependent variable. can lead unstable coefficient estimates, small changes data can result significant changes estimates. can assess multicollinearity creating correlation matrix independent variables looking variables correlation coefficient greater 0.7 less -0.7. However, strict rule, just indication strong correlation.table , suppose use five personality constructs predict behaviour, recidivism. Checking correlation coefficients, largest value -0.368, indicating multicollinearity likely issue case. mindful confuse correlation coefficients p-values, easy mistake make.multicollinearity can make interpretation challenging, necessarily deal-breaker. cases, focus may predicting outcome rather interpreting individual contributions predictor. -depth discussion multicollinearity implications see Vatcheva et al (2016).","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"check-for-outliers","chapter":"17 Statistical assumption checks for multiple regression","heading":"17.3 Check for Outliers","text":"Outliers data points differ significantly observations. can detected various methods. boxplot, outliers typically displayed individual points outside whiskers, extend first third quartiles (Q1 Q3) Q1 - 1.5 * IQR (interquartile range) Q3 + 1.5 * IQR, respectively (see link full explanation read boxplot).JASP can automatically identify outliers boxplots, making easier spot . However, addressing outliers often contentious issue treatment can significant impact results statistical analysis. decision keep remove outliers depends multiple factors.First, important consider context data. Outliers result measurement errors, data entry errors, random anomalies. However, also represent genuine extreme values offer valuable insights. example, study wealth distribution, billionaires outliers real-world phenomenon might important research.Secondly, outliers can heavily influence data assumptions, linearity, normality residuals, homoscedasticity. crucial evaluate whether outliers affecting assumptions justify removal retention. , fact, can away addressing violations assumptions listed chapter.Thirdly, outliers can disproportionately affect key statistical measures like mean standard deviation. easy fall trap removing outliers significant results appears. kind practice may fall grey area academic misconduct discussed first year, known Questionable Research Practices.Ultimately, decision handle outliers involves balance statistical judgment domain expertise. necessary ensure decision involves transparently reporting justifying decisions.","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"checks-of-residuals","chapter":"17 Statistical assumption checks for multiple regression","heading":"17.4 Checks of residuals","text":"last two assumption checks require examination residual. Residuals fundamental concept regression analysis, representing difference observed predicted values dependent variable. simple regression, visualised predicted value line best fit, observed value just data point relationship.observation, residual calculated subtracting predicted value observed value. simple linear regression, one independent variable, residual observation distance data point regression line. multiple regression, distance observation regression plane.goal regression analysis minimize residuals, words, make predicted values close possible observed values. Residuals can used evaluate fit regression model, detect outliers, check violations assumptions regression.","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"check-for-normality-of-residuals","chapter":"17 Statistical assumption checks for multiple regression","heading":"17.4.1 Check for Normality of Residuals","text":"assumption normality residuals important hypothesis testing linear regression. ensures standard errors coefficients unbiased significant testing coefficients valid. , turn, helps us make valid inferences relationships variables.contrast, checking normality individual variables required linear regression. Instead, focus distribution residuals, normally distributed model's assumptions met. residuals represent differences observed predicted values dependent variable, distribution tells us model's fit relationships variables.working program can calculate residuals, can check normality residuals using Shaperio Wilks test assessing skewness kurtosis. JASP means checking normality residuals normality residuals plot option.","code":""},{"path":"statistical-assumption-checks-for-multiple-regression.html","id":"check-for-homoscedasticity","chapter":"17 Statistical assumption checks for multiple regression","heading":"17.4.2 Check for Homoscedasticity","text":"Homoscedasticity, word strikes fear undergrad (postgrad) psychology students throughout land, complicated sounds.\"Homos\": Greek \"\" \"equal.\" context homoscedasticity, refers equality sameness variances.\"Homos\": Greek \"\" \"equal.\" context homoscedasticity, refers equality sameness variances.\"Skedasis\": Greek word related concept dispersion spreading . statistics, associated variance scatter data points.\"Skedasis\": Greek word related concept dispersion spreading . statistics, associated variance scatter data points.scatter plots demonstrate data might look. Heteroscedasticity (.e. different \"spreadness\") means line best fit becomes less reliable different points model.context, looking equal distribution data points along line best fit, avoiding funnel-shaped pattern. dealing regression analyses involve two predictor variables, often feasible visualise data. cases, plot predicted variables residuals can used check homoscedasticity. plot, , , looking absence funnel shape points.check can also performed using z-scores predicted values residuals; level line context indicate homoscedasticity (uniform spread residuals). method used software like SPSS checking homoscedasticity, may see guides talking read wider topic.","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"jasp-workshop---multiple-regression","chapter":"18 JASP Workshop - Multiple regression","heading":"18 JASP Workshop - Multiple regression","text":"JASP free, open-source statistical software package user-friendly, point--click interface suitable research psychology. offers wide range statistical analyses, basic descriptive statistics advanced methods like regression ANOVA.JASP available university computers, however, recommend also install personal version laptop desktop computer can continue learning outside class time.JASP can downloaded : www.jasp-stats.org/","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"multiple-regression---example-analysis","chapter":"18 JASP Workshop - Multiple regression","heading":"18.1 Multiple regression - Example analysis","text":"video go analysis answers Multiple Regression Exercise 1. like follow along (go exercise first) question sheet data can found Week 2 module area NS5108.","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"where-to-click-guide---multiple-regression","chapter":"18 JASP Workshop - Multiple regression","heading":"18.2 \"Where to click\" guide - Multiple Regression","text":"Open JASP.Open JASP.Load Data: click File tab top left select Open. navigate folder containing data file open .Load Data: click File tab top left select Open. navigate folder containing data file open .Identify variable predictors dependent variable (variable aiming predict).Identify variable predictors dependent variable (variable aiming predict).Check linear relationships predictor variables: Click Regression -> Correlation top bar. Move predictor variables dependent variable Variables Box. Click Scatter plots options. Visually interpret Correlation plots determine predictor variables linear relationship dependent variable.Check linear relationships predictor variables: Click Regression -> Correlation top bar. Move predictor variables dependent variable Variables Box. Click Scatter plots options. Visually interpret Correlation plots determine predictor variables linear relationship dependent variable.Check multicollinearity: Within results section step 3 check correlation table. Interpret correlation coefficients predictor variables large enough indicate issue multicollinearity.Check multicollinearity: Within results section step 3 check correlation table. Interpret correlation coefficients predictor variables large enough indicate issue multicollinearity.Check normality (residuals): Click Regression -> Linear Regression top bar. Pick dependent variable place Dependent variable box. Pick predictor variables place Covariates box. Plots drop select Residuals histogram. Visually interpret histogram indication assumption check.Check normality (residuals): Click Regression -> Linear Regression top bar. Pick dependent variable place Dependent variable box. Pick predictor variables place Covariates box. Plots drop select Residuals histogram. Visually interpret histogram indication assumption check.Check Homoscedasticity: Also Plots drop , select Residuals vs. predicted. Visually interpret plot assess Homoscedasticity.Check Homoscedasticity: Also Plots drop , select Residuals vs. predicted. Visually interpret plot assess Homoscedasticity.Extract Adjusted \\(R^2\\) value: first table Linear Regression section.Extract Adjusted \\(R^2\\) value: first table Linear Regression section.Determine significance model: can obtained though ANOVA table. F(df,df)=F-value, p=p-value.Determine significance model: can obtained though ANOVA table. F(df,df)=F-value, p=p-value.Extract values model regression equation: unstandardized values coefficients table. See report findings.Extract values model regression equation: unstandardized values coefficients table. See report findings.","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"jasp-lab-exercises-2","chapter":"18 JASP Workshop - Multiple regression","heading":"18.3 JASP lab exercises","text":"two additional exercises Moodle. exercise question sheet comes dataset answers sheet. Work question check answers answer sheet.","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"apa-style-guide-for-multiple-regression","chapter":"18 JASP Workshop - Multiple regression","heading":"18.4 APA Style Guide for Multiple Regression","text":"","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"hypothesis-for-multiple-regression-analysis","chapter":"18 JASP Workshop - Multiple regression","heading":"18.4.1 Hypothesis for Multiple Regression Analysis","text":"Null Hypothesis (H0): Neither self-esteem study hours significant predictors academic performance.Alternative Hypothesis (H1): least one self-esteem study hours significant predictor academic performance.","code":""},{"path":"jasp-workshop---multiple-regression.html","id":"reporting-multiple-regression-in-apa-style","chapter":"18 JASP Workshop - Multiple regression","heading":"18.4.2 Reporting Multiple Regression in APA Style","text":"multiple linear regression conducted explore impact self-esteem study hours academic performance. Assumptions linearity, independence, multicollinearity, normality checked met.multiple regression model significantly predicted academic performance, F(2, 97) = 36.8, p < .001, \\(R^2\\) = .43.regression equation found : Academic Performance = 45.2 + 4.1 * Self-Esteem + 5.2 * Study Hours.self-esteem study hours significantly added prediction academic performance. Specifically, self-esteem significant predictor, t(97) = 2.9, p = .005, contributed increase 4.1 points every unit increase self-esteem. Study hours also significant predictor, t(97) = 5.1, p < .001, contributed increase 5.2 points every additional hour spent studying.Overall, model explained approximately 43% variance academic performance (R² = .43)., null hypothesis can rejected, indicating least one predictors significantly impacts academic performance.","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"19 Logistic regression","heading":"19 Logistic regression","text":"[Chapter currently construction]chapter look final regression technique, logistic regression. Unlike linear regression, predicts continuous outcome variable based one predictor variables, logistic regression used outcome variable categorical. common form logistic regression binary logistic regression, outcome limited two categories Yes , Diagnosis Diagnosis, 1 0.instance, imagine interested exploring factors predict whether individuals likely high blood pressure. linear regression help predict something continuous like systolic blood pressure measurments, logistic regression help predict whether someone likely diagnosis hypertention (high blood pressure) .Logistic regression uses mathematical function transforms linear input probability 0 1. logistic function \"S\" shape, allowing smoothly transition two extremes.","code":""},{"path":"logistic-regression.html","id":"modelling-a-logistic-function","chapter":"19 Logistic regression","heading":"19.1 Modelling a logistic function","text":"S-curve logistic regression essentially plotting probability event occurring across range predictor variable. example plotted stress level (scale 0 50) probability individual relapsing particular damaging behaviour (.e, drug taking, smoking etc).curve add relationship works similarly line best fit datapoints first used simple regression chapters. want work chance relaps individual scores 30 stress variable need trace point 30 line read corresponding y-axis point. case value around 0.6 indicating , according model, scores 30 scale indicates individual 85% estimated probability experiencing relapse.","code":""},{"path":"logistic-regression.html","id":"understanding-odds-ratios-in-the-context-of-logistic-regression","chapter":"19 Logistic regression","heading":"19.2 Understanding odds ratios in the context of logistic regression","text":"previous section talked probability event (probabilities 0 1) however can also talk data terms odds, particularly odds relaps event increase function increase stress variable.Odds ratios calculated coefficients (\\(\\beta\\)) logistic regression model (JASP calculates automatically tick odds ration button wont go maths ). odds ratio greater 1 indicates predictor variable increases, odds outcome occurring also increase. Conversely, odds ratio less 1 indicates predictor increases, odds outcome occurring decrease.case current model odds ratio 1.29 indicating every unit increase stress chance relaps increases 29%.Possible data set: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0255683#sec019[Chapter currently construction]","code":""},{"path":"further-reading.html","id":"further-reading","chapter":"20 further reading","heading":"20 further reading","text":"Beyond multiple linear regression: https://bookdown.org/roback/bookdown-BeyondMLR/Introduction modern statistics: https://openintro-ims.netlify.app/","code":""}]
