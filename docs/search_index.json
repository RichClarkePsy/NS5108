[["index.html", "NS5108 handbook Chapter 1 Welcome", " NS5108 handbook Dr Richard Clarke 2023-08-15 Chapter 1 Welcome Welcome to the R Handbook for NS5108 "],["introduction.html", "Chapter 2 Introduction 2.1 Data-driven decision making. 2.2 Further real world examples 2.3 Use of regression analysis in psychology 2.4 Overview of regression techniques covered in this module", " Chapter 2 Introduction Regression analysis is a powerful statistical tool that investigates the relationships between variables. At its core, it seeks to understand and quantify how one variable (the criterion variable) changes in response to another or several others (predictor variables). More than merely determining a singular correlation, regression analysis offers a nuanced lens, enabling researchers to predict outcomes and uncover sophisticated patterns embedded within datasets. This method provides a richer understanding of the multifaceted interactions that underpin psychological phenomena. 2.1 Data-driven decision making. The 2011 film \"Moneyball\" is based on the true story of the Oakland Athletics baseball team's 2002 season. Their general manager, Billy Beane (played by Brad Pitt in the film), faced a significant problem: he had a limited budget to put together a winning team. Rather than relying on traditional baseball scouting methods, which often depended heavily on scouts' intuitions and were prone to various biases, Beane employed the skills of a young Yale economics graduate named Peter Brand (played by Jonah Hill). Brand used statistical analysis to evaluate players' values. The traditional method of valuing players was subjective. Scouts often looked at the physique of players, their style, how they moved, or even things like the attractiveness of their girlfriends as an indicator of their confidence. Beane and Brand shifted the focus to objective evidence, including statistics. One of the major statistics Brand focused on was a player's on-base percentage. This metric looks at how often a batter reaches base, whether through a hit, walk, or being hit by a pitch. Rather than paying high prices for stars, they could recruit undervalued players with good OBP, which could lead to more runs and, consequently, more wins. At its core, the analytics used in \"Moneyball\" is about predicting runs, and more importantly, wins. Using regression analysis, they could determine which statistics were most strongly correlated with creating runs. Once they had an understanding of what leads to runs, they could use that to build a model to aid them on player acquisition decisions. The film highlights the tensions that arise when data-driven metrics clash with entrenched traditional norms. Yet, as time progresses, the efficacy of these analytical techniques becomes increasingly evident. Today, it's a rarity to encounter a professional sports team that doesn't incorporate some form of statistical analysis into its strategy and decision-making processes, showcasing the undeniable impact and relevance of regression in our modern world. 2.2 Further real world examples Of course, its not just the world of sports that have taken advantage of regression analysis to inform decision making. Here are some examples from everyday life where regression analysis has played a role. Streaming Services and Recommendations: Most of you probably use platforms like Netflix, Spotify, or YouTube. These platforms utilise regression analysis to predict what shows, songs, or videos you might like based on your past behaviour and the behaviour of others with similar tastes. Predicting Box-office Income from Different Forms of Advertising: Film producers often use regression analyses to gauge which adverts, from TV spots to social media campaigns, most influence cinema ticket sales, optimising advertising budgets for upcoming films based on these insights. Price Sensitivity of the Public for a Particular Product: A coffee chain might employ regression analysis to determine the optimal price for its lattes, balancing customer demand against profit margins, ensuring both consistent footfall and profitability. Predicted Crop Yield Based on Weather Conditions: Regression analysis is used extensively in farming to predict something like corn yields based on weather forecasts, and then adjusting farming strategies in anticipation of conditions like a particularly dry summer. 2.3 Use of regression analysis in psychology Regression analysis is also a cornerstone of psychological research, as it allows psychologists to simultaneously explore and dissect the influence of numerous variables on a single outcome variable. Such a comprehensive approach is indispensable in a field like psychology, where behaviours and mental processes are often the outcome of a web of interconnected variables. For instance: Cognitive Development in Children: Researchers might use regression analysis to determine how variables such as hours of sleep, nutritional intake, and classroom environment influence cognitive development scores in children. Efficacy of Therapeutic Interventions: In clinical psychology, regression might be employed to predict the effectiveness of different therapeutic interventions, considering variables such as the duration of therapy, patient's initial symptom severity, and therapist experience. Predictors of Job Satisfaction: Organizational psychologists could use regression to determine which factors (e.g., salary, working hours, team dynamics, or leadership style) are the most significant predictors of job satisfaction among employees. Influence on Social Behaviours: Social psychologists might employ regression analysis to understand how various factors like media exposure, peer influence, and past experiences predict certain social behaviours or attitudes, such as aggression or altruism. Test yourself activity Reflect on a psychological phenomenon or behaviour that has piqued your interest or that you've recently studied. Jot down potential factors or variables that you believe might influence this phenomenon or behaviour. Consider how these factors might be integrated into a regression model. Which variable would you choose as the criterion variable (the one youll be predicting)? Which ones might serve as the predictors? 2.4 Overview of regression techniques covered in this module In this module we will cover three main regression techniques: Simple linear regression (basically just a fancy correlation) Multiple linear regression (the technique that is required for your assignment) Binary logistic regression (multiple regression with a binary, instead of continuous, criterion variable) Continue with statistics in psychology and youll learn further forms of regression, and regression like, analysis techniques such as: Ordinal regression Mediation and moderation Multi-level modelling Structural equation modelling "],["simple-linear-regression.html", "Chapter 3 Simple linear regression 3.1 Recap of correlation 3.2 Simple linear regression 3.3 Week 1 - Test yourself mcqs", " Chapter 3 Simple linear regression 3.1 Recap of correlation At its core, correlation provides a measure of how data points on two variables are related to one another. Positive Correlation: (top left figure) As one variable increases, the other also increases. Similarly, as one decreases, the other also decreases. Negative Correlation: (top right figure) As one variable increases, the other decreases, or vice versa. No Correlation: (bottom figure) No discernible pattern in the relationship between the two variables. The strength and direction of the correlation are represented by the correlation coefficient, typically denoted as \\(r\\). The value of \\(r\\) ranges from -1 to +1. An \\(r\\) value close to +1 indicates a strong positive correlation. An \\(r\\) value close to -1 indicates a strong negative correlation. An \\(r\\) value close to 0 suggests little to no correlation. 3.1.1 Correlation as a Statistical Test Going beyond visualisation of relationships, correlation can also be used as a formal statistical test to determine if there's a significant linear relationship between two variables. When conducting a correlation test, the tests p-value informs us about the significance of our observed correlation coefficient (\\(r\\)). If the p-value is below a predetermined threshold (commonly 0.05), we infer that the correlation in our sample is likely not due to random chance, and thus, there's a statistically significant relationship between the two variables. Reminder: what is a p-value Surface Level Explanation: A p-value is a number between 0 and 1 that tells us if the result of an experiment is likely due to chance or if there's something more going on. A small p-value (typically less than 0.05) suggests that the result is significant and not just a random occurrence. Intermediate Explanation: A p-value represents the probability of observing the data (or something more extreme) given that a specific null hypothesis is true. If we have a p-value less than a pre-decided threshold (like 0.05), we reject the null hypothesis in favour of the alternative hypothesis. This suggests that our observed data is unlikely under the assumption of the null hypothesis. However, a smaller p-value does not necessarily mean a result is meaningful; it just indicates it's statistically significant. In-Depth Explanation: Mathematically, the p-value is the probability of observing data as extreme as, or more extreme than, the observed data under the assumption that the null hypothesis is true. This is not a direct measure of the probability that either hypothesis is true. Instead, it's a measure of the extremity of the data relative to a specific model. Lower p-values suggest that the observed data are less likely under the null hypothesis, leading us to reject then null in favour of alternative hypothesis. However, it's crucial to understand that a p-value doesn't measure the size of an effect or the practical significance of a result. Furthermore, while a threshold of 0.05 is common, it's arbitrary and must be chosen with context and caution. Lastly, it's essential to remember the p-value is contingent on the correctness of the underlying statistical model and the level to which the data meet the statistcial assumptions. Misunderstandings and misuse of p-values have led to various controversies in the scientific community (see the replication crisis). See Lakens (2022) Improving Your Statistical Inferences for a comprehensive overview of p-values. 3.1.2 Statistical assumptions of correlation In frequentist statistical analysis, we often aim to use parametric tests when the statistical assumptions underlying these tests are met, as these tests can be more sensitive to effects if their assumptions hold. For correlation analysis, Pearson's correlation is the most commonly used parametric test and Spearman's Rank correlation is the most commonly used non-parametric test. Assumption checks for correlation analysis Assumption checks are often the part of statistics that confuse students the most. For now, I've put them safely away in this box, so as not to scare you off in the first week :-) We will do a fully refresh of the theory behind assumption check aspect next week when we cover multiple regression. For correlation analyse the the following assumptions should be checked and assessed: Linearity: Both variables should have a linear relationship, which means that when you plot them on a scatterplot, the distribution of data points should roughly form a straight line. (Bivariate) Normality: The data pairs should be bivariately normally distributed, meaning both variables being analysed should be approximately normally distributed when considered together. We do this by checking the normality of the residuals. Homoscedasticity: The the spread (variance) of the residuals are constant across all values of the variable on the x-axis. i.e. data points shouldn't start of clustered and the widen out (like a funnel) further along in the correlation. Absence of Outliers: Outliers can disproportionately influence the correlation coefficient. So, it's important to check for and consider the impact of any outliers in the data. Outliers can be checked through the use of a boxplot or calculated manually. 3.2 Simple linear regression The content in the sections above, likely reflects the extent to which you explored correlation in your first year here with us. Regression analysis is simply an extension for these concepts. Central to this understanding is the \"line of best fit.\" While you might have previously used this line as a visual representation of the relationship between two variables, in linear regression, it takes on a more pivotal role. 3.2.1 The line of best fit In the scatter plot below, you'll see data points representing the hours of sleep and test performance for various participants. Notice how the data points are positively correlated? We can capture this trend by drawing a straight line through the data. This line, which we call the \"line of best fit,\" gives us a simplified representation of the relationship between hours of sleep and test performance.The optimal line of best fit is the one that minimises the total distance between itself and all the individual data points on the plot. The Concept of Residuals Even with the best possible straight line drawn through our data points, it's rare that the line will pass exactly through every point. The vertical distance between each data point and our line is called a \"residual\". For every data point, the residual is the difference between its actual value and what our line predicts the value should be. If our line of best fit is doing its job well, these residuals will be quite small, indicating that our predictions are close to the actual data. However, if the residuals are large, it suggests that our line might not be capturing the relationship between the two variables adequately. We talk more about residuals, in the context of our assumption checks, in the next chapter. The purpose of the line of best fit is to model the relationship between test performance and number of hours slept. What does this model suggest the performance on the test will be if a participant gets 7 hours sleep? 100110120130140 Hint Find the point for 7 hours on the x-axis. Draw your finger up to the blueline. Draw your finger across to the y-axis. Take that number. That is the test score that our model suggest for a participant that has 7 hours of sleep. 3.2.2 The equasion of a line Every straight line on a scatter plot can be written as a simple formula: \\[ Y = mX + c \\] Where: \\(Y\\) is the dependent variable. \\(X\\) is the independent variable. \\(c\\) is where the line intersects the Y-axis, representing the predicted test performance when no hours of sleep are had. \\(m\\) is the slope of the line, indicating the predicted change in test performance for each additional hour of sleep. To make the formula for our previous scatter plot all we need is to work out how much the blue line \"goes up\" by how far it \"goes along\", and where the line crosses our y-axis. The below scatter plot is the same data but with each axes going to zero this time. This allows us to get our \\(c\\) value, 50, and by taking any two points on the line we can work out our \\(m\\) value, in this case 10 (60/6). This give us the following equasion of our line: \\[ y = 10x + 50 \\] Look back at the question from before. All we need to do now is sub in the number 7 and we get our predicted test score: (10*7) + 50 = 120 3.2.3 Is this a good model? \"All models are wrong, but some are useful.\" - George E. P. Box This oft-cited quote sums up the a central truth in statistics and data modelling: no model can capture the full intricacy and unpredictability of real-world phenomena. However, that doesn't diminish the value of models. When a model simplifies complex systems and highlights important relationships, it can offer invaluable insights and guide decision-making. The line on our sleep and test score scatter plot (above) would appear to be quite representative of our data and therefore is perhaps fairly a good model (so long as our sample is representative of the population we want to use or model on in the future). However, if we try and use the relationship between participants height and their test scores as out model, this is likely to be a fairly poor model: The model derived from this data, \\(y = -0.05x + 77.89\\), would actually give an answer close to 70 no matter an individuals height. In other words, whether you're taller or shorter, the model essentially shrugs and predicts something close to 70 (the mean of the dataset). I would not describe this as a very useful model, as in, I would not spend an evening on the rack to try and make myself taller for the test. While we can see it on the scatter plot, none of the numbers in our equations allow us to gauge how 'useful' or fitting the sleep model is compared to the height model. To understand model fit we need the final new concept for this week, shared variance 3.2.4 Shared Variance When examining the relationship between two variables, the proportion of variation in one variable that can be predicted or explained by the other variable is known as the shared variance . In correlational research, this concept is crucial as it tells us how well one variable explains the variation in the other variable. In other word how useful would it be if we used a scores from one variable to predict scores on the other variable. A good way to think about this visually is with a Venn diagram. This value of shared variance is derived by taking the square of \\(r\\), the correlation coefficient, giving us our \\(R^2\\) value for the model. Take for instance, for our sleep model, if we run a pearsons correlation test on the data we find the relationship has a correlation coefficient of \\(r\\) = 0.94, squaring this give us a \\(R^2\\) of 0.89 between our predictor (study hours) and the outcome (test scores). Another way to talk about \\(R^2\\) is to say that 89% of the variance in test scores can be predicted or explained by the number of hours studied. The remaining 11% of the variance is due to factors not included in our model or random error. When we say two variables share a certain percentage of variance, it's a indication of the strength and utility of the relationship. However, while a high shared variance can be promising, it's essential to remember that correlation does not imply causation. Other underlying factors, third variables, or even coincidences could create correlations. In practical terms, understanding shared variance is critical for researchers. When a significant shared variance exists, the predictor variable becomes valuable in understanding, predicting, or even potentially influencing the outcome variable. However, the unexplained variance might also prompt researchers to consider additional predictors or factors that weren't initially in the model. Next week we'll start looking at multiple regression modeling, which as the name suggest involves multiple variables sharing variance with a dependent variable. 3.3 Week 1 - Test yourself mcqs 1. Which of the following best describes correlation? The probability of one event occurring given another has occurred. The relationship between the means of two groups. The strength and direction of a linear relationship between two variables. A statistical test to determine significance. 2. A negative correlation coefficient between two variables indicates: there's no relationship between the two variables. as one variable increases, the other decreases. both variables increase together. the data is scattered without any clear trend. 3. Which statistical method is used to predict the value of one variable based on the value of another variable? T-test. ANOVA. Simple linear regression. Chi-square test. 4. In the context of a correlation test, the p-value informs us: about the strength of the correlation. about the slope of the line of best fit. whether the observed correlation is likely due to random chance. about the effect size of the relationship. 5. What does the \"line of best fit\" represent in the scatter plot? The line that maximizes the total distance between itself and the data points. The line that minimizes the total distance between itself and the data points. The line that represents the mean of the data points. The line that represents the mode of the data points. 6. In the formula \\(Y=mX+c\\), what does \\(c\\) represent? The slope of the line. The predicted change in test performance for each additional hour of sleep. Where the line intersects the Y-axis. The independent variable. 7. What is the shared variance in a simple linear regression? The proportion of variation in one variable that can be predicted by another variable. The difference in variance between two variables. The total variance of both variables. The square root of the variance of both variables. 8. How is the shared variance value derived? By taking the cube of the correlation coefficient. By taking the square root of the correlation coefficient. By dividing the correlation coefficient by 2. By taking the square of the correlation coefficient. "],["week-1---jasp-workshop---simple-linear-regression.html", "Chapter 4 Week 1 - JASP Workshop - Simple linear Regression 4.1 Simple linear regression - example analysis 4.2 \"Where to click\" guide - simple linear regression analysis 4.3 JASP lab exercises 4.4 APA style guide", " Chapter 4 Week 1 - JASP Workshop - Simple linear Regression JASP is a free, open-source statistical software package with a user-friendly, point-and-click interface suitable for research in psychology. It offers a wide range of statistical analyses, from basic descriptive statistics to advanced methods like regression and ANOVA. JASP is available on most of the university computers, however, we recommend that you also install a personal version for your own laptop or desktop computer so that you can continue learning outside of class time. JASP can be downloaded here: www.jasp-stats.org/ 4.1 Simple linear regression - example analysis In this video I go through the analysis and answers for Simple Linear Regression Exercise 1. If you would like to follow along (or have a go at the exercise yourself first) the question sheet and data can be found in the Week 1 module area for NS5108. 4.2 \"Where to click\" guide - simple linear regression analysis The following is a step by step guide for performing a correlation and simple linear regression analysis in JASP. Watch the video above for more context related to these steps. 4.2.1 Correlation Open JASP. Load the Data: click on the File tab at the top left and select Open. Then navigate to the folder containing your data file and open it. Visualise the data: Identify the two variables you want to run your correlation on. Click Descriptives and move the two variables over to the Variables box. Below, in the Customizable plots drop down tick the Scatter plots. You may want to click none for the graphs above and to the right (to make your plot clearer). Also change that regression line (line of best fit) to linear. Identify outliers: Also in the customizable plots drop down, select boxplots. Tick label outliers if they show any dots outside of the wihiskers for the box plots. Note and interpret any outliers. Calculate correlation coefficent: click Regression -&gt; Correlation from the top bar. Pick the variables you are interested in running a correlation on. Extract and interpret the Pearson's \\(r\\) and p-value. 4.2.2 Simple linear regression Check normality (of residuals): Click Regression -&gt; Linear Regression from the top bar. Pick your criterion variable (the variable you are aiming to predict) and place this in the Dependent variable box. Pick your predictor and place that in the Covariates box. In the Plots drop down select Residuals histogram. Visually interpret this histogram for an indication of this assumption check. Check Homoscedasiticity: Also in the Plots drop down, select Residuals vs. predicted. Visually interpret this plot to assess Homoscedasiticity. Extract the \\(R^2\\) value: This should be in the first table of the Linear Regression section. Determine the significance of the model: This can be obtained though the ANOVA table. F(df,df)=F-value, p=p-value. Extract values for your model regression equation: H1 intercept = \\(c\\), H1 variable = \\(m\\) 4.3 JASP lab exercises There are two additional exercises on Moodle. Each exercise question sheet comes with a dataset and answers sheet. Work through the question and then check your answers with the answer sheet. 4.4 APA style guide Here is an example of a hypothesis for a correlational analysis: Null Hypothesis (H0): There is no correlation between participants' age and their scores on a cognitive ability test. Alternative Hypothesis (H1): There is a correlation between participants' age and their scores on a cognitive ability test. Here is an example of a correlation analysis reported in APA style: A Pearson correlation was conducted to assess the relationship between participants' age and their scores on a cognitive ability test. Assumptions checks were performed to ensure no violation of the assumptions of normality, linearity, and homoscedasticity. There was a significant negative correlation between age and cognitive ability scores, r(98) = -.45, p &lt; .001, with older participants tending to have lower scores on the cognitive ability test. As such, the null hypothesis can be rejected. Here is an example of a hypothesis for a simple linear regression analysis: Null Hypothesis (H0): The number of study hours is not a significant predictor of test scores. Alternative Hypothesis (H1): The number of study hours is a significant predictor of test scores. Here is an example of a simple linear regression analysis reported in APA style: A simple linear regression was conducted to predict test scores based on the number of study hours. The assumptions of linearity, independence, and normality were checked and met. The regression equation was found to be: Test Score = 50.2 + 6.7 * Study Hours. The results indicated that there was a significant relationship between the number of study hours and test scores, F(1, 98) = 34.5, p &lt; .001. As such, the null hypothesis can be rejected The R² value was .26, indicating that approximately 26% of the variance in test scores can be explained by the number of study hours. For each additional hour of study, there was an increase of 6.7 points in the test score. The intercept value of 50.2 indicates that a student who does not study at all (0 hours) is expected to score 50.2 points on the test. "],["multiple-regression.html", "Chapter 5 Multiple regression 5.1 Shared variance with mulitple variables 5.2 The Regression Equasion", " Chapter 5 Multiple regression Multiple regression is a method that helps us predict an outcome variable based on several independent variables. It is a technique that tells us which variables are the most important for predicting an outcome, and how such predictor variables interact. By building a prediction model, multiple regression allows us to understand how variables are connected and helps us make powerful data-informed decisions. 5.1 Shared variance with mulitple variables In the previous chapter on simple linear regression, we suggested that one predictor variable (exercise) shared variance with another variable (well-being). Visually, this can be represented using a Venn diagram with two circles: one for exercise and one for well-being. The overlap between these two circles represents the shared variance, or the extent to which variations in exercise can explain variations in well-being. This shared variance can be represented numerically as the square of the correlation coefficient between the two variables. In this hypothetical example exercise is correlated with well-being with a correlation coefficent of r=0.46 (i.e. a moderate-positive correlation) and, as such, it can be said that exercise explains 21% of the variance in well-being (\\(R^2\\)=0.21) In a multiple regression analysis, we use two or more independent variables to predict the value of a dependent variable (also referred to as the criterion variable). In doing so, each independent variable may share some variance with the dependent variable, but also share variance among the other independent variables. Understanding this shared variance is crucial for interpreting the results of a multiple regression analysis. For instance, let's say we're using three independent variables to predict well-being: exercise, age, and income. Each of these variables could contribute to the variance of well-being. When we add the variables one-by-one, the overlap of each independent variable circle with the well-being circle represents the unique variance each variable contributes to well-being. Adding age data for each participant, we might see significant correlations for the three different relationships. Perhaps a positive correlation between exercise and well-being, and negative correlations between age and well-being (older people may deal with more health issues that reduce their well-being), and another negative correlation between age and exercise (older people may exercise less). Both age and exercise now explain some of the variability in well-being, and since they are also related to each other, some of the variance they explain overlaps. Finally, when we add income we can see that the variable shares variance with both age and well-being. Adding more variance explained to our model but also overlapping with some of the variance explained by age The outcome of this being that each variable that we add here allowed us to better explain our concept of well-being. And as such, if we were to know a person's amount of exercise, age, and income we could predict their well-being to a greater degree than if we just know their amount of exercise. 5.2 The Regression Equasion As you've probably realized, using Venn diagrams to illustrate the relationships among multiple variables can get quite confusing, especially when there are numerous variables that all have some degree of correlation with each other. That's where the regression equation comes in handy. The regression equation is a mathematical representation of the relationships among the variables in a multiple regression analysis. It shows how the outcome variable (also called the dependent variable) is predicted by the independent variables. In its simplest form, the regression equation can be written as: \\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_kx_k + \\varepsilon \\] Here, \\(y\\) is the dependent variable, \\(x_1\\), \\(x_2\\),...,\\(x_k\\) are the independent variables, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\), \\(\\beta_2\\),...,\\(\\beta_k\\) are the coefficients for the independent variables, and \\(\\varepsilon\\) is the error term, representing the unexplained variability in the dependent variable (i.e. the left over white space in the Venn diagram figure above). The coefficients (all the \\(\\beta\\) values) represent the average change in the dependent variable for a one-unit change in the respective independent variable, holding all other variables constant. The intercept \\(\\beta_0\\) represents the value of the dependent variable when all independent variables are zero (just like in the equation of the line from last week) Running our regression analysis allowed us to substituent number sof all the coefficients to the degree that all we are left with are \\(x\\)'s that we can use to sub in our data. Explain how shared variance workes for multiple variables Explain how the regression equasion changes explain how to interprate the results of the regression model Full explaner of resudials and their role in assumption checks show an example multiple regression from a health or environmental psych paper "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
