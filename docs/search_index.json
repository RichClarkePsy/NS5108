[["index.html", "NS5108 Research Methods Handbook Chapter 1 Welcome to the NS5108 quantiatitve methods and analysis handbook 1.1 Exercises and statistical software 1.2 Note for MSc Health Psychology and MSc Forensic Psychology students 1.3 This book is a work in progress", " NS5108 Research Methods Handbook Dr Richard Clarke 2024-10-29 Chapter 1 Welcome to the NS5108 quantiatitve methods and analysis handbook This year I have tried to make my half of NS5108 as structured and accessible as possible. Part of this plan was to place all the necessary content together in one place: this book. Here you will find an explanation of all the concepts we will cover this semester, in depth explanations of concepts that we won’t have time to cover in class, relevant video clips from our lectures, and demonstrations of how to perform the various statistical analyses. This guide is designed to work alongside your NS5108 lectures, not replace them entirely. Personally, I find that with any topic I need several explanations worded slightly differently before a concept starts to click. Please use this book as a way to consolidate your in class learning. Now, before we get started. Yes, there is going to be maths, and yes there are going to be formulas, sorry I can’t teach statistics without these aspects. However, there are different depths to which you can learn statistics. This handbook is here to give you the basics, but it also allows you to delve a little deeper into the theory behind how the various statistical analysis work. I've tried to make it so that some of the more terrifying statistical maths is hidden away behind blue boxes (like the one below) that you can click to reveal more information. These blue boxes also contain extra explanation around graphs, and some tangents that I didn’t want to get bogged down into in the main text. Click here if you have anxiety about statistics If you're feeling anxious about statistics, know that you're not alone; many people feel the same way. Think of learning statistics like learning to drive or ride a bike. Initially, it's full of painful stops and bumps, but after a few lessons, things eventually start to make sense. I struggled a lot when I first encountered this material, but once I realised how incredibly powerful it can be when applied to the topics I care about, my motivation skyrocketed, that got me over the hump and now I’m hooked for life. Just remember that learning is a marathon, and it's perfectly fine to move at your own pace. Our goal is to make this journey as smooth as possible through clear explanations and practical exercises. If you find yourself stuck, don't hesitate to ask for help. Both myself and the graduate teaching assistants are more than willing to explain concepts repeatedly and in various ways; in fact, we welcome it—it keeps us on our toes! Asking questions is an integral part of the learning process, and we're here to support you every step of the way. So, take a deep breath, and be patient and kind with yourself. The skills you'll acquire here will not only serve you well in your academic journey but hopefully lead you to having a meaningful impact in your future work. 1.1 Exercises and statistical software Throughout this workbook there are exercises where I suggest you download data and conduct an analysis with the use of JASP. JASP is a simple open source (i.e. free) program that can be used to run statistical tests. Here is a link to the website where you can download JASP. Link to download JASP Data files can be found in the weekly materials of the NS5108 Moodle page. 1.2 Note for MSc Health Psychology and MSc Forensic Psychology students If you are speed running this module as part of your NS7004 or NS7151 research methods modules you may want to conduct your analysis using SPSS or R. You absolutely can and should do so, all the theoretical content here will still apply, you might just need to do a further search around for additional where to click guides. Feel free to email me (rclarke8@glos.ac.uk) if you want me to point you in the direction of other good online content to help you with this aspect. 1.3 This book is a work in progress This semester, I will be \"gromiting\" my way through this course. Laying Tracks GIFfrom Wallace And Gromit GIFs I have a plan, and I have most of the content, but I'll be building out parts of this book during the semester. So, if you've come to this book early and expect to watch video walkthrough guides of the techniques later in the course, apologies, maybe check back in a few weeks. Also, if you spot any glaring errors please do contact me on rclarke8@glos.ac.uk and I'll correct as we go. "],["introduction-to-difference-tests.html", "Chapter 2 Introduction to difference tests 2.1 Why Do We Compare Groups? 2.2 Choosing the appropriate test", " Chapter 2 Introduction to difference tests 2.1 Why Do We Compare Groups? One of the goals of quantitative psychological research is the quest to understand differences: differences between individuals, groups, conditions, or time points. This pursuit is rooted in the scientific method, which relies on empirical evidence to support or refute hypotheses. When we compare groups in a study, we are essentially asking whether the differences we observe are meaningful or merely the result of random variation. The underlying philosophy is that by systematically comparing groups, we can uncover patterns, relationships, and causal effects that contribute to our understanding of human behaviour, cognition, and emotions. These comparisons help us test theories, evaluate interventions, and make predictions about future outcomes. In essence, difference tests are the tools that allow us to determine whether observed variations are significant and worth further consideration, or if they can be attributed to chance. To help clarify how we can compare groups in different research scenarios, let's explore the various statistical tests available, each suited to specific types of comparisons: 2.1.1 Comparing Two Groups: The T-Test The t-test is one of the most commonly used statistical methods for comparing the means of two groups (i.e. an independent variable with two levels). It helps determine whether there is a significant difference between the two groups, considering the variability within each group. Independent Samples T-Test: Used when comparing two independent groups (e.g., treatment vs. control). Paired Samples T-Test: Used when comparing two related groups (e.g., pre-test vs. post-test scores for the same individuals). 2.1.2 Comparing Three or More Groups: One-Way ANOVA When you have more than two groups to compare (i.e., an independent variable with more than two levels), a one-way Analysis of Variance (ANOVA) is used to determine whether there are statistically significant differences between the means of three or more independent groups. For example, if you were comparing the effectiveness of three different therapies on anxiety levels, a one-way ANOVA would be the appropriate test. We use ANOVA so that we can avoid performing multiple t-tests, which increases the risk of Type I errors (see chapter on power). ANOVA provides a single test that controls for this risk, offering a more reliable way to assess differences across multiple groups. 2.1.3 Comparing Multiple Factors: Factorial ANOVA Factorial ANOVA extends the one-way ANOVA by allowing additional independent variables. This type of ANOVA not only examines the differences within independent variables, but also investigates possible interactions between independent variables. For example, we could examine the effects of therapy type (CBT vs. psychotherapy) and session frequency (weekly vs. biweekly) on anxiety reduction. From performing a factorial ANOVA we can determine the main effect of therapy type, the main effect of frequency, and how the effectiveness of therapy type differs based on frequency (or vice versa). 2.1.4 Controlling for Covariates: ANCOVA Analysis of Covariance (ANCOVA) combines ANOVA and regression. It allows for the comparison of group means while controlling for the influence of one or more continuous covariates. This helps in isolating the effect of the independent variable by accounting for variance that is explained by the covariates. For example, Comparing the effectiveness of different teaching methods on exam performance while controlling for prior knowledge (measured by a pre-test score). 2.1.5 Extending to Multiple Dependent Variables: MANOVA Multivariate Analysis of Variance (MANOVA) is used when there are multiple dependent variables, and it tests whether the mean differences among groups on a combination of dependent variables are significant. For example, studying the effect of a training program on both job satisfaction and productivity simultaneously. 2.2 Choosing the appropriate test In this module, we will focus on ANOVA and factorial ANOVA, but it's also valuable to be aware of ANCOVA and MANOVA, especially if you consider using these techniques for your dissertation. Each of these tests is parametric, meaning they rely on certain statistical assumptions. Before using them, it is essential to conduct checks to ensure these assumptions are met. If the assumptions are violated, non-parametric alternatives should be used. While we will only cover the parametric tests in this module, the non-parametric alternatives are listed below for your reference in case you need to explore them for your dissertation. "],["t-tests-theory-visualisation-and-calculation-by-hand.html", "Chapter 3 T-Tests: Theory, Visualisation, and Calculation (by hand) 3.1 What does statistical difference look like? 3.2 What actually is a t-test? 3.3 Is all this maths stressing you out? 3.4 Writing up findings in APA style", " Chapter 3 T-Tests: Theory, Visualisation, and Calculation (by hand) As mentioned in the previous chapter, t-tests help us answer key questions such as whether a new treatment is more effective than an existing one or if two groups differ significantly on a psychological measure. The theory behind t-tests is rooted in hypothesis testing, specifically Null Hypothesis Significance Testing (NHST), which helps us determine whether observed differences in data are likely due to chance or reflect true differences in the population. When comparing groups, our goal is to determine whether the differences we observe are statistically significant—that is, whether they are unlikely to have occurred by chance. T-tests allow us to test hypotheses about differences between group means, providing a way to infer from sample data about the larger population. In essence, when we conduct a t-test, we are assessing whether the difference in means between two groups is large enough to be considered meaningful given the variability in the data. There are two main types of t-tests that are commonly used in research, each suited to different study designs: Independent Samples (between-subjects) T-Test: This test compares the means of two independent groups to determine if there is a statistically significant difference between them. It is used when the groups are distinct and not related. For example, a researcher could compare the average reaction times of two different groups of participants, one receiving caffeine and the other receiving water. Paired Samples (within-subjects / repeated measures) T-Test: This test compares the means of two related groups, such as the same participants tested under two different conditions (e.g., before and after an intervention). The paired samples t-test is often used when the same subjects are measured more than once, or when pairs of subjects are matched in some way. The same experiment from the above independent samples design could be redesigned as a paired samples design. Meaning that each participant takes part in each condition of the experiment. When designing an experiment, you will likely be choosing between an independent measures design (between-subjects) and a repeated measures design (within-subjects) for each of your IVs. Each choice has distinct advantages and disadvantages that can influence the study's feasibility and data quality. See the blue box below for more detail on these advantages and disadvantages. Advantages and disadvantages of experimental design choice In an independent measures design (between-subjects), each participant is assigned to only one condition, which avoids the risk of order effects such as practice, fatigue, or carryover effects that can confound the results. This design is simpler to analyse since each participant provides only one data point, making the setup straightforward. Additionally, because participants are exposed to only one condition, there is less risk of participant fatigue or boredom affecting the data. However, independent measures design typically requires a larger sample size, which increases the cost and time needed for recruitment and testing. There is also greater between-group variability since different participants are in each group, which can introduce differences that are unrelated to the independent variable and may complicate the interpretation of results. In a repeated measures design (within-subjects), the same participants are used across all conditions, which significantly reduces the number of participants needed and, therefore, the overall cost and time required for the study. This design also reduces variability due to individual differences, as each participant serves as their own control, leading to increased statistical power. However, repeated measures designs are prone to order effects, which can confound the results. To mitigate these issues, researchers often need to use counterbalancing, which adds complexity to the design and analysis. Additionally, participants may become fatigued or lose focus when completing multiple conditions, which can affect their performance and potentially skew the results. The following video is an extract from one of my lectures where I explain the design aspects of t-tests: 3.1 What does statistical difference look like? Before diving into the mechanics of t-tests, it's important to understand what statistical difference looks like. Even when two groups have different mean values, this difference might not be meaningful unless it exceeds what could be expected by chance. This is where inferential statistics come in, allowing us to determine whether the difference observed in our sample data reflects a true difference in the population. Here is an example of reaction time (in milliseconds) data from a N=8 independent samples design of the IV=coffee/water DV=reaction time study from above. From this data we can say that, on average, the reaction time in the coffee condition is 7.25ms faster than for those participants in the water condition. Is this difference meaningful or is this difference likely to be down to chance? Well, we could plot the data and see how it looks. Here is the data as a line graph: The points represent the mean reaction time for each condition, with the vertical lines indicating the 95% confidence intervals (CIs) around these means. The CIs provide a range where the true mean RT is likely to fall with 95% confidence, based on the mean and the variability within the data. It’s a great concept to use to help determine significance within your data. In this case, the overlap of the CIs suggests that the difference in mean RT between the Caffeine and Water conditions might be small and potentially not statistically significant, although a formal statistical test would be needed to confirm this. So, to me, just by looking at the data in this way, I’m not overly convinced. Especially due to the sample size of N=8 (four in each condition). This aspect will likely mean that the study has a particularly low statistical power, a concept that we will cover in more depth later in this handbook. Say instead, we run the study on N=100 participants. Another way to visualise our data is by looking at our distribution rather than just the mean and 95% CI. The following histograms are based on running the experiment 100 times, with 50 participants in each condition. How to Read these Histograms In the histograms shown above, each bar represents a \"bin\" that covers a range of reaction times (RT) within a 10-unit interval. The height of each bar corresponds to the number of data points (or \"count\") that fall within that range. For example, in the histogram for the Caffeine condition, the bar at 200 on the x-axis indicates that eight participants had reaction times between 200 and 210 milliseconds. In the water condition, no participants had a reaction time between 170 to 180 milliseconds. The red dashed line in each histogram marks the mean reaction time for that condition. By looking at the distribution of bars around this line, you can get a sense of how the data is spread out (as we previously did with the 95% CI). From this example we can see a difference in the mean RT once again. The caffeine condition participants are, on average, 15.1ms faster than participants in the water conditions. If we add a smooth line to the histograms and overlap them, this difference becomes more apparent. When the bars are removed this type of data visualisation is known as a density plot. From this density plot we can see the distribution of data in the caffeine condition is shifted to the left of the water condition. You won’t see this type of visualisation in papers as it limits you to the comparison of two conditions (it gets very confusing with three or more overlapping colours). However, you might see these in combination with a box plot. Here is how I would choose to best visually represent this data: How to read a box plot (with an additional violin plot and points) A box plot summarises the distribution, central tendency, and variability of data. The box itself represents the interquartile range (IQR), which contains the middle 50% of the data, with the line inside the box marking the median (note: not the mean). The whiskers extend from the box to the smallest and largest values that are not considered outliers, providing a clear view of the spread of the data. Outliers, or values that fall outside 1.5 times the IQR from the quartiles, are displayed as individual points beyond the whiskers (there are no outliers within this data but you might find them in yours, see assumption checks for more on this topic). In this visualisation, the box plot is combined with a violin plot, which is essentially a mirrored density plot that shows the distribution of the data. The wider sections of the violin indicate areas where data points are more concentrated, while the narrower sections represent less frequent values. This additional layer helps you understand not just where the central values lie, but also how the data is distributed across the range of reaction times. Finally, individual data points are plotted using a technique called jittering, which spreads the points horizontally to avoid overlap, making it easier to see the distribution of values. It’s important to note that jittering does not alter the actual values of the data points; it merely adjusts their horizontal position slightly for clarity. Together, the box plot, violin plot, and jittered points provide a comprehensive view of the differences in reaction times between the Caffeine and Water conditions. Now that’s a good-looking graph! But we still don’t know if this difference is meaningful, and it’s not immediately noticeable from the image. As such, we should move on to conducting a statistical test. 3.1.1 Sidenote on assumption checks Before running any statistical test, including a t-test, it is crucial to perform assumption checks. Statistical assumption checks are procedures that verify whether the data meet the specific requirements of the test you plan to use. For t-tests, these assumptions typically include the level of measurement, normality of the data, and homogeneity of variances. Ensuring that these assumptions are met is essential for the validity of the test results. However, while these checks are fundamental, it can be more conceptually intuitive to first understand how the t-test works and what it aims to achieve. By grasping the basic mechanics of the t-test and its purpose in determining whether differences between groups are statistically significant, you will be better equipped to appreciate why these assumptions matter. After we walk through the process of conducting a t-test, we will then revisit these assumption checks in detail in the next chapter. 3.2 What actually is a t-test? The t-test is a statistical method used to determine whether there is a significant difference between the means of two groups. It compares the observed difference between group means to the variation within each group, taking into account the sample sizes. The result of this comparison is the t-value, which tells us how likely it is that the observed difference could have occurred by chance. Here is the formula used to determine the t-statistic (don’t worry it’s not as scary as it looks): The formula for the t-statistic is: \\[ t = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}} \\] Where: \\(\\bar{X}\\) is the sample mean, \\(\\mu\\) is the population mean, \\(s\\) is the sample standard deviation, \\(n\\) is the sample size. As such, we actually have everything we need to work out the t-statistics from the basic group descriptives of our data: \\[ t = \\frac{183.460 - 198.560}{\\sqrt{\\frac{27.637^2}{50} + \\frac{31.022^2}{50}}} \\] In this example our data gives us a t-statistic of 2.57. After calculating the t-value, we need to determine whether this value is large enough to indicate a statistically significant difference. This is done by comparing the t-value to a critical value from what is known as the t-distribution. In this case, our critical value (which is based on the desired level of significance, commonly 0.05) and the degrees of freedom (which depend on the sample sizes) guide us to the number that our t-statistics needs to be higher than for the finding to be judged as significant. If the calculated t-value exceeds the critical value, we reject the null hypothesis and conclude that there is a statistically significant difference between the groups. (Note: this value will differ depending on if you have a one-tailed or two-tailed hypothesis) In our case, if we take our usual alpha value of 0.05, and find a t-distribution table that stretches to our degrees of freedom, which is df = (n1 – 1) + (n2 - 1) = 98 The distribution says we need to have a t-value of over around 1.98 for our finding to be significant to the 0.05 level and over 2.62 to be significant to the 0.01 level. So, with a t-value of 2.57, we’re a little over the 0.01 level. In fact, when we run this in JASP we see that our exact p-value for the comparison is 0.012. But where do all these numbers in the table come from? That’s certainly a very nerdy question for you to ask, I will try my best to answer it! The t-distribution itself is a probability distribution that is similar in shape to the normal distribution (bell curve), but it has heavier tails, meaning it is more spread out. Each number in the t-distribution table corresponds to a specific critical value of the t-distribution for a given confidence level (like 95%) and degrees of freedom (df). Degrees of freedom are calculated as the total number of observations in both groups minus the number of groups. When you look up a critical value in the table, you're finding the t-value that cuts off the most extreme 5% (for a 95% confidence level) of the t-distribution. If your calculated t-value exceeds this critical value, it falls within this extreme region, leading you to reject the null hypothesis. Here is a good explainer video on the topic: link 3.3 Is all this maths stressing you out? While understanding the formula is valuable, it is important to note that modern statistical software, such as JASP, can perform these calculations automatically. In practice, you will not need to manually compute the t-value; instead, you will simply input your data into JASP, and with a few clicks, the software will provide you with the t-value, degrees of freedom, and the corresponding p-value. This makes conducting t-tests accessible, even if the underlying mathematics may seem complex at first glance. 3.4 Writing up findings in APA style Once we put our findings together, this is what we can reliably report (minus our assumption checks): We conducted an independent samples t-test to compare mean reaction time across levels of caffeine. The results showed a statistically significant difference between the means of the two groups (t(98) = 2.57, p = 0.012). The mean reaction time for participants in the caffeine condition (Mean = 183.5, SD = 27.64) was significantly lower than the mean reaction time for participants in the water condition (Mean = 198.6, SD = 31.02), indicating a quicker reaction time. And add a graph just to make it easier for participants to interpret: In the next chapter we'll talk through the assumption checks for t-tests and why they are important. "],["assumption-checks-of-t-tests-and-other-difference-tests.html", "Chapter 4 Assumption checks of t-tests and other difference tests 4.1 Why We Run Assumption Checks for T-Tests 4.2 Assumption 1: Type of Data 4.3 Assumption 2: Normal Distribution 4.4 Assumption 3: Homogeneity of Variance", " Chapter 4 Assumption checks of t-tests and other difference tests 4.1 Why We Run Assumption Checks for T-Tests Before conducting a t-test, it's essential to check that the data meets specific assumptions required for the test to produce valid and reliable results. The primary reason for running these assumption checks is that t-tests rely on certain mathematical properties, such as the normality of data and equal variances between groups, to accurately determine whether differences between group means are statistically significant. If these assumptions are violated, the t-test may yield misleading results, increasing the risk of Type I or Type II errors (see section on statistical power for more detail on type errors). To illustrate why assumptions are critical, consider that t-tests use the mean as the measure of central tendency. However, if your data is not normally distributed or if the variance between groups is unequal, the mean may not accurately represent the central tendency of your data (see the below histograms with the same mean but different levels of normality). This could lead to incorrect conclusions about the differences between groups. The following video is an extract from one of my lectures where I explain the concept of assumption checks for t-tests 4.2 Assumption 1: Type of Data T-tests are designed for use with interval or ratio data, where the differences between values are meaningful and consistent across the scale. These types of data allow for precise measurement of differences in means between groups. In contrast, ordinal data, which ranks data without assuming equal intervals between ranks, is not suitable for t-tests because the test assumes that the dependent variable is measured on a continuous scale. Using ordinal data in a t-test can lead to inaccurate results because the distances between data points are not consistent or quantifiable in the same way they are with interval or ratio data. For example, if you were to rank participants' satisfaction levels from 1 to 5, the difference between a 1 and a 2 might not be the same as the difference between a 4 and a 5, making the mean a less reliable measure of central tendency. 4.3 Assumption 2: Normal Distribution One of the critical assumptions of the t-test is that the dependent variable is normally distributed within each group. Normality is essential because the t-test relies on the distribution of the sample means to be approximately normal, especially when sample sizes are small. This assumption allows the test to accurately assess whether the observed difference between group means could have occurred by chance. To check for normality, you can use visual methods like histograms or Q-Q plots, or statistical tests such as the Shapiro-Wilk test (see JASP example for how to read such a test). If your sample size is large (typically over 50 observations per group), visual inspection of histograms or using the skewness and kurtosis statistics may suffice. For smaller samples, the Shapiro-Wilk test is recommended. If the test indicates a significant deviation from normality, the assumption is violated, and you may need to consider a non-parametric alternative. How to conduct a skewness and kurtosis check For Small Samples (n &lt; 50): Calculate the skewness or kurtosis value and divide it by its standard error. If the resulting value falls within the range of -1.96 to 1.96, the data is likely normal. For Larger Samples (50 &lt; n &lt; 300): Use a wider range of -3.29 to 3.29 for the skewness or kurtosis value divided by its standard error to assess normality. If the skewness and kurtosis values fall within their respective ranges, your data likely meets the normality assumption. If they fall outside these ranges, you may need to consider transforming the data or using a non-parametric test instead. The below table show the values for the two variables presented in the figure above. 4.4 Assumption 3: Homogeneity of Variance The assumption of homogeneity of variance states that the variance (the square root of standard deviation) within each of the groups being compared should be roughly equal. This assumption is crucial for the t-test because it ensures that the groups are comparable, allowing the test to accurately assess differences in means. The below images illustrate a violation of the assumption of homogeneity of variance. They demonstrate how the purple variable has a much larger variance than the red variable, however, they are both still normally distributed. To statistically test for homogeneity of variance, you can use Levene’s test, which compares the variances of the groups. If Levene’s test is significant, it indicates that the variances are not equal, and the assumption of homogeneity of variance is violated. In such cases, you might need to consider an alternative non-parametric test. In the next chapter we will work through two examples of t-test analyis using data and worksheets that you can find on Moodle. "],["jasp-workshop---conducting-t-tests-independent-and-paired.html", "Chapter 5 JASP Workshop - Conducting T-Tests (Independent and Paired) 5.1 T-Test Video Walkthrough 5.2 \"Where to Click\" Guide - Conducting T-Tests in JASP 5.3 APA Style Guide for Reporting T-Tests 5.4 Dr Clarke’s Automatic t-test Generator", " Chapter 5 JASP Workshop - Conducting T-Tests (Independent and Paired) JASP is a free, open-source statistical software package designed with a user-friendly, point-and-click interface, ideal for research in psychology. It offers a wide range of statistical analyses, including t-tests, ANOVA, regression, and more. This guide will walk you through the steps for conducting independent and paired samples t-tests in JASP, highlighting the critical points to check and report in your analysis. JASP is available on most university computers, but we recommend installing it on your personal laptop or desktop to continue learning outside class. You can download JASP here: www.jasp-stats.org. The following video shows you how to install JASP on your own computer, talks you through how to open your data in JASP, and gives you an overview of how to use the program. 5.1 T-Test Video Walkthrough In the below video, I work through example analyses of both independent and paired samples t-tests. If you want to follow along, please find the corresponding question sheets and datasets available in the NS5108 module on Moodle. Independent samples t-test in JASP: Paired samples t-tests in JASP: 5.2 \"Where to Click\" Guide - Conducting T-Tests in JASP Sometimes you just want to know where to click to run the test. Below is a step-by-step guide for performing independent and paired samples t-tests in JASP. Refer to the video above for more context regarding these steps. 5.2.1 Conducting an Independent Samples T-Test Open JASP and load your data: Click on the File tab at the top left, select Open, and navigate to the folder containing your data file. Visualise the Data: Click Descriptives -&gt; Descriptive Statistics and move the variables of interest into the Variables box. To visualise the distribution, use histograms or boxplots to check the spread and identify potential outliers. Check Assumptions: Normality: Check normality by clicking Plots -&gt; Q-Q plots. Look for data points that should lie approximately along the diagonal line. Homogeneity of Variance: Conduct Levene’s test by clicking T-Tests -&gt; Independent Samples T-Test. Move the grouping variable to Grouping Variable and your dependent variable to Dependent Variable. Ensure the option Equality of variances test (Levene’s) is checked. Run the T-Test: Under T-Tests -&gt; Independent Samples T-Test, input your dependent and grouping variables (you’ll have already done this if you’ve check for homogeneity of variance). Extract the t-value, degrees of freedom (df), and p-value from the output. Interpret the Results: Determine if there is a statistically significant difference between the groups based on the p-value (p &lt; .05 typically indicates significance). 5.2.2 Conducting a Paired Samples T-Test Open JASP and load your data as before. Visualise the Data: Identify your paired variables and use histograms or boxplots to visualise their distribution. This helps in assessing normality and checking for potential outliers. Check Assumptions: Normality: Use the Q-Q plots to visually check if the paired differences are normally distributed. Homogeneity of Variance: For paired samples t-test, homogeneity of variance is assumed. Due to the same participants providing data for each level of the IV. Run the Paired Samples T-Test: Click T-Tests -&gt; Paired Samples T-Test. Move the paired variables into the Paired Variables box. Extract the t-value, degrees of freedom, and p-value from the results. Interpret the Results: Check the p-value to determine if the difference between the paired groups is statistically significant. 5.3 APA Style Guide for Reporting T-Tests Here’s how to format your hypotheses and report results from your t-tests in APA style. Hypothesis for an Independent Samples T-Test (two-tail/non-directional): - Null Hypothesis (H0): There is no significant difference in [the dependent variable] between [the two groups]. - Alternative Hypothesis (H1): There is a significant difference in [the dependent variable] between [the two groups]. Hypothesis for an Independent Samples T-Test (One-Tail/Directional): - Null Hypothesis (H0): There is no significant difference or [Group 1] does not have a higher/lower [dependent variable] than [Group 2]. - Alternative Hypothesis (H1): [Group 1] has a significantly higher/lower [dependent variable] than [Group 2]. Example APA Report for an Independent Samples T-Test: &gt; “An independent samples t-test was conducted to compare reaction times between a [variable name] group and a [variable name] group. Assumption checks confirmed normality and equal variances. The results showed a significant difference between the groups, t(df) = t-statistic, p = p-value, indicating that [dependent variable] [greater/lower] in the [condition 1/2] group compared to the [condition 1/2].” Hypothesis for a Paired Samples T-Test (Two-Tail/Non-Directional): - Null Hypothesis (H0): There is no significant difference in [the dependent variable] between [Condition 1] and [Condition 2]. - Alternative Hypothesis (H1): There is a significant difference in [the dependent variable] between [Condition 1] and [Condition 2]. Hypothesis for a Paired Samples T-Test (One-Tail/Directional): - Null Hypothesis (H0): There is no significant difference, or [Condition 1] does not result in a higher/lower [dependent variable] than [Condition 2]. - Alternative Hypothesis (H1): [Condition 1] results in a significantly higher/lower [dependent variable] than [Condition 2]. Example APA Report for a Paired Samples T-Test: &gt; “A paired samples t-test was conducted to compare test scores before and after an intervention. Assumption checks confirmed normality of dependent variable within each condition. The results indicated a significant improvement in scores after the intervention, t(df) = t-statistic, p = p-value, suggesting that the intervention was effective.” 5.4 Dr Clarke’s Automatic t-test Generator Lately, I’ve been learning to code and, well, I’ve made something for you: Automatic t-test generator This link takes you to a web application that automatically generated data for an independent samples t-test, performs the analysis and then reports the findings. By moving the sliders around you should be able to see how data with different means and SD look and how they affect the assumption checks, choice of test, statistical significance (and observed power, effect size. Both of which are explained in detail in the next chapter). "],["understanding-effect-size-and-power-in-psychological-research.html", "Chapter 6 Understanding Effect Size and Power in Psychological Research 6.1 Real-World Example of Effect Size and Power 6.2 What is Effect Size? 6.3 Other Measures of Effect Size 6.4 Effect Size Meaning Recap 6.5 Overview of Statistical Power 6.6 Power Analysis", " Chapter 6 Understanding Effect Size and Power in Psychological Research In psychological research, understanding not just whether a difference exists, but also how large that difference is, is crucial. This chapter will explore two key concepts that help us do this: effect size and statistical power. Effect size measures the magnitude of a relationship or the difference between groups, providing insight into the practical significance of findings. Statistical power, on the other hand, refers to the likelihood that a study will detect an effect when there is one. Together, these concepts allow us to draw more meaningful conclusions from our data, moving beyond simple yes-or-no answers provided by statistical significance. By the end of this chapter and the associated exercise, you will understand how to calculate and interpret effect sizes, how to consider the power of a study before data collection begins, and why these factors are just as important—if not more so—than the p-value traditionally used in hypothesis testing. 6.1 Real-World Example of Effect Size and Power Imagine a scenario where a psychologist is comparing the effectiveness of two types of therapy for reducing anxiety: Cognitive Behavioural Therapy (CBT) and Mindfulness-Based Stress Reduction (MBSR). After conducting a study, they find a statistically significant difference between the two therapies in terms of their impact on anxiety levels. But what does this significant difference actually mean? Is one therapy truly better, or is the difference so small that it wouldn't matter in a real-world setting? This is where understanding effect size comes in. Effect size will tell us how big the difference between CBT and MBSR is. If the effect size is large, it suggests that one therapy is meaningfully better than the other, and this difference could have important implications for treatment decisions. However, if the effect size is small, even though the difference is statistically significant, it might not be practically important. Now, consider statistical power. If the study had a small sample size, the power might be low, meaning there was a higher chance of not detecting a difference even if one existed. In contrast, a study with high power reduces the risk of missing a true effect, giving more confidence in the results. In the following sections we’ll look at these concepts in detail, how they are calculated and how we can use those calculations to build on our existing statistical findings. 6.2 What is Effect Size? While p-values tell us whether there is evidence of an effect, effect size represents the magnitude or strength of such an effect. It provides a way to understand the practical significance of research findings by indicating the size of the difference between groups or the strength of the relationship between variables. In essence, effect size helps us move beyond asking, \"Is there difference?\" to answering, \"How big is the difference?\" Effect size can be measured in several ways, depending on the type of data and the nature of the analysis. Below are some of the most commonly used effect size measures in psychological research. 6.2.1 Cohen’s d Cohen’s d is one of the most widely used measures of effect size. It quantifies the difference between the means of two groups in terms of standard deviations. For instance, if we were comparing the average anxiety levels of participants receiving CBT versus those receiving MBSR, Cohen’s d would tell us how much these groups differ, relative to the variability within each group. - Small Effect (d ~0.2): A small effect size indicates a modest difference between groups, often noticeable only under certain conditions or in large samples. - Medium Effect (d ~ 0.5): A medium effect size suggests a moderate difference, generally visible and meaningful in most contexts. - Large Effect (d ~ 0.8): A large effect size indicates a substantial difference, easily noticeable and likely to be of practical importance. Below is an illustration of these different effect sizes: Advantages and disadvantages of experimental design choice Each of the three graphs represents how data from each group is spread out. Think of it as a smoothed version of a histogram (a graph that shows where most of the data points are). The x-axis (horizontal) shows the possible values that participants score, and the y-axis (vertical) shows how likely participants are to score such a score. In each graph, the green area represents \"Group 1,\" and the red area represents \"Group 2.\" The dotted lines show the average (mean) value for each group. The less the two areas overlap the greater the difference between the two groups and the greater the effect size. A interactive version of this image can be seen in the automatic t-test generator that I shared in the previous chapter: Automatic t-test generator 6.2.2 How is Cohen’s d calculated? You’ll likely calculate Cohen’s d with a single click of a box in JASP or other statistical software, but it’s helpful to understand where this number comes from and what it represents. So, let’s break down how Cohen’s d is calculated. Cohen’s d measures the difference between the means of two groups, relative to the variability within those groups. It’s essentially a way of standardising the difference so that it is possible to compare findings across different studies, even if they used different scales or measurements. Here’s the basic formula: \\[ d = \\frac{M_1 - M_2}{SD_{pooled}} \\] Where: \\(M_1\\) and \\(M_2\\) are the means (averages) of the two groups. \\(SD_{pooled}\\) is the pooled standard deviation of the two groups, which is a measure of the spread or variability of the data. 6.2.3 Breaking it Down: Difference in Means First, we calculate the difference between the mean values of the two groups. This tells us how far apart the groups are on whatever measure you’re using (e.g. anxiety scores). Pooled Standard Deviation Pooled SD is a bit more complex, but in simple terms, it’s an average of the standard deviations of the two groups. It gives us a sense of how spread out the scores are in each group. The formula for the pooled standard deviation is: \\[ SD_{pooled} = \\sqrt{\\frac{(n_1 - 1) SD_1^2 + (n_2 - 1) SD_2^2}{n_1 + n_2 - 2}} \\] Where: \\(SD_1\\) and \\(SD_2\\) are the standard deviations of each group. \\(n_1\\) and \\(n_2\\) are the sample sizes of each group. This part of the formula adjusts for the fact that the groups might have different variabilities or different numbers of participants. Finally, we divide the difference in means by the pooled standard deviation. This gives us Cohen’s d, which tells us how big the difference is in standard deviation units. And, as mentioned previously, Cohen’s d can be interpreted as: Small effect when d is roughly 0.2 or lower. The minimum value is 0. Medium effect when d is roughly 0.5 Large effect when d is roughly 0.8 or higher. There is no upper bound, however, if the effect size is above around 2.0, I’d take that as an indication that I’d done something wrong in the analysis or data entry/cleaning as no meaningful, well-designed experiment should have a difference that big. 6.3 Other Measures of Effect Size Cohen’s d is not the only measure of effect size. In fact, Cohen’s d is mostly just used in t-test comparisons. Later in this book we will talk about Eta-Squared (η²) and Partial-Eta Squared (η²p). These are used primarily in ANOVA (Analysis of Variance), but the principle is much the same (with just a slightly different interpretation of the values). We will also be looking at r and R-squared in the regression section of this book, these are the correlational equivalent of effect size measurement. 6.4 Effect Size Meaning Recap Understanding effect size is critical because it goes beyond statistical significance to address the real-world implications of research findings. A p-value might tell us that a difference between groups is unlikely to be due to chance, but it doesn't tell us if that difference is large enough to be meaningful. For example, in a large study, even a tiny difference might yield a significant p-value. However, if the effect size is small, this difference might not be important in practical terms. Conversely, a large effect size in a smaller study could indicate a very meaningful difference, even if it doesn't yield statistical significance due to a small sample size. In psychological research, reporting effect size allows for a better understanding of the magnitude of findings and facilitates comparison across studies. It also aids in meta-analyses, where effect sizes from different studies are combined to assess the overall strength of a phenomenon. By considering both statistical significance and effect size, researchers can provide a fuller, more nuanced interpretation of their results, leading to more informed decisions in both research and practice. For a deeper understanding of effect size see Lakens, D. (2022). Improving Your Statistical Inferences. Section 6.1 – 6.6 6.5 Overview of Statistical Power Statistical power is a crucial concept in psychological research that is closely related to effect size. While effect size tells us the magnitude of a difference, statistical power tells us how likely we are to detect that magnitude of a difference if that difference actually exists. Understanding statistical power is essential in the planning and design stages of your research, as it directly influences the decision on how many participants are needed in a study. Given that participant recruitment can be both costly and time-consuming, assuring adequate power will makes sure your study is sensitive enough to detect meaningful differences, while reducing the risk of overlooking important effects. 6.5.1 Recap of type I and type II error When you conduct a study in psychology, the evidence you collect to test your hypotheses is almost guaranteed not to be a perfect representation of reality. Your measures won’t be perfect, your manipulations might be fallible, your participants are unlikely to be truly representative of your population, and/or countless other confounding factors. As such, it’s best to think in terms of there being four possible outcomes to your research: You find evidence of a difference, and this difference is a reliable reflection of the difference found in reality (True Positive) You find no evidence of a difference, and this difference is a reliable reflection of the difference found in reality (True Negative) You find evidence of a difference, however, in reality there is no difference (false positive, Type I error) You find no evidence of a difference, however, in reality there is a difference (false negative, Type II error) A good way to think of this concept is to pretend that you are a judge in a court case. Your role is to decide guilt or innocents based on the evidence presented to you. Underlying that decision is the knowledge that if the evidence is unreliable, you maybe convicting an innocent person or letting a guilty person go free. Our job in science, just as in the legal system, is to minimize the likelihood of Type I and Type II errors while increasing the chance of correctly identifying the true nature of reality. Imagine now that the grid is the full set of possible outcomes from our experiment. Conducting an experiment is much like throwing a dart at this grid. If we start with the simplistic view that each box has an equal area, we’d have a 25% chance of hitting each one of the boxes—meaning there's a 50% chance of our experiment working as designed (landing in the \"correct decision\" boxes) and a 50% chance of giving an answer that is not representative of reality (landing in one of the error boxes). This scenario illustrates a poorly designed experiment with no mechanisms in place to improve our odds. 6.5.2 Reducing Type I Errors: To improve our chances, we need to adjust the likelihood of each outcome. You’ve already encountered one key way we reduce Type I errors: setting an alpha level. In psychology, we typically accept a false positive rate (Type I error) of 5%. This means we are willing to risk concluding that there is an effect (when there isn’t) 5% of the time. In practice, this is controlled by the p-value, which tells us the probability of observing the data (or something more extreme) if the null hypothesis were true. If the p-value falls below the threshold of 0.05 (which represents 5%, or 1 in 20), we reject the null hypothesis, accepting a small chance that we're making a Type I error. 6.5.3 Addressing Type II Errors: Now, let's consider Type II errors—where we fail to detect a true effect. Unlike Type I errors, which are controlled by setting the alpha level, Type II errors are influenced by the power of the study. Power is the probability of correctly rejecting the null hypothesis when it is false (i.e., avoiding a Type II error). The higher the power, the less likely we are to miss a true effect. In psychology, we generally aim for a power of 0.80, meaning there is an 80% chance that our study will detect a true effect if one exists, leaving a 20% chance of making a Type II error. Together this is how the hypothetical grid will typically look for a psychology study under the above criteria. In psychological research, we are generally stricter with Type I errors than Type II errors because the consequences of a false positive (Type I error) are often considered more serious. A Type I error means concluding that an effect or difference exists when it actually does not, which can lead to false theories, wasted resources, and potentially harmful applications in practice, such as the adoption of ineffective treatments. By setting a low alpha level (commonly 0.05), we aim to minimize the risk of making such incorrect claims. While Type II errors (false negatives) are also important, they are often viewed as less critical because they imply that we missed an existing effect, which can be corrected with further research. In contrast, the propagation of a false positive can have far-reaching and lasting impacts on both science and practice. Again, think in terms of convicting an innocent person as compared to not convicting a guilty person. 6.6 Power Analysis Power analysis helps us determine the necessary sample size for our study to achieve a desired level of power. By conducting a power analysis before collecting data, we can estimate how many participants we need to reliably detect an effect of a given size. The analysis takes into account the alpha level, the effect size we expect, and the desired power. For example, if we expect a small effect size, we would need a larger sample to achieve 80% power than we would if we expected a large effect size. Without a sufficient number of participants, our study might be underpowered, increasing the risk of a Type II error. In the next chapter I walk you through how to conduct an a-priori power analysis for an independent samples t-test. And later in this book we’ll return to the concept to illustrate how you can plan your intended sample size for more complex studies. For a deeper understanding of effect size see Lakens, D. (2022). Improving Your Statistical Inferences. Section 8 In the next chapter we will run though how to conduct a power analysis for your study design. "],["how-to-conduct-a-power-analysis.html", "Chapter 7 How to Conduct a Power Analysis 7.1 Cohen’s Power Primer 7.2 G*power", " Chapter 7 How to Conduct a Power Analysis To conduct an a-priori power analysis (working out the statistical power of a study before conducting the study), for a difference test, we need a number of pieces of information: 1. The design of the study, how many IVs, and how many levels to those IVs 2. A likely effect size that we would expect in our designed study 3. Our chosen values for alpha and power (1-beta) There is a mathematical way in which you can calculate the sample size needed in such a study but for the purposes of your undergraduate studies there are two simpler ways in which you can calculate sample size for your own studies. Firstly, there is Cohen’s power primer, a simple but limited way, and secondly is with G*Power an application that allows for more customisable power calculations. 7.1 Cohen’s Power Primer Imagine you are conducting a study to examine the effect of a new relaxation technique on reducing stress levels. You plan to compare stress scores between two groups: one group practicing the relaxation technique and a control group not practicing it. Your goal is to determine the required sample size to detect a significant difference in stress scores between the groups. To conduct an a-priori power calculation using Cohen’s Power Primer, we would start by decide on the expected effect size, often to do this we would have read the literature and hopefully found previous research, studying a similar topic, which we could base out calculation off of. If not we often assumed to be medium a medium effect size (Cohen’s d = 0.5). Next, we’d choose our alpha level (α), the significance threshold, commonly set at .05 (5%), .01 (1%), or .10 (10%); for this example, we will use α = .05. Then, we’d determine our desired power (1 - β), which is the probability of correctly rejecting the null hypothesis when an effect exists; a typical target is .80 (80%). After defining these parameters, locate the appropriate section in Cohen’s Power Primer table (see below), specifically the section titled \"1. Mean dif\" (which stands for the difference between means, i.e. a t-test). Find the column corresponding to your chosen alpha level (.05) and locate the row for a medium effect size (Med). From the table, read the sample size required for adequate power; for a medium effect size at α = .05, the table indicates 64 participants per group. To determine the total sample size for your study, multiply the number of participants per group by 2, as this is an independent samples t-test comparing two groups: 64 participants in Group 1 plus 64 participants in Group 2 equals a total of 128 participants. This calculation would therefore means that to detect a medium effect size with 80% power at an alpha level of .05, we would need a total of 128 participants (64 per group). The different rows on grid can be used for different statistical tests with different number of conditions or different number of variables. However, this is not comprehensive (for instance the table cannot be used for factorial ANOVA). For more complex power calculations it is best to use the program G*Power 7.2 G*power 7.2.1 Installation and opening the application First step is for you to download the application, if you are on a university computer it should already be installed and you should be able to access it through the desktop search bar. If you are on your own computer you’ll need to download it from the following webpage: https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower Yes, this is the right place, its hosted on the university of University Düsseldorf which is why some of the web page is in German. The instructions to download and the app itself is in English, so this won’t be an issue for you. 7.2.2 Conducting an a-priori power calculation for a t-test Select the t-test from the test family drop down menu. Then select that in this case we would like a power analysis for a independent samples difference between two means. Then input our expected effect size, alpha level, and power The allocation ratio is just for if we wanted unequal groups for some reason (i.e. the intervention is more expensive than the control), we can just leave this as one. Then click calculate The graph at the top shows two curves: the red curve represents the null hypothesis distribution, and the blue dashed curve represents the alternative hypothesis distribution, with shaded areas highlighting the probabilities of Type I error (α) and Type II error (β). And as with the Cohen’s power primer, the results of this analysis indicate that we should recruit 64 participants per group, totalling 128 participants, to achieve the target power. In this case it would give us an actual power of 0.801, assuming that our effect size for our study is (which is often a big assumption) [check back for video of power analysis demo later in the semester] "],["introduction-to-anova.html", "Chapter 8 Introduction to ANOVA 8.1 Example of a one-way ANOVA design 8.2 The Omnibus Test 8.3 Interpreting the results of an ANOVA omnibus test 8.4 Post-Hoc Testing 8.5 Why we can’t just run multiple t-tests 8.6 Interpretation of post-hoc comparisons", " Chapter 8 Introduction to ANOVA ANOVA, which stands for Analysis of Variance, is a statistical test used when comparing a continuous dependent variable across three or more groups. While it might seem similar to the t-test you've already learnt about, there is a fundamental issue that needs to be overcome when we move from two groups to more than two groups. In this chapter we will learn about the basic one-way ANOVA, a design with a single independent variable with 3 or more levels, before moving on to factorial ANOVA, a design with more than one independent variable, in the next chapter. This lecture chunk gives an overview of the ANOVA family of tests: 8.1 Example of a one-way ANOVA design Imagine a study designed to investigate how to best frame an information campaign to prepare residence of a local area for an extreme weather event. In this experiment, participants are randomly assigned to one of four levels of an independent variable (i.e. a between subjects design). Messaging (independent variable): Fear-Based Messaging: Messages that emphasise the severe consequences of not being prepared. Efficacy-Based Messaging: Messages that focus on practical steps individuals can take and highlight their ability to effectively prepare. Community-Focused Messaging: Messages that highlight the importance of collective preparedness and how working together can improve community resilience. Control condition: No message Preparedness (Dependent variable): - Measured using a scale developed for the study. Scored from 0 to 100, with higher scores indicating greater intentions to engage in preparedness behaviours, such as creating an emergency kit or making a family plan. Hypothesis: H1: It is hypothesised that the type of preparedness messaging will significantly affect participants' intentions to prepare for disasters. H2: Efficacy-based messaging is expected to generate the highest preparedness intentions, followed by community-focused messaging, fear-based messaging generating the lowest intentions. H3: All forms of messaging will lead to higher preparedness than the control, no messaging, condition. Procedure: After a power analysis using G*Power (see below), N= 180 participants are randomly assigned to receive one of the three types of preparedness messages, or no message. After reading the message, participants complete the questionnaire assessing their intentions to engage in preparedness behaviours. A one-way ANOVA is used to compare the mean preparedness intentions across the three messaging conditions. Here is the (simulated) data from this study design: The graph shows preparedness intentions across four messaging conditions: Community-Focused, Control, Efficacy-Based, and Fear-Based (N=45 per condition). Violin plots display the distribution of scores, with wider sections indicating more frequent scores. Boxplots within the violins show the median and interquartile range, highlighting the central 50% of scores. Jittered grey dots represent individual scores, and red dots mark outliers. Efficacy-Based messaging shows the highest median and range, followed by Community-Focused, while Fear-Based, and Control conditions show lower scores. But are these differences meaningful or just random noise in the data? That’s what we have to use our ANOVA test to find out This lecture chunk explains the above design: 8.2 The Omnibus Test The ANOVA omnibus test is our first step in determining whether there are significant differences between our four groups. The key idea behind the omnibus test is to assess whether at least one group mean differs significantly from the others. In our study, the ANOVA omnibus test evaluates whether the mean preparedness scores differ across the four messaging conditions. However, it does not specify which groups are different; it only tells us if a significant difference exists somewhere among the groups. If the omnibus test is significant, post hoc tests are then used to pinpoint where those differences lie. ANOVA calculates an F-statistic by comparing two sources of variability in the data: 1. the variance between the group means (between-group variance) and 2. the variance within each group (within-group variance). As with the t-test, it’s possible to work out the F-statistic by hand, but for this course we’ll be using a stats program (JASP/SPSS/R) to perform all of this for us (see blue box below for why I don’t make you do this by hand). 8.2.1 Basic maths behind the ANOVA omnibus test Between-Group Variance - This measures the variability of the group means around the overall mean of all groups combined. It captures how much the different messaging conditions (e.g., Efficacy-Based vs. Fear-Based) affect preparedness intentions. A larger between-group variance indicates that the group means are spread far apart, suggesting a potential effect of the messaging type. Within-Group Variance - This measures the variability within each group around its own group mean. It reflects individual differences in preparedness intentions that are not explained by the messaging condition. High within-group variance suggests that participants' scores vary widely within each messaging condition. Calculation of the F-statistic - The F-ratio is calculated by dividing the between-group variance by the degrees of freedom between and dividing the within-group variance by the degrees of freedom within and then dividing the products of each of those. Mathematically, this is represented as: F-statistics = (Between-group variance/ df between) / (within group variance/ df within) Click for more maths Let’s generate the F-statistics for the above experiment. The following formular is used to calculate the between-group variance. In simple terms it tells us to take each group’s mean and compared them to the overall mean (grand mean) of all participants. The difference between each group mean and the overall mean is taken and then squared, weighted by the number of participants in that group (45 in all conditions), and then summed across all groups. \\[ SSB = \\sum_{i=1}^{k} n_i \\left(\\bar{X}_i - \\bar{X}_{\\text{overall}}\\right)^2 \\] Where: \\(n_i\\) is the number of participants in group \\(i\\). \\(\\bar{X}_i\\) is the mean of group \\(i\\). \\(\\bar{X}_{\\text{overall}}\\) is the overall mean of all participants. For our data we get the following: Next, we need to calculate the within-group variance using the formular below. Within each group, each participant’s score is compared to the group’s mean. The difference is squared, and these squared differences are summed for each group and then combined across all groups. \\[ SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left( X_{ij} - \\bar{X}_i \\right)^2 \\] Where: \\(X_{ij}\\) is the score of participant \\(j\\) in group \\(i\\). \\(\\bar{X}_i\\) is the mean of group \\(i\\). \\(n_i\\) is the number of participants in group \\(i\\). So, in other words, for this part I need to find the difference for each participant to the groups mean, square and sum that difference for each group, and then sum the group totals. Here is a snippet of how this works for the first two participants of each group, but in not going to do this by hand for all 180 participants (I do have life some of the time!), so I did the totals with code in R instead. The final step is to take our Between-group variance and divide that by our degrees of freedom between (number of groups – 1 = 3). And divide our within-groups variance by our degrees of freedom within (number of participants – number of groups = 176) Between groups value = 4468/ 3 = 1489.3 Within groups value = 16017 /176 = 91.01 And then divide the between groups value by the within groups value T-statistics = 1489.3 / 91.01 = 16.36 Or you can just click a couple of buttons in JASP, your call! 8.3 Interpreting the results of an ANOVA omnibus test In our study, if the F-ratio is significantly greater than 1 (which it is, F = 16.365), it suggests that the differences between the messaging conditions are larger than would be expected by chance. A significant F-value (p&lt;0.05, which ours again is, p&lt;0.001) indicates that at least one messaging condition has a different effect on preparedness intentions compared to the others. note: Number in this table differ slightly to my by hand calculation due to my rounding of numbers each step. 8.4 Post-Hoc Testing While a significant omnibus result tells us that there is a significant difference somewhere among the group means, it does not specify where the differences lie. To identify which groups differ from each other, we need to perform additional “post-hoc” tests, to make comparisons between each individual group. In a way this turns it back into a number of 2 group comparisons, so its tempting to just resort back to running a number of t-tests again. That’s almost what we do, but just running t-test after t-test would cause an issue with our Type I error rate. 8.5 Why we can’t just run multiple t-tests As you likely remember from the previous chapter, when conducting statistical tests, such as t-tests or correlations, each test is associated with a specific error rate, typically set at an alpha level of 0.05. This means that for any single test, there is a 5% chance of making a Type I error (falsely rejecting the null hypothesis). This error rate is known as the “test-wise error rate.” However, when performing multiple tests on the same dataset, the probability of making at least one Type I error increases with each additional test. For instance, if you perform 20 comparisons, the likelihood is high* that at least one of the results will be statistically significant by chance alone, even if the null hypothesis is true. This accumulation of error across multiple tests is known as the multiple comparisons problem. An ANOVA is therefore split into two separate tests. The omnibus test, a test that determines significant difference across all levels of the independent variable, and the post-hoc test, a test that looks at the difference between each individual level of the independent variable controlling for the multiple comparisons. *Probability of type one error if a study is run 20 times So, I got distracted and took a bit of a tangent when I wrote this section. Probability is hard and counterintuitive, I originally wanted to give the actual probability of a type 1 error if a study is run 20 times. But I didn’t want you to get bogged down in the value here (as that’s not the main take away) so I’ve relegated the maths to another nerdy blue box. Question: If there is a 5% probability of a false positive in a study and we run the study 20 times what is the probability that one or more study results will be a false positive? Probability of false positive: p=0.05 Probability of not a false positive: 1 – p = 0.95 N = 20 Probability of no false positives: P(x=0) P(x=0) = (20 | 0)x (0.05)^0 x (0.95)^20 P(x=0) = 0.95^20 = 0.3585 Probability of more than one false positive: P(x=1) P(x=1) = 1 - P(x=0) = 1- 0.358 = 0.6415 This gives us a 64.15% probability that at least one out of 20 studies would produce a false positive result. To say it another way, if we ran our study 20 times and found a significant result, at some point across those 20 studies, it would be more likely than not to be a false positive. i.e. not good science! 8.6 Interpretation of post-hoc comparisons Once again, your statistics program does the heavy statistical lifting of accounting for multiple tests and should give you a table something like this. In this table you can see that each group is paired with each other group, if the p-value is less than 0.05 we can say that there is a difference between those two groups. This can take a bit of effort to get your head around in table form, so here is the graph again, this time with the significant differences added. 8.6.1 Test yourself Look back at the hypotheses from earlier. Are all of them confirmed? See if you can explain the results in real world terms, what have we actually found? Which technique(s) do you suggest we use and why? This lecture chunk explains the above analysis: In the next chapter we'll work through the analysis for different types of one-way ANOVA "],["jasp-workshop-one-way-anova.html", "Chapter 9 JASP workshop – one way ANOVA 9.1 “Where to Click” Guide - Conducting One-Way independent samples ANOVAs in JASP 9.2 JASP Lab Exercises 9.3 APA Style Guide for Reporting One-Way ANOVAs", " Chapter 9 JASP workshop – one way ANOVA In the video below, I work through example analyses of both independent and repeated measures one-way ANOVAs. If you want to follow along, please find the corresponding question sheets and datasets available in the NS5108 module on Moodle. This video walks you through an example of a Repeated measures ANOVA in JASP: 9.0.1 Conducting a Repeated Measures ANOVA Open JASP and Load Your Data: As before, open your data file in JASP. Visualise the Data: Go to Descriptives -&gt; Descriptive Statistics. Move all levels of your repeated measure into the Variables box. Use plots to visualise distributions for each condition. Check Assumptions: Normality: Skewness and Kurtosis: In the Descriptive Statistics window, under Statistics, check Skewness and Kurtosis for each condition. Shapiro-Wilk Test: Under the same Statistics tab, check Shapiro-Wilk to perform the Shapiro-Wilk test for each condition. When to Use: Use the Shapiro-Wilk test when sample sizes per condition are small (less than 50). For larger samples, rely more on skewness, kurtosis, and graphical assessments. Sphericity: Navigate to ANOVA -&gt; Repeated Measures ANOVA. Under Assumption Checks, check Sphericity tests to perform Mauchly's Test. Run the Repeated Measures ANOVA: Define your repeated measures factor in the Repeated Measures Factors box (e.g., Time with levels Time1, Time2, Time3). Move the corresponding variables into the Repeated Measures Cells. Click OK to run the analysis. Post Hoc Tests: Select your repeated measures factor for pairwise comparisons. Choose an appropriate correction method, such as Bonferroni. **This video walks you through a one-way independent samples ANOVA in JASP 9.1 “Where to Click” Guide - Conducting One-Way independent samples ANOVAs in JASP Sometimes you just want to know where to click to run the test. Below is a step-by-step guide for performing independent and repeated measures one-way ANOVAs in JASP. Refer to the video above for more context regarding these steps. Conducting an Independent Samples One-Way ANOVA ### Conducting an Independent Samples One-Way ANOVA Open JASP and Load Your Data: Click on the File tab at the top left. Select Open, and navigate to the folder containing your data file. Visualise the Data: Go to Descriptives -&gt; Descriptive Statistics. Move your dependent variable into the Variables box and your independent variable (factor) into the Split box. Use Plots to create histograms or boxplots for each group to check the distribution and identify potential outliers. Check Assumptions: Normality: Skewness and Kurtosis: In the Descriptive Statistics window, under Statistics, check Skewness and Kurtosis. Shapiro-Wilk Test: Under the same Statistics tab, check Shapiro-Wilk to perform the Shapiro-Wilk test for normality for each group. The Shapiro-Wilk test assesses whether the data in each group are normally distributed. When to Use: Use the Shapiro-Wilk test when your sample size is relatively small (typically less than 50 per group). For larger samples, the test becomes overly sensitive, and minor deviations from normality can lead to significant results. Homogeneity of Variance: Navigate to ANOVA -&gt; ANOVA. Move your dependent variable to Dependent Variable and your independent variable to Fixed Factors. Under Assumption Checks, check Homogeneity tests to perform Levene's Test. A non-significant p-value (p &gt; .05) in Levene's Test indicates that the assumption of homogeneity of variances is met. Post Hoc Tests: Go to the Post Hoc Tests tab. Move your independent variable into the Post Hoc Tests box. Select a correction method (e.g. Bonferroni) for multiple comparisons. 9.2 JASP Lab Exercises Further practice with additional exercises can be found in the Moodle course area. Each exercise comes with a dataset and an answer sheet. Work through the exercises to solidify your understanding and then compare your results with the provided answers. 9.3 APA Style Guide for Reporting One-Way ANOVAs Here's how to format your hypotheses and report results from your one-way ANOVAs in APA style. Hypotheses for a Repeated Measures One-Way ANOVA (Two-Tailed/Non-Directional) Null Hypothesis (H₀): There is no significant difference in [dependent variable] across [the different conditions]. Alternative Hypothesis (H₁): There is a significant difference in [dependent variable] across [the different conditions]. Continue to specify specific group difference predictions Example APA Report for a Repeated Measures One-Way ANOVA \"A repeated measures one-way ANOVA was conducted to examine the effect of [independent variable] on [dependent variable] over [number] conditions. Mauchly's Test indicated that the assumption of sphericity had been met/violated (χ²(df) = value, p = p-value). The results revealed a significant effect of [independent variable] on [dependent variable], F(df_between, df_error) = F-value, p = p-value, η² = effect size. Using the Greenhouse-Geisser correction (if sphericity is violated), the effect remained significant/non-significant. Post hoc pairwise comparisons showed that [specific condition differences], indicating that [interpretation of results].\" Hypotheses for an Independent Samples One-Way ANOVA (Two-Tailed/Non-Directional) Null Hypothesis (H₀): There is no significant difference in [dependent variable] among [the different groups]. Alternative Hypothesis (H₁): There is a significant difference in [dependent variable] among [the different groups]. Continue to specify specific group difference predictions Example APA Report for an Independent Samples One-Way ANOVA \"An independent samples one-way ANOVA was conducted to compare the effect of [independent variable] on [dependent variable] across [number] groups. Assumption checks confirmed normality and equal variances. The results showed a significant effect of [independent variable] on [dependent variable], F(df_between, df_within) = F-value, p = p-value, η² = effect size. Post hoc analyses, using a Bonferroni correction, indicated that [specific group differences], suggesting that [interpretation of results].\" Note: Remember to replace placeholders (e.g., [dependent variable], [independent variable], df values, F-values, p-values) with your actual data when reporting your results. "],["factorial-anova-important-for-your-assignment.html", "Chapter 10 Factorial ANOVA (Important for your assignment) 10.1 What is a Factorial ANOVA? 10.2 Example of a factorial ANOVA design 10.3 Visualizing Factorial ANOVA Data 10.4 Conducting a Factorial ANOVA", " Chapter 10 Factorial ANOVA (Important for your assignment) In the previous chapter, we explored one-way ANOVA, a statistical method used to compare means across three or more groups when you have a single independent variable. But sometimes we want to fit more than one independent variables into our research. This is where factorial ANOVA comes into play. Factorial ANOVA allows us to examine the effects of multiple independent variables on a dependent variable, as well as the interactions between these factors. In this chapter, we'll work through the basics of factorial ANOVA using a 2x2 (pronounced two-by-two) design as an example. We'll explore how to interpret main effects and interaction effects, and understand how these contribute to our overall understanding of the data. 10.1 What is a Factorial ANOVA? A factorial ANOVA is an extension of the one-way ANOVA that includes two or more independent variables. Each independent variable, or factor, can have multiple levels. When combined, these factors create various conditions or groups. The \"factorial\" aspect refers to the fact that every level of one factor is combined with every level of the other factors. For example, in a 2x2 factorial design, there are two independent variables, each with two levels, resulting in four unique experimental conditions. The following lecture chunk introduces you to the concept of factorial ANOVA: 10.2 Example of a factorial ANOVA design Let's consider a study designed to investigate how the presentation of climate change information across different psychological distances affects changes in risk perception. This experiment employs a 2x2 between-subjects factorial design, manipulating temporal distance (now vs. future) and physical distance (near vs. far). The dependent variable is the change in risk perception, measured as the difference between pre- and post-presentation scores. Research Question: How do temporal and physical distances in the framing of climate change information influence changes in risk perception? Design: A 2x2 between-subjects factorial design. Independent Variables: Temporal Distance (Between-Subjects) Now: Information presented as an immediate threat. Future: Information presented as a distant future threat. Physical Distance (Between-Subjects) Near: Information framed as affecting a geographically close location (e.g., within the participant’s country). Far: Information framed as affecting a geographically distant location (e.g., another continent). Dependent Variable: Change in Risk Perception: Calculated as the difference between post-presentation and pre-presentation risk perception scores. Higher values indicate a greater increase in perceived risk. Hypotheses H1: Temporal distance will have a main effect on the change in risk perception, with information presented as a near-term threat (now) resulting in a greater increase in risk perception compared to information presented as a future threat. H2: Physical distance will have a main effect on the change in risk perception, with information presented as affecting a near location resulting in a greater increase in risk perception compared to information presented as affecting a far location. H3: There will be a significant interaction between temporal and physical distance on the change in risk perception, with the combined effect of near-term and near-location presentations leading to the greatest increase in risk perception. Procedure Participant Recruitment: Recruit N = 128 participants (as specified by an a-priori power analysis) who are randomly assigned to one of the four conditions, ensuring equal group sizes (n = 32 per condition). Pre-Presentation Measure: All participants complete a baseline measure of risk perception using a standardised risk perception scale. Presentation of Information: Now, Near Condition: Immediate threat affecting a nearby location. Now, Far Condition: Immediate threat affecting a distant location. Future, Near Condition: Future threat affecting a nearby location. Future, Far Condition: Future threat affecting a distant location. Post-Presentation Measure: After the presentation, participants complete the risk perception scale again. Calculation of Change Scores: The change in risk perception is calculated by subtracting pre-presentation scores from post-presentation scores. 10.3 Visualizing Factorial ANOVA Data With a four-condition setup, it's tempting to visualise the data as we did for the one-way ANOVA—treating each combination of factors as its own separate condition. This approach allows us to compare across the four conditions to see where the differences lie. And from this, we can already start to see differences between the conditions. However, this method somewhat misses the core aspect of our study design. We have two separate independent variables, each with two levels, and this factorial structure isn't well presented in the previous image. By visualising the data this way, we treat each combination as an isolated condition, neglecting the interactions between the variables. To better capture the relationships in our data, it makes more sense to use a line graph with 95% confidence intervals. This type of visualisation reflects both the main effects and the interaction effect between the two variables. It allows us to see how the effect of one independent variable may depend on the level of the other independent variable. Here is the same data presented as a line graph: From the line graph we can now see that risk perception increases when climate change is presented as an immediate threat (\"Now\") compared to a future threat (\"Future\"), this is the main effect of Temporal Distance. Additionally, risk perception is higher when the threat is framed as affecting a nearby location (\"Near\") rather than a distant one (\"Far\"), this is the main effect of Physical Distance. Importantly, the lines are not parallel, suggesting a significant interaction effect between Temporal and Physical Distance. Specifically, the combination of presenting climate change as both immediate and nearby results in the greatest increase in perceived risk, while the effect is less pronounced when either the temporal or physical aspect is framed as distant. But as always, we need to check to see if these differences are occurring by chance or if it is likely to be the experimental manipulation that is causing these differences. For that we need to return to statistical analysis. The following lecture chunk is me further explaining this design: 10.4 Conducting a Factorial ANOVA 10.4.1 The Omnibus Test In factorial ANOVA, the omnibus test is our first step in assessing whether there are any statistically significant effects within the data. This test evaluates the main effects of each independent variable as well as any interaction effects between them. The main effects tell us whether each independent variable (e.g., Temporal Distance and Physical Distance) independently influences the dependent variable. The interaction effect, on the other hand, tests whether the impact of one independent variable depends on the level of the other variable. Importantly, the omnibus test does not specify which groups are significantly different from one another; it simply tells us if there is a statistically significant effect present in the data, either in the main effects or in the interaction. In this example we can see that we have a significant main effect for both temporal distance and physical distance, and a significant interaction effect. We can look at the descriptive statistics to determine the direction of each of the main effects. The mean dependent variable value (which is the difference in risk perception before and after the presented information) for the “Now” information framing is significantly higher than that of the “Future” information framing and the mean dependent variable value for the “Near” information framing is significantly higher than the “Far” information framing. The interaction effect, however, takes a little more thought. 10.4.2 Interaction Effect The interaction effect shows whether the effect of one independent variable on the dependent variable changes depending on the level of the other independent variable. In our study, for instance, the interaction effect reveals that the impact of Temporal Distance on risk perception varies depending on whether the information is framed as \"Near\" or \"Far.\" This is crucial because it highlights the combined influence of both factors, providing a deeper understanding than looking at the main effects alone. If an interaction effect is significant, it suggests that the factors do not operate independently of each other but rather interact in a way that influences the outcome variable. It’s possible to determine this from the breakdowns of means. However, it is far easier to determine this from the line graph, see pervious section. 10.4.3 Post-Hoc Tests in Factorial ANOVA Once the omnibus test reveals significant effects, particularly interaction effects, post-hoc tests are needed to pinpoint exactly where these differences lie. Post-hoc tests in factorial ANOVA are used to explore the specific group comparisons that are driving the significant findings observed in the interaction or main effects. These tests make pairwise comparisons between the levels of the independent variables while controlling for the increased risk of Type I errors that comes with multiple testing. For example, if the interaction effect is significant, post-hoc tests will help determine whether the difference between “Now” and “Future” is significant at both levels of Physical Distance (“Near” and “Far”), and vice versa. Essentially, post-hoc tests break down the overall effects detected by the omnibus test into specific, interpretable comparisons, allowing for a more detailed understanding of the nature of the effects in the data. 10.4.4 Assumption checks Before interpreting the results of a factorial ANOVA, it is crucial to ensure that the data meet the necessary assumptions for this type of analysis. These assumptions are similar to those discussed in the chapter on t-tests, specifically the assumptions of homogeneity of variance and normality. However, there are some key differences in how these assumptions apply when conducting a factorial ANOVA. Homogeneity of Variance, refers to the assumption that the variance within each group (or condition) is equal. In a factorial ANOVA, this assumption must be met not just across the levels of a single independent variable (as in a one-way ANOVA or independent samples t-test), but across all combinations of levels for both (or more) independent variables. For example, if we have a 2x2 design with Temporal Distance and Physical Distance as independent variables, we must ensure that the variance in each of the four unique conditions (e.g., \"Now_Near\", \"Now_Far\", \"Future_Near\", \"Future_Far\") is similar. Violations of this assumption can lead to biased F-statistics, making it difficult to trust the results of the ANOVA. The Levene’s Test is typically used to check this assumption, and in a factorial ANOVA, it assesses variance across all conditions simultaneously. Normality refers to the assumption that the dependent variable is normally distributed within each group. In factorial designs, we must ensure that normality holds within each combination of the independent variables. While minor deviations from normality are unlikely to impact results significantly, severe violations can lead to incorrect inferences. As with the t-test, this assumption can be evaluated using visual inspections of histograms or statistical tests like the Shapiro-Wilk test. In factorial ANOVA, however, the complexity increases with each additional independent variable, as normality must be checked within each subgroup. Factorial ANOVA also assumes that the observations are independent of each other, just as in simpler designs. This means that there should be no systematic relationship between the scores of participants in different conditions. While this assumption may not require statistical testing, it should be ensured during the design and data collection phases. "],["jasp-workship---factorial-anova.html", "Chapter 11 JASP workship - Factorial ANOVA 11.1 “Where to Click” Guide - Conducting Factorial ANOVAs in JASP 11.2 APA Style Guide for Reporting Factorial ANOVAs 11.3 Example APA Report for an Independent Samples Factorial ANOVA 11.4 Example APA Report for an Repeated Measures Factorial ANOVA", " Chapter 11 JASP workship - Factorial ANOVA In the video below, I walk through examples of analysing factorial ANOVAs. If you want to follow along, please find the corresponding question sheets and datasets available in the NS5108 module on Moodle. 11.1 “Where to Click” Guide - Conducting Factorial ANOVAs in JASP Sometimes you just want to know where to click to run the test. Below is a step-by-step guide for performing factorial ANOVAs in JASP. Refer to the video above for more context regarding these steps. 11.1.1 Conducting a fully Independent Samples Factorial ANOVA Open JASP and Load Your Data: Click on the File tab at the top left. Select Open, and navigate to the folder containing your data file. Explore data: Due to the factorial design, you will need to filter the data as you are only able to split data by one variable. To filter there is a funnel symbol to click in the top right of the screen. Once you have the filter box up you will need to bring in one of your IVs, click “=” and then type one of the levels. Then click “apply pass through filter” After this everything you run will be just on that level of the variable. Go to Descriptives -&gt; Descriptive Statistics. Move your dependent variable into the Variables box and your independent variables (factors) into the Split box. Use Plots to create histograms or boxplots for each combination of factors to check the distribution and identify potential outliers. Clear change the filtered level to the other level of the IV and repeat Once finished make sure to clear the filter entirely. Check Assumptions: Normality: Check for normality within each combination of the independent variables. Again you will need to filter to account of the design. In the Descriptive Statistics window, under Statistics, check Skewness and Kurtosis for each condition. Use the Shapiro-Wilk Test to assess normality within each condition if sample sizes are small (typically less than 50 per group). Homogeneity of Variance: Navigate to ANOVA -&gt; ANOVA. Move your dependent variable to Dependent Variable and your independent variables to Fixed Factors. Under Assumption Checks, check Homogeneity tests to perform Levene's Test. A non-significant p-value (p &gt; .05) in Levene's Test indicates that the assumption of homogeneity of variances is met across all conditions. Run the Factorial ANOVA: Ensure your dependent variable and independent variables are correctly specified. Select to report effect sizes Select to create line graphs to illustrate the relationship between variables Select to run post hoc tests Post Hoc Tests: If significant main effects or interactions are found, go to the Post Hoc Tests tab. Move your independent variables or interactions into the Post Hoc Tests box. Select a correction method (e.g., Bonferroni or Tukey) for multiple comparisons. This will help you identify where the significant differences lie among your levels of the independent variables. 11.1.2 Conducting a Factorial ANOVA with a repeated measures variable Name your repeated measures variable by clicking where it says RM Factor 1 and renaming it. Bring each of the levels of your repeated measures variable into where it says level 1 and level 2, adding more levels if needed. Add new repeated measure IVs in if needed and repeat Bring in any independent samples IV in to the between subject factors box Repeat all other steps form the independent samples factorial ANOVA 11.2 APA Style Guide for Reporting Factorial ANOVAs Here's how to format your hypotheses and report results from your factorial ANOVAs in APA style. Hypotheses for an Independent Samples Factorial ANOVA: Null Hypothesis (H₀): There wil be no significant main effect of IV1], no significant main effect of [IV2], and no interaction effect between [IV1] and [IV2] on [dependent variable]. Alternative Hypothesis (H1): There will be a significant main effect of [IV1] whereby ... Alternative Hypothesis (H2): There will be a significant main effect of [IV2] whereby ... Alternative Hypothesis (H3): There will be a significant interaction effect whereby the change in IV1 will be ... as compared to the change in IV2 11.3 Example APA Report for an Independent Samples Factorial ANOVA \"An independent samples factorial ANOVA was conducted to examine the effects of [IV1] and [IV2] on [dependent variable]. Assumption checks confirmed normality and homogeneity of variance. The results revealed a significant main effect of [IV1], F(df_between, df_within) = F-value, p = p-value, η² = effect size, and a significant main effect of [IV2], F(df_between, df_within) = F-value, p = p-value, η² = effect size. Additionally, there was a significant interaction effect between [IV1] and [IV2], F(df_between, df_within) = F-value, p = p-value, η² = effect size. Post hoc analyses using Bonferroni correction indicated that [specific group differences], suggesting that [interpretation of results].\" The following is an example analysis for a fully repeated measures ANOVA design: 11.4 Example APA Report for an Repeated Measures Factorial ANOVA \"A repeated measures factorial ANOVA was conducted to examine the effects of [Repeated Measure IV] and [Between-Subjects IV] on [dependent variable]. Mauchly's Test indicated that the assumption of sphericity had been met/violated (χ²(df) = value, p = p-value). The results showed a significant main effect of [Repeated Measure IV], F(df_between, df_error) = F-value, p = p-value, η² = effect size, and a significant main effect of [Between-Subjects IV], F(df_between, df_error) = F-value, p = p-value, η² = effect size. There was also a significant interaction effect, F(df_between, df_error) = F-value, p = p-value, η² = effect size. Using the Greenhouse-Geisser correction (if sphericity is violated), the effect remained significant/non-significant. Post hoc pairwise comparisons showed that [specific condition differences], indicating that [interpretation of results].\" "],["how-to-ask-good-research-questions.html", "Chapter 12 How to ask good research questions", " Chapter 12 How to ask good research questions The following is my lecture on how to ask good research questions: "],["introduction-to-regression-analysis.html", "Chapter 13 Introduction to Regression analysis 13.1 Data-driven decision making 13.2 Further real world examples 13.3 Use of regression analysis in psychology 13.4 Overview of regression techniques covered in this module", " Chapter 13 Introduction to Regression analysis Regression analysis is a powerful statistical tool that investigates the relationships between variables. At its core, the technique is used to understand and quantify how one variable (the criterion variable) changes in response to another or several others (predictor variables). More than merely determining a singular correlation, regression analysis offers a nuance, enabling researchers to predict outcomes and uncover sophisticated patterns embedded within datasets. 13.1 Data-driven decision making The 2011 film Moneyball is based on the true story of the Oakland Athletics baseball team's 2002 season. Their general manager, Billy Beane (played by Brad Pitt in the film), faced a problem: he had a limited budget to put together a winning team. Rather than relying on traditional baseball scouting methods, which often depended heavily on scouts' intuitions and were prone to various biases, Beane employed the skills of a young Yale economics graduate named Peter Brand (played by Jonah Hill). Brand used statistical analysis to evaluate players' values. The traditional method of valuing players was subjective. Scouts often looked at the physique of players, their style, how they moved, or even things like the attractiveness of their girlfriends as an indicator of their confidence. Beane and Brand shifted the focus to objective evidence, including statistics. At its core, the analytics used in \"Moneyball\" is about predicting runs, and more importantly, wins. Using regression analysis, they could determine which statistics were most strongly correlated with creating runs. Once they had an understanding of what leads to runs, they could use that to build a model to aid them on player acquisition decisions. The film (and also the far more nerdy book by Michael Lewis) highlights the tensions that arise when data-driven metrics clash with entrenched traditional norms. Yet, as time progresses, the efficacy of these analytical techniques becomes increasingly evident. Today, it's a rarity to encounter a professional sports team that doesn't incorporate some form of statistical analysis into its strategy and decision-making processes, showcasing the undeniable impact and relevance of regression in our modern world. 13.2 Further real world examples Of course, it’s not just the world of sports that have taken advantage of regression analysis to inform decision making. Here are some examples from everyday life where regression analysis plays a role. Streaming Services and Recommendations: Most of you probably use platforms like Netflix, Spotify, or YouTube. These platforms utilise regression analysis to predict what shows, songs, or videos you might like based on your past behaviour and the behaviour of others with similar tastes. Predicting Box-office Income from Different Forms of Advertising: Film producers often use regression analyses to gauge which adverts, from TV spots to social media campaigns, most influence cinema ticket sales, optimising advertising budgets for upcoming films based on these insights. 13.3 Use of regression analysis in psychology Regression analysis is also a cornerstone of psychological research, as it allows psychologists to simultaneously explore and dissect the influence of numerous variables on a single outcome variable. Such a comprehensive approach is indispensable in a field like psychology, where behaviours and mental processes are often the outcome of a web of interconnected variables. For instance: Predictors of Job Satisfaction: Organizational psychologists could use regression to determine which factors (e.g., salary, working hours, team dynamics, or leadership style) are the most significant predictors of job satisfaction among employees. Influence on Social Behaviours: Social psychologists might employ regression analysis to understand how various factors like media exposure, peer influence, and past experiences predict certain social behaviours or attitudes, such as aggression or altruism. Personally I'm interested in public health and climate change. Regression is indispensable for studying areas like these: Determinants of Vaccine Hesitancy: I've used regression analysis to predict the likelihood of individuals being hesitant to take vaccines. Predictors for such a analysis being demographic factors like age and education level, psychological factors such as risk perception and trust in healthcare, as well as societal variables like exposure to misinformation on social media. Predictors of Pro-Environmental Behaviours: I am currently using regression model to predict pro-environmental behaviour through varying levels of capability, motivation, and opportunity (i.e. the COM-B model). Click for test yourself activity Reflect on a psychological phenomenon or behaviour that has piqued your interest or that you've recently studied. Jot down potential factors or variables that you believe might influence this phenomenon or behaviour. Consider how these factors might be integrated into a regression model. Which variable would you choose as the dependent variable (the variable you will be predicting)? What variables might serve as the predictors of the dependent variable? 13.4 Overview of regression techniques covered in this module In this handbook we will cover three main regression techniques: Simple linear regression (basically just a fancy correlation) Multiple linear regression (the technique that is required for your assignment) Binary logistic regression (multiple regression with a binary, instead of continuous, criterion variable) Continue with statistics in psychology and you’ll learn further forms of regression, and regression like, analysis techniques such as: Ordinal regression Mediation and moderation Multi-level modelling Structural equation modelling "],["simple-linear-regression.html", "Chapter 14 Simple linear regression 14.1 Recap of correlation 14.2 Simple linear regression 14.3 Week 1 - Test yourself mcq’s", " Chapter 14 Simple linear regression 14.1 Recap of correlation At its core, correlation provides a measure of how data points on two variables are related to one another. Positive Correlation: (top left figure) As one variable increases, the other also increases. Negative Correlation: (top right figure) As one variable increases, the other decreases. No Correlation: (bottom figure) There is no discernible pattern in the relationship between the two variables. The strength and direction of the correlation are represented by the correlation coefficient, typically denoted as \\(r\\). The value of \\(r\\) ranges from -1 to +1. An \\(r\\) value closer to +1 indicates a stronger positive correlation. An \\(r\\) value closer to -1 indicates a stronger negative correlation. An \\(r\\) value closer to 0 suggests little to no correlation. 14.1.1 Correlation as a Statistical Test Going beyond visualisation of relationships, correlation can also be used as a formal statistical test to determine if there's a significant linear relationship between two variables. When conducting a correlation test, the p-value informs us about the significance of our observed correlation coefficient (\\(r\\)). If the p-value is below a predetermined threshold (commonly 0.05), we infer that the correlation in our sample is likely not due to random chance, and thus, there's a statistically significant relationship between the two variables. Reminder: what is a p-value Surface Level Explanation: A p-value is a number between 0 and 1 that tells us if the result of an experiment is likely due to chance or if there's something more going on. A small p-value (typically less than 0.05) suggests that the result is significant and not just a random occurrence. Intermediate Explanation: A p-value represents the probability of observing the data (or something more extreme) given that a specific null hypothesis is true. If we have a p-value less than a pre-decided threshold (like 0.05), we reject the null hypothesis in favour of the alternative hypothesis. This suggests that our observed data is unlikely under the assumption of the null hypothesis. However, a smaller p-value does not necessarily mean a result is \"meaningful\"; it just indicates it is statistically significant. In-Depth Explanation: Mathematically, the p-value is the probability of observing data as extreme as, or more extreme than, the observed data under the assumption that the null hypothesis is true. This is not a direct measure of the probability that either hypothesis is true. Instead, it's a measure of the extremity of the data relative to a specific model. Lower p-values suggest that the observed data are less likely under the null hypothesis, leading us to reject then null in favour of alternative hypothesis. However, it's crucial to understand that a p-value doesn't measure the size of an effect or the practical significance of a result. Furthermore, while a threshold of 0.05 is common, it's arbitrary and must be chosen with context and caution. Lastly, it's essential to remember the p-value is contingent on the correctness of the underlying statistical model and the level to which the data meet the statistical assumptions. Misunderstandings and misuse of p-values have led to various controversies in the scientific community (see the replication crisis). See Lakens (2022) Improving Your Statistical Inferences for a comprehensive overview of p-values. 14.1.2 Statistical assumptions of correlation In frequentist statistical analysis, we often aim to use parametric tests when the statistical assumptions underlying these tests are met, as these tests can be more sensitive to smaller effect sizes. For correlation analysis, Pearson's correlation is the most commonly used parametric test and Spearman's Rank correlation is the most commonly used non-parametric test. Assumption checks for correlation analysis In my experience of teaching regression, the assumption checks are often the part of statistics that confuse students the most. For now, I've put them safely away in this box, so as not to scare you off in the first week :-) We will do a fully refresh of the theory behind assumption check aspect next week when we cover multiple regression. For correlation analyse the the following assumptions should be checked and assessed: Linearity: Both variables should have a linear relationship, which means that when you plot them on a scatterplot, the distribution of data points should roughly form a straight line. Normality of Residuals: The data pairs should be normally distributed, meaning both variables being analysed should be approximately normally distributed when considered together. We do this by checking the normality of the residuals. Homoscedasticity: The spread (variance) of the residuals are constant across all values of the variable on the x-axis. i.e. data points shouldn't start of clustered and the widen out (like a funnel) further along in the correlation. Absence of Outliers: Outliers can disproportionately influence the correlation coefficient. So, it's important to check for and consider the impact of any outliers in the data. Outliers can be checked through the use of a boxplot or calculated manually. 14.2 Simple linear regression The content in the sections above, likely reflects the extent to which you explored correlation in your first year here with us. Regression analysis is simply an extension of this knowledge. Central to this understanding is the concept of the \"line of best fit\". While you might have previously used this line as a visual representation of the relationship between two variables, in linear regression, it takes on a more pivotal role. 14.2.1 The line of best fit In the scatter plot below, you'll see data points representing the hours of sleep and test performance for various participants. Notice how the data points are positively correlated? We can capture this trend by drawing a straight line through the data. This line, which we call the \"line of best fit,\" gives us a simplified representation of the relationship between hours of sleep and test performance. The optimal line of best fit is the one that minimises the total distance between itself and all the individual data points on the plot. The Concept of Residuals Even with the best possible straight line drawn through our data points, it's rare that the line will pass exactly through every point. The vertical distance between each data point and our line is called a \"residual\". For every data point, the residual is the difference between its actual value and what our line predicts the value should be. If our line of best fit is doing its job well, these residuals will be quite small, indicating that our predictions are close to the actual data. However, if the residuals are large, it suggests that our line might not be capturing the relationship between the two variables adequately. We talk more about residuals, in the context of our assumption checks, in the next chapter. The purpose of the line of best fit is to model the relationship between test performance and number of hours slept. What does this model suggest the performance on the test will be if a participant gets 7 hours sleep? 100110120130140 Hint Find the point for 7 hours on the x-axis. Draw your finger up to the blueline. Draw your finger across to the y-axis. Take that number. That is the test score that our model suggest for a participant that has 7 hours of sleep. 14.2.2 The equation of a line Every straight line on a scatter plot can be written as a simple formula: \\[ Y = mX + c \\] Where: \\(Y\\) is the dependent variable. \\(X\\) is the independent variable. \\(c\\) is where the line intersects the Y-axis, representing the predicted test performance when no hours of sleep are had. \\(m\\) is the slope of the line, indicating the predicted change in test performance for each additional hour of sleep. To make the formula for our previous scatter plot all we need is to work out how much the blue line \"goes up\" by how far it \"goes along\", and where the line crosses our y-axis. The below scatter plot is the same data but with each axes going to zero this time. This allows us to get our \\(c\\) value, 50, and by taking any two points on the line we can work out our \\(m\\) value, in this case 10 (60/6). This give us the following equation of our line: \\[ y = 10x + 50 \\] Look back at the question from before. All we need to do now is sub in the number 7 and we get our predicted test score: (10*7) + 50 = 120 14.2.3 Is this a good model? \"All models are wrong, but some are useful.\" - George E. P. Box This oft-cited quote sums up the a central truth in statistics and data modelling: no model can capture the full intricacy and unpredictability of real-world phenomena. However, that doesn't diminish the value of models. When a model simplifies complex systems and highlights important relationships, it can offer invaluable insights and guide decision-making. The line on our sleep and test score scatter plot (above) would appear to be quite representative of our data and therefore is perhaps fairly a good model (so long as our sample is representative of the population we want to use or model on in the future). However, if we try and use the relationship between participants height and their test scores as out model, this is likely to be a fairly poor model: The model derived from this data, \\(y = -0.05x + 77.89\\), would actually give an answer close to 70 no matter an individuals height. In other words, whether you're taller or shorter, the model essentially shrugs and predicts something close to 70 (the mean of the dataset). I would not describe this as a very useful model, as in, I would not spend an evening on the rack to try and make myself taller for the test. While we can see it on the scatter plot, none of the numbers in our equations allow us to gauge how 'useful' or fitting the sleep model is compared to the height model. To understand model fit we need the final new concept for this week, shared variance 14.2.4 Shared Variance When examining the relationship between two variables, the proportion of variation in one variable that can be predicted or explained by the other variable is known as the shared variance . In correlational research, this concept is crucial as it tells us how well one variable explains the variation in the other variable. In other word how useful would it be if we used a scores from one variable to predict scores on the other variable. A good way to think about this visually is with a Venn diagram. This value of shared variance is derived by taking the square of \\(r\\), the correlation coefficient, giving us our \\(R^2\\) value for the model. Take for instance, for our sleep model, if we run a pearsons correlation test on the data we find the relationship has a correlation coefficient of \\(r\\) = 0.94, squaring this give us a \\(R^2\\) of 0.89 between our predictor (study hours) and the outcome (test scores). Another way to talk about \\(R^2\\) is to say that 89% of the variance in test scores can be predicted or explained by the number of hours studied. The remaining 11% of the variance is due to factors not included in our model or random error. When we say two variables share a certain percentage of variance, it's a indication of the strength and utility of the relationship. However, while a high shared variance can be promising, it's essential to remember that correlation does not imply causation. Other underlying factors, third variables, or even coincidences could create correlations. In practical terms, understanding shared variance is critical for researchers. When a significant shared variance exists, the predictor variable becomes valuable in understanding, predicting, or even potentially influencing the outcome variable. However, the unexplained variance might also prompt researchers to consider additional predictors or factors that weren't initially in the model. Next week we'll start looking at multiple regression modelling, which as the name suggest involves multiple variables sharing variance with a dependent variable. 14.3 Week 1 - Test yourself mcq’s 1. Which of the following best describes correlation? The probability of one event occurring given another has occurred. The relationship between the means of two groups. The strength and direction of a linear relationship between two variables. A statistical test to determine significance. 2. A negative correlation coefficient between two variables indicates: there's no relationship between the two variables. as one variable increases, the other decreases. both variables increase together. the data is scattered without any clear trend. 3. Which statistical method is used to predict the value of one variable based on the value of another variable? T-test. ANOVA. Simple linear regression. Chi-square test. 4. In the context of a correlation test, the p-value informs us: about the strength of the correlation. about the slope of the line of best fit. whether the observed correlation is likely due to random chance. about the effect size of the relationship. 5. What does the \"line of best fit\" represent in the scatter plot? The line that maximizes the total distance between itself and the data points. The line that minimizes the total distance between itself and the data points. The line that represents the mean of the data points. The line that represents the mode of the data points. 6. In the formula \\(Y=mX+c\\), what does \\(c\\) represent? The slope of the line. The predicted change in test performance for each additional hour of sleep. Where the line intersects the Y-axis. The independent variable. 7. What is the shared variance in a simple linear regression? The proportion of variation in one variable that can be predicted by another variable. The difference in variance between two variables. The total variance of both variables. The square root of the variance of both variables. 8. How is the shared variance value derived? By taking the cube of the correlation coefficient. By taking the square root of the correlation coefficient. By dividing the correlation coefficient by 2. By taking the square of the correlation coefficient. "],["jasp-workshop---simple-linear-regression.html", "Chapter 15 JASP Workshop - Simple linear Regression 15.1 Simple linear regression - example analysis 15.2 \"Where to click\" guide - simple linear regression analysis 15.3 JASP lab exercises 15.4 APA style guide", " Chapter 15 JASP Workshop - Simple linear Regression JASP is a free, open-source statistical software package with a user-friendly, point-and-click interface suitable for research in psychology. It offers a wide range of statistical analyses, from basic descriptive statistics to advanced methods like regression and ANOVA. JASP is available on most of the university computers, however, we recommend that you also install a personal version for your own laptop or desktop computer so that you can continue learning outside of class time. JASP can be downloaded here: www.jasp-stats.org/ 15.1 Simple linear regression - example analysis In this video I go through the analysis and answers for Simple Linear Regression Exercise 1. If you would like to follow along (or have a go at the exercise yourself first) the question sheet and data can be found in the Week 1 module area for NS5108. 15.2 \"Where to click\" guide - simple linear regression analysis The following is a step by step guide for performing a correlation and simple linear regression analysis in JASP. Watch the video above for more context related to these steps. 15.2.1 Correlation Open JASP. Load the Data: click on the File tab at the top left and select Open. Then navigate to the folder containing your data file and open it. Visualise the data: Identify the two variables you want to run your correlation on. Click Descriptives and move the two variables over to the Variables box. Below, in the Customizable plots drop down tick the Scatter plots. You may want to click none for the graphs above and to the right (to make your plot clearer). Also change that regression line (line of best fit) to linear. Identify outliers: Also in the customizable plots drop down, select boxplots. Tick label outliers if they show any dots outside of the whiskers for the box plots. Note and interpret any outliers. Calculate correlation coefficient: click Regression -&gt; Correlation from the top bar. Pick the variables you are interested in running a correlation on. Extract and interpret the Pearson's \\(r\\) and p-value. 15.2.2 Simple linear regression Check normality (of residuals): Click Regression -&gt; Linear Regression from the top bar. Pick your criterion variable (the variable you are aiming to predict) and place this in the Dependent variable box. Pick your predictor and place that in the Covariates box. In the Plots drop down select Residuals histogram. Visually interpret this histogram for an indication of this assumption check. Check Homoscedasticity: Also in the Plots drop down, select Residuals vs. predicted. Visually interpret this plot to assess Homoscedasticity. Extract the \\(R^2\\) value: This should be in the first table of the Linear Regression section. Determine the significance of the model: This can be obtained though the ANOVA table. F(df,df)=F-value, p=p-value. Extract values for your model regression equation: H1 intercept = \\(c\\), H1 variable = \\(m\\) 15.3 JASP lab exercises There are two additional exercises on Moodle. Each exercise question sheet comes with a dataset and answers sheet. Work through the question and then check your answers with the answer sheet. 15.4 APA style guide Here is an example of a hypothesis for a correlational analysis: Null Hypothesis (H0): There is no correlation between participants' age and their scores on a cognitive ability test. Alternative Hypothesis (H1): There is a correlation between participants' age and their scores on a cognitive ability test. Here is an example of a correlation analysis reported in APA style: A Pearson correlation was conducted to assess the relationship between participants' age and their scores on a cognitive ability test. Assumptions checks were performed to ensure no violation of the assumptions of normality of residuals, linearity, and homoscedasticity. There was a significant negative correlation between age and cognitive ability scores, r(98) = -.45, p &lt; .001, with older participants tending to have lower scores on the cognitive ability test. As such, the null hypothesis can be rejected. Here is an example of a hypothesis for a simple linear regression analysis: Null Hypothesis (H0): The number of study hours is not a significant predictor of test scores. Alternative Hypothesis (H1): The number of study hours is a significant predictor of test scores. Here is an example of a simple linear regression analysis reported in APA style: A simple linear regression was conducted to predict test scores based on the number of study hours. The assumptions of linearity, independence, and normality were checked and met. The results indicated that there was a significant relationship between the number of study hours and test scores, F(1, 98) = 34.5, p &lt; .001. As such, the null hypothesis can be rejected The R² value was .26, indicating that approximately 26% of the variance in test scores can be explained by the number of study hours. The regression equation was found to be: Test Score = 50.2 + 6.7*Study Hours. For each additional hour of study, there was an increase of 6.7 points in the test score. The intercept value of 50.2 indicates that a student who does not study at all (0 hours) is expected to score 50.2 points on the test. "],["multiple-regression.html", "Chapter 16 Multiple regression 16.1 Shared variance with multiple variables 16.2 The Regression Equation 16.3 Interpreting a regression model 16.4 Building our regression equation from the model 16.5 So what? 16.6 Week 2 - Test yourself mcq’s", " Chapter 16 Multiple regression Multiple regression is a method that helps us predict an outcome variable based on several independent variables. It is a technique that tells us which variables are the most important for predicting an outcome, and how such predictor variables interact. By building a prediction model, multiple regression allows us to understand how variables are connected and helps us make powerful data-informed decisions. 16.1 Shared variance with multiple variables In the previous chapter on simple linear regression, we suggested that one predictor variable (exercise) shared variance with another variable (well-being). Visually, this can be represented using a Venn diagram with two circles: one for exercise and one for well-being. The overlap between these two circles represents the shared variance, or the extent to which variations in exercise can explain variations in well-being. This shared variance can be represented numerically as the square of the correlation coefficient between the two variables. In this hypothetical example exercise is correlated with well-being with a correlation coefficient of r=0.307 (i.e. a moderate-positive correlation) and, as such, it can be said that exercise explains 9.4% of the variance in well-being (\\(R^2\\)=0.094) In a multiple regression analysis, we use two or more independent variables to predict the value of a dependent variable. In doing so, each independent variable may share some variance with the dependent variable, but also shares variance among the other independent variables. Understanding this shared variance is crucial for interpreting the results of a multiple regression analysis. For instance, let's say we're using three independent variables to predict well-being: exercise, age, and income. Each of these variables could contribute to the variance of well-being. When we add the variables one-by-one, the overlap of each independent variable circle with the well-being circle represents the unique shared variance each variable contributes to well-being. When adding our samples age data, to help us explain well-being, we see significant correlations for the three different relationships. 1. a positive correlation exists between exercise and well-being, 2. a negative correlations exist between age and well-being 3. a negative correlation between age and exercise. Why are the \\(R^2\\) positive for negative correlations? Because the \\(R^2\\) is derived from squaring an r value, and when you square any negative value, it loses its negative sign. Both age and exercise now explain some of the variability in well-being, and since they are also related to each other, some of the variance they explain overlaps. Finally, when we add income, we find that the variable shares variance with both age and well-being. Income therefore adds more variance explained to our model while also overlapping with some of the variance explained by age The outcome of this is that each variable that we add here allowed us to better explain our concept of well-being. And as such, if we were to know a person's amount of exercise, age, and income we could predict their well-being to a greater degree than if we just know their amount of exercise. Extra detail on overlapping variance It's important to understand that the Venn diagrams in this guide offer a simplified representation of variance explained by predictor variables in a regression model. In reality, when two predictor variables have overlapping variance, this shared variance can contribute more to the model than what the individual variables might suggest. This occurs due to interaction effects, where the combined influence of two predictors can explain more variance in the dependent variable than the sum of their individual effects. When looking at the Venn diagrams, remember that this additional explained variance from interactions isn't explicitly depicted, but it's crucial to consider when interpreting regression results. This is why, as you'll see later on, the final \\(R^2\\) is greater than the sum of all the simple (univariate) regressions. 16.2 The Regression Equation As you've probably realised, using Venn diagrams to illustrate the relationships among multiple variables can get quite confusing, especially when there are numerous variables that all have some degree of correlation with each other. That's where the regression equation comes in handy. The regression equation is a mathematical representation of the relationships among the variables in a multiple regression analysis. It shows how the dependent variable is predicted by the independent variables. In its abstract form, the regression equation can be written as: \\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_kx_k + \\varepsilon \\] Here, \\(y\\) is the dependent variable, \\(x_1\\), \\(x_2\\),...,\\(x_k\\) are the independent variables (i.e. the amount of exercise, age, and income of the participants), \\(\\beta_0\\) is the intercept, \\(\\beta_1\\), \\(\\beta_2\\),...,\\(\\beta_k\\) are the coefficients for the independent variables, and \\(\\varepsilon\\) is the error term, representing the unexplained variability in the dependent variable (i.e. the left over white space in the Venn diagram figure above). The intercept \\(\\beta_0\\) represents the value of the dependent variable when all independent variables are zero. This \\(\\beta_0\\) is equivalent to the intercept, \\(c\\), value in the equation from last week. The coefficients (all the remaining \\(\\beta\\) values) represent the average change in the dependent variable for a one-unit change in the respective independent variable, holding all other variables constant. Just like the gradient of the line \\(m\\) value in the equation from last week. The \"...\" and the \\(k\\)'s are just a way of saying \"add in as many beta values as you have variables in your model\". Running our regression analysis allowed us to substituent numbers for all of the coefficients to the degree that all we are left with are \\(x\\)'s, that we can use to sub in our data. 16.3 Interpreting a regression model I will explain the steps for running a regression model, in full, in this weeks JASP workshop. For now, I want us just to focus on the interpretation. Here is the JASP output for a model that uses the variables exercise, age and income to predict well-being scores. Let's break this down by table. 16.3.1 Descriptive statistics The descriptive statistics are useful for context. This detail isn't given here but I can tell you that in this dataset (N=1000), the well-being score is out of 100, the exercise variable is based off of number of hours of exercise a week, income is in £GBP, and age is in years. 16.3.2 Model Summary The Model Summary table provides an overview of the overall performance of the regression model. It contains key statistics that helps us assess how well the model fits the data. The table contains two rows \\(H_0\\) and \\(H_1\\). \\(H_0\\) refers to the null model, a model that assumes that none of the predictors in the model have an effect on the dependent variable, meaning that the coefficients for all predictors are equal to zero. \\(H_1\\) is our alternative model, the model containing our variables. This is the line that interests us. The main details of interest are: The Adjusted \\(R^2\\) Value: This is a modified version of \\(R^2\\) that takes into account the number of independent variables in the model. It allows us to say how much variance in our dependent variable is explained by our predictor variables. RMSE: RMSE stands for Root Mean Square Error. It is a measure of the differences between values predicted by a model and the values actually observed. RMSE is a measure of how spread out the residuals are. In other words, it tells you how concentrated the data is around the line of best fit. It is used as our \\(\\varepsilon\\) in the regression model. We will often take the Adjusted \\(R^2\\) for multiple regression and the \\(R^2\\) only for simple regression (regression with just one predictor variable). For our well-being model, the Adjusted \\(R^2\\) of 0.469 tells us that our three variables together explain 46.9% of the variability in well-being. Which is pretty good. 16.3.3 ANOVA The ANOVA table provides information about the overall significance of the model. It shows whether the model is better at predicting the dependent variable than a model with no independent variables (i.e., the null model). The findings from this table will need to be reported in our write-up, however, to interpret it, all we actually need is the p-value. A significant finding on this test tells us that our model is significantly better than the null model. What is an ANOVA ANOVA stands for ANalysis Of VAriance. We'll come back to ANOVA in week 4. For now, just think of it as a slightly fancier t-test, a test that compares across groups (rather than correlates). In this case it is comparing the null model to the alternative model and seeing if they are significantly different. 16.3.4 Coefficients Finally, to understand the role of each variable in the model we need to look at the coefficients table. This table provides detailed information about the individual variables in the model. It includes statistics for each independent variable and the intercept for the model. There are two columns that are key to our understanding of the model. The Unstandardized column and the p column. Starting with the p column, this is the probability of obtaining a t-value as extreme as the one observed if the null hypothesis is true. A small p-value indicates that the variable plays a statistically significant role in our model. It could be the case that we include a variable that is not related to our dependent variable or the variance it explains is accounted for by other variables in the model. In those cases, the p-value would be above 0.05. A p-value of &lt;.001 for all three of our variables indicates that they are all relevant to include in our model. The values in our Unstandardized column correspond to the \\(\\beta\\) values in our regression equation. They play the same role as the gradient of the line of best fit from simple regression last week. They represent the absolute change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant. In our model, this table tells us that: For every 1 hour increase of exercise a week, well-being increases by 1.88 points. For every 1 year of ageing, well-being decreases by 1.09 points. For every £1 of income, well-being increased by 0.001 points. That last finding is not as intuitive as it could be, so it makes sense to convert income into values of 1000's of pounds (i.e. if someone earns £32500 a year they can be said to earn 32.5 thousand pounds a year). To do this all I need to do is divide each of our income values by 1000 and run the model again. Doing so gives us this: For every £1000 of income, well-being increases by 1.35 points. 16.4 Building our regression equation from the model From this we can now start building out our regression equation. We have three variables predicting well-being so all we need to do is fill in the \\(\\beta\\) values and the \\(\\varepsilon\\) in the following equation. \\[ Wellbeing = \\beta_0 + \\beta_1income + \\beta_2age + \\beta_3exercise + \\varepsilon \\] \\(\\beta_0\\) = The unstandardized (beta) value for the intercept of \\(H_1\\) = 35.25 \\(\\beta_1\\) - \\(\\beta_3\\) = the unstandardized (beta) value of the three variables = 1.35, -1.09 and 1.88 \\(\\varepsilon\\) = the Root Mean Square Error (RMSE) of \\(H_1\\) = 15.7 This give us the following regression equation: \\[ Wellbeing = 35.25 + 1.35income -1.09age + 1.88exercise + 15.7\\] From this equation, if we knew an individual's income, age and how much exercise they engaged in per week we could estimate their well-being with a fairly high degree of accuracy. Alongside the equation it is important to note our Adjusted \\(R^2\\) of 0.469, and indicate that this means that over half of the variability in well-being is not explained by the variables we have included in the model. 16.5 So what? What is the point of doing all this work? Well, if I'm wanting to increase the well-being of my local community this finding might put me off putting money into community exercise classes. While there all likely other benefits of such an intervention, there are only so many hours in a week and, based on this data, a 1.88 point increase in well-being (on a scale out of 100) for an extra hour of exercise might not be the most cost-effective way of spending my limited pool of money. Then again, it's not like we can stop someone from ageing, or increase their income. Perhaps further research that compares the effects of, for example, exercise, mindfulness and diet, while controlling for demographic factors, would be useful to inform what type of intervention might be more cost-effective. 16.6 Week 2 - Test yourself mcq’s What is multiple regression? Using a single categorical variable to predict the value of a dependent variable. A method that helps predict an outcome variable based on one independent variable. A method that helps predict an outcome variable based on several independent variables. A measure of the association between two variables. What is the regression equation? A graphical representation of the relationships among the variables. A mathematical calculation of the shared variance. A mathematical representation of the relationships among the variables in a multiple regression analysis. An empirical observation of the relationships among the variables. In the regression equation, what do the β values represent? The dependent variable. The coefficients for the independent variables. The error term. The independent variables. What do the values in the Unstandardized column in the coefficients table represent? the shared variance between the variables. the average change in the dependent variable for a one-unit change in the respective independent variable, holding all other variables constant. the correlation coefficient between the variables. the absolute value of the correlation coefficient between the variables. What does the Adjusted \\(R^2\\) value tell us in a multiple regression analysis? The strength of the linear relationship between the dependent variable and each independent variable. The shared variance among the independent variables. The percentage of variability in the dependent variable that is explained by the predictor variables. The effect size of the independent variables on the dependent variable. "],["statistical-assumption-checks-for-multiple-regression.html", "Chapter 17 Statistical assumption checks for multiple regression 17.1 Check for Linearity 17.2 Check for Multicollinearity 17.3 Check for Outliers 17.4 Checks of residuals", " Chapter 17 Statistical assumption checks for multiple regression At this point in your studies, it's important for you to understand how to assess and report the statistical assumptions underlying regression analysis. However, it's not until more advanced stages in your career that you'll learn how to address such violations of assumptions. For now, just interpret and report these assumptions. If the results deviate greatly from the assumptions, then it's important to report such deviations and state that findings \"should be taken with caution\". Here are the key assumptions in multiple regression and how to check for them: 17.1 Check for Linearity The assumption of linearity states that the relationship between the independent and dependent variables is linear. You can check this visually by examining scatter plots of the independent variables against the dependent variable. 17.2 Check for Multicollinearity Multicollinearity occurs when two or more independent variables in a regression model are closely related, making it difficult to disentangle their individual effects on the dependent variable. This can lead to unstable coefficient estimates, as small changes in the data can result in significant changes in the estimates. You can assess multicollinearity by creating a correlation matrix of the independent variables and looking for variables with a correlation coefficient greater than 0.7 or less than -0.7. However, this is not a strict rule, just an indication of a strong correlation. In the table above, suppose we use these five personality constructs to predict a behaviour, such as recidivism. Checking the correlation coefficients, the largest value is -0.368, indicating that multicollinearity is not likely an issue in this case. Be mindful not to confuse correlation coefficients with p-values, as it's an easy mistake to make. While multicollinearity can make interpretation challenging, it's not necessarily a deal-breaker. In some cases, the focus may be on predicting the outcome rather than interpreting the individual contributions of each predictor. For a more in-depth discussion on multicollinearity and its implications see Vatcheva et al (2016). 17.3 Check for Outliers Outliers are data points that differ significantly from other observations. They can be detected through various methods. In a boxplot, outliers are typically displayed as individual points outside the whiskers, which extend from the first and third quartiles (Q1 and Q3) to Q1 - 1.5 * IQR (interquartile range) and Q3 + 1.5 * IQR, respectively (see this link for a full explanation of how read a boxplot). JASP can automatically identify outliers in boxplots, making it easier to spot them. However, addressing outliers is often a contentious issue because their treatment can have a significant impact on the results of a statistical analysis. The decision to keep or remove outliers depends on multiple factors. First, it's important to consider the context of your data. Outliers could be the result of measurement errors, data entry errors, or other random anomalies. However, they could also represent genuine extreme values that offer valuable insights. For example, in a study on wealth distribution, billionaires would be outliers but they are a real-world phenomenon that might be important to the research. Secondly, outliers can heavily influence other data assumptions, such as linearity, normality of residuals, and homoscedasticity. It is crucial to evaluate whether outliers are affecting these assumptions and to justify their removal or retention. This, in fact, can be away of addressing the violations of assumptions listed in this chapter. Thirdly, outliers can disproportionately affect key statistical measures like the mean and standard deviation. It's very easy to fall into the trap of removing outliers until a significant results appears. This kind of practice may fall into that grey area of academic misconduct that we discussed in first year, known as Questionable Research Practices. Ultimately, the decision to handle outliers involves a balance of statistical judgment and domain expertise. It's necessary to ensure that any decision involves transparently reporting and justifying your decisions. 17.4 Checks of residuals The last two assumption checks require an examination of the residual. Residuals are a fundamental concept in regression analysis, representing the difference between observed and predicted values of the dependent variable. In a simple regression, we visualised the predicted value with our line of best fit, and the observed value are just the data point in our relationship. For each observation, a residual is calculated by subtracting the predicted value from the observed value. In simple linear regression, where there is one independent variable, the residual for each observation is the distance from the data point to the regression line. In multiple regression, it is the distance from the observation to the regression plane. The goal of regression analysis is to minimize the residuals, or in other words, to make the predicted values as close as possible to the observed values. Residuals can be used to evaluate the fit of a regression model, to detect outliers, and to check for any violations of the assumptions of regression. 17.4.1 Check for Normality of Residuals The assumption of normality of residuals is important for hypothesis testing in linear regression. It ensures that the standard errors of the coefficients are unbiased and that the significant testing for the coefficients are valid. This, in turn, helps us make valid inferences about the relationships between the variables. In contrast, checking for normality of the individual variables is not required for linear regression. Instead, the focus is on the distribution of the residuals, which should be normally distributed if the model's assumptions are met. The residuals represent the differences between the observed and predicted values of the dependent variable, and their distribution tells us about the model's fit and the relationships between the variables. If you're working on a program where you can calculate the residuals, then you can check the normality of the residuals using the Shaperio Wilks test or by assessing the skewness and kurtosis. In JASP the only means of checking the normality of residuals is through the normality of residuals plot option. 17.4.2 Check for Homoscedasticity Homoscedasticity, the word that strikes fear into undergrad (and most postgrad) psychology students throughout the land, is not as complicated as it sounds. \"Homos\": Greek for \"same\" or \"equal.\" In the context of homoscedasticity, it refers to the equality or sameness of the variances. \"Skedasis\": Greek word related to the concept of dispersion or spreading out. In statistics, it is associated with the variance or scatter of data points. The scatter plots below demonstrate how such data might look. Heteroscedasticity (i.e. different \"spreadness\") means that the line of best fit becomes less reliable at different points in the model. In this context, what we're looking for an equal distribution of data points along the line of best fit, avoiding a funnel-shaped pattern. When dealing with regression analyses that involve more than two predictor variables, it's often not feasible to visualise the data. In such cases, a plot of predicted variables against residuals can be used to check for homoscedasticity. In this plot, we are, again, looking for the absence of a funnel shape in the points. This check can also be performed using z-scores of the predicted values against the residuals; a level line in this context would indicate homoscedasticity (a uniform spread of the residuals). This method is used in software like SPSS for checking homoscedasticity, so you may see some other guides talking about this if you read wider on the topic. "],["jasp-workshop---multiple-regression.html", "Chapter 18 JASP Workshop - Multiple regression 18.1 Multiple regression - Example analysis 18.2 \"Where to click\" guide - Multiple Regression 18.3 JASP lab exercises 18.4 APA Style Guide for Multiple Regression", " Chapter 18 JASP Workshop - Multiple regression JASP is a free, open-source statistical software package with a user-friendly, point-and-click interface suitable for research in psychology. It offers a wide range of statistical analyses, from basic descriptive statistics to advanced methods like regression and ANOVA. JASP is available on most of the university computers, however, we recommend that you also install a personal version for your own laptop or desktop computer so that you can continue learning outside of class time. JASP can be downloaded here: www.jasp-stats.org/ 18.1 Multiple regression - Example analysis In this video I go through the analysis and answers for Multiple Regression Exercise 1. If you would like to follow along (or have a go at the exercise yourself first) the question sheet and data can be found in the Week 2 module area for NS5108. 18.2 \"Where to click\" guide - Multiple Regression Open JASP. Load the Data: click on the File tab at the top left and select Open. Then navigate to the folder containing your data file and open it. Identify which of your variable will be your predictors and which will be the dependent variable (the variable you are aiming to predict). Check linear relationships between predictor variables: Click Regression -&gt; Correlation from the top bar. Move your predictor variables and your dependent variable over to the Variables Box. Click the Scatter plots options. Visually interpret the Correlation plots to determine if all the predictor variables have a linear relationship to the dependent variable. Check for multicollinearity: Within the same results section from step 3 check the correlation table. Interpret if the correlation coefficients between your predictor variables is large enough to indicate an issue with multicollinearity. Check normality (of residuals): Click Regression -&gt; Linear Regression from the top bar. Pick your dependent variable and place this in the Dependent variable box. Pick your predictor variables and place them in the Covariates box. In the Plots drop down select Residuals histogram. Visually interpret this histogram for an indication of this assumption check. Check Homoscedasticity: Also in the Plots drop down, select Residuals vs. predicted. Visually interpret this plot to assess Homoscedasticity. Extract the Adjusted \\(R^2\\) value: This should be in the first table of the Linear Regression section. Determine the significance of the model: This can be obtained though the ANOVA table. F(df,df)=F-value, p=p-value. Extract values for your model regression equation: This will be the unstandardized values in the coefficients table. See below for how to report your findings. 18.3 JASP lab exercises There are two additional exercises on Moodle. Each exercise question sheet comes with a dataset and answers sheet. Work through the question and then check your answers with the answer sheet. 18.4 APA Style Guide for Multiple Regression 18.4.1 Hypothesis for Multiple Regression Analysis Null Hypothesis (H0): Neither self-esteem nor study hours are significant predictors of academic performance. Alternative Hypothesis (H1): At least one of self-esteem or study hours is a significant predictor of academic performance. 18.4.2 Reporting Multiple Regression in APA Style A multiple linear regression was conducted to explore the impact of self-esteem and study hours on academic performance. Assumptions of linearity, independence, multicollinearity, and normality were checked and met. The multiple regression model significantly predicted academic performance, F(2, 97) = 36.8, p &lt; .001, \\(R^2\\) = .43. The regression equation was found to be: Academic Performance = 45.2 + 4.1 * Self-Esteem + 5.2 * Study Hours. Both self-esteem and study hours significantly added to the prediction of academic performance. Specifically, self-esteem was a significant predictor, t(97) = 2.9, p = .005, and contributed to an increase of 4.1 points for every unit increase in self-esteem. Study hours were also a significant predictor, t(97) = 5.1, p &lt; .001, and contributed to an increase of 5.2 points for every additional hour spent studying. Overall, the model explained approximately 43% of the variance in academic performance (R² = .43). As such, the null hypothesis can be rejected, indicating that at least one of the predictors significantly impacts academic performance. "],["logistic-regression.html", "Chapter 19 Logistic regression 19.1 Modelling a logistic function 19.2 Understanding odds ratios in the context of logistic regression", " Chapter 19 Logistic regression [Chapter currently under construction] In this chapter we'll look at our final regression technique, logistic regression. Unlike linear regression, which predicts a continuous outcome variable based on one or more predictor variables, logistic regression is used when the outcome variable is categorical. The most common form of logistic regression is binary logistic regression, where the outcome is limited to two categories such as Yes or No, Diagnosis or No Diagnosis, or 1 or 0. For instance, imagine you're interested in exploring factors that predict whether individuals are likely to have high blood pressure. While linear regression could help you predict something continuous like systolic blood pressure measurments, logistic regression would help you predict whether someone is likely to have a diagnosis of hypertention (high blood pressure) or not. Logistic regression uses a mathematical function that transforms linear input into a probability between 0 and 1. The logistic function has an \"S\" shape, allowing it to smoothly transition between the two extremes. 19.1 Modelling a logistic function The S-curve in logistic regression is essentially plotting the probability of an event occurring across the range of a predictor variable. In the example below I have plotted stress level (on a scale between 0 and 50) against the probability of an individual relapsing for a particular damaging behaviour (i.e, drug taking, smoking etc). The curve that we add to such a relationship works similarly to the line of best fit between the datapoints that we first used in the simple regression chapters. If we want to work out the chance of relaps for an individual that scores a 30 on the stress variable all we need to do is trace the point from 30 to the line and then read off the corresponding y-axis point. In this case the value is around 0.6 indicating that, according to out model, a scores of 30 on the scale indicates that an individual has a 85% estimated probability of experiencing a relapse. 19.2 Understanding odds ratios in the context of logistic regression In the previous section we talked about the probability of an event (probabilities being between 0 and 1) however we can also talk about the same data in terms of odds, particularly how the odds of an relaps event increase as a function of an increase in the stress variable. Odds ratios are calculated from the coefficients (\\(\\beta\\)) in the logistic regression model (JASP calculates them automatically if you tick the odds ration button so I wont go into the maths here). An odds ratio greater than 1 indicates that as the predictor variable increases, the odds of the outcome occurring also increase. Conversely, an odds ratio less than 1 indicates that as the predictor increases, the odds of the outcome occurring decrease. In the case of our current model our odds ratio is 1.29 indicating that for every unit increase in stress the chance of relaps increases by 29%. Possible data set: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0255683#sec019 [Chapter currently under construction] "],["further-reading.html", "Chapter 20 further reading", " Chapter 20 further reading Beyond multiple linear regression: https://bookdown.org/roback/bookdown-BeyondMLR/ Introduction to modern statistics: https://openintro-ims.netlify.app/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
