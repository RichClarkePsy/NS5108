[["index.html", "Regression handbook Chapter 1 Welcome to the NS5108 Regression Handbook 1.1 Where is the handbook for ANOVA?", " Regression handbook Dr Richard Clarke 2023-09-05 Chapter 1 Welcome to the NS5108 Regression Handbook If you are learning regression analysis for the first time or just want a refresher, this handbook is here to help you navigate through the complexity. Throughout this book you'll likely encounter complicated new words and concepts, but don't worry. To assist you, I've included a glossary as one of the last pages in this guide. If, after this, a concept still isn't clear, Ive included a list of recommended sources (some of which are even nice, easy to understand, YouTube videos) that will explain things from a different angle. This guide is designed to work alongside your NS5108 lectures, not replace them. Attending lectures will give you the chance to ask questions and get a better grasp of the material, making the learning process easier. Now, before we get started. Yes, there is going to be maths, and yes there are going to be formulas, sorry I cant teach statistics without these aspects. However, there are different depths to which you can learn statistics. This handbook is here to give you the basics, but it also allows you to delve a little deeper into the theory behind how regression modelling work. So if you don't understand absolutely everything on your first read through, that's ok and is in fact completely understandable! The exercises and data that I mention throughout this book can be found on the NS5108 Moodle page. If you're not enrolled in this module and want to try the exercises for yourself, you can email me at rclarke8@glos.ac.uk, and I'll send you the data. Click here if you have anxiety about statistics If you're feeling anxious about diving into statistics, know that you're not alone; many people feel the same way. Think of learning statistics like learning to drive or ride a bike. Initially, it's full of painful stops and bumps, but with persistence, things eventually start to make sense. Even I struggled greatly when I first encountered this material, but once I realised how incredibly useful and powerful it can be, my motivation skyrocketed, that got me over the hump and now Im hooked for life. Just remember that learning is a marathon, and it's perfectly fine to move at your own pace. Our goal is to make this journey as smooth as possible through clear explanations and practical exercises. If you find yourself stuck, don't hesitate to ask for help. Both myself and the graduate teaching assistants are more than willing to explain concepts repeatedly and in various ways; in fact, we welcome itit keeps us on our toes. Asking questions is an integral part of the learning process, and we're here to support you every step of the way. So, take a deep breath, and be patient and kind with yourself. The skills you'll acquire here will not only serve you well in your academic journey but hopefully lead you to having a meaningful impact in your future work. 1.1 Where is the handbook for ANOVA? This regression guide has been my own little summer project, for details related to ANOVA we already have an excellent guide written by Dr Kim Schenke in the Moodle area: Psychology Statistics Workbook JASP. This page contains a wide range of content related to research methods, JASP, and difference testing. "],["introduction-to-regression-analysis.html", "Chapter 2 Introduction to Regression analysis 2.1 Data-driven decision making 2.2 Further real world examples 2.3 Use of regression analysis in psychology 2.4 Overview of regression techniques covered in this module", " Chapter 2 Introduction to Regression analysis Regression analysis is a powerful statistical tool that investigates the relationships between variables. At its core, the technique is used to understand and quantify how one variable (the criterion variable) changes in response to another or several others (predictor variables). More than merely determining a singular correlation, regression analysis offers a nuance, enabling researchers to predict outcomes and uncover sophisticated patterns embedded within datasets. 2.1 Data-driven decision making The 2011 film Moneyball is based on the true story of the Oakland Athletics baseball team's 2002 season. Their general manager, Billy Beane (played by Brad Pitt in the film), faced a problem: he had a limited budget to put together a winning team. Rather than relying on traditional baseball scouting methods, which often depended heavily on scouts' intuitions and were prone to various biases, Beane employed the skills of a young Yale economics graduate named Peter Brand (played by Jonah Hill). Brand used statistical analysis to evaluate players' values. The traditional method of valuing players was subjective. Scouts often looked at the physique of players, their style, how they moved, or even things like the attractiveness of their girlfriends as an indicator of their confidence. Beane and Brand shifted the focus to objective evidence, including statistics. At its core, the analytics used in \"Moneyball\" is about predicting runs, and more importantly, wins. Using regression analysis, they could determine which statistics were most strongly correlated with creating runs. Once they had an understanding of what leads to runs, they could use that to build a model to aid them on player acquisition decisions. The film (and also the far more nerdy book by Michael Lewis) highlights the tensions that arise when data-driven metrics clash with entrenched traditional norms. Yet, as time progresses, the efficacy of these analytical techniques becomes increasingly evident. Today, it's a rarity to encounter a professional sports team that doesn't incorporate some form of statistical analysis into its strategy and decision-making processes, showcasing the undeniable impact and relevance of regression in our modern world. 2.2 Further real world examples Of course, its not just the world of sports that have taken advantage of regression analysis to inform decision making. Here are some examples from everyday life where regression analysis plays a role. Streaming Services and Recommendations: Most of you probably use platforms like Netflix, Spotify, or YouTube. These platforms utilise regression analysis to predict what shows, songs, or videos you might like based on your past behaviour and the behaviour of others with similar tastes. Predicting Box-office Income from Different Forms of Advertising: Film producers often use regression analyses to gauge which adverts, from TV spots to social media campaigns, most influence cinema ticket sales, optimising advertising budgets for upcoming films based on these insights. 2.3 Use of regression analysis in psychology Regression analysis is also a cornerstone of psychological research, as it allows psychologists to simultaneously explore and dissect the influence of numerous variables on a single outcome variable. Such a comprehensive approach is indispensable in a field like psychology, where behaviours and mental processes are often the outcome of a web of interconnected variables. For instance: Predictors of Job Satisfaction: Organizational psychologists could use regression to determine which factors (e.g., salary, working hours, team dynamics, or leadership style) are the most significant predictors of job satisfaction among employees. Influence on Social Behaviours: Social psychologists might employ regression analysis to understand how various factors like media exposure, peer influence, and past experiences predict certain social behaviours or attitudes, such as aggression or altruism. Personally I'm interested in public health and climate change. Regression is indispensable for studying areas like these: Determinants of Vaccine Hesitancy: I've used regression analysis to predict the likelihood of individuals being hesitant to take vaccines. Predictors for such a analysis being demographic factors like age and education level, psychological factors such as risk perception and trust in healthcare, as well as societal variables like exposure to misinformation on social media. Predictors of Pro-Environmental Behaviours: I am currently using regression model to predict pro-environmental behaviour through varying levels of capability, motivation, and opportunity (i.e. the COM-B model). Click for test yourself activity Reflect on a psychological phenomenon or behaviour that has piqued your interest or that you've recently studied. Jot down potential factors or variables that you believe might influence this phenomenon or behaviour. Consider how these factors might be integrated into a regression model. Which variable would you choose as the dependent variable (the variable you will be predicting)? What variables might serve as the predictors of the dependent variable? 2.4 Overview of regression techniques covered in this module In this handbook we will cover three main regression techniques: Simple linear regression (basically just a fancy correlation) Multiple linear regression (the technique that is required for your assignment) Binary logistic regression (multiple regression with a binary, instead of continuous, criterion variable) Continue with statistics in psychology and youll learn further forms of regression, and regression like, analysis techniques such as: Ordinal regression Mediation and moderation Multi-level modelling Structural equation modelling "],["simple-linear-regression.html", "Chapter 3 Simple linear regression 3.1 Recap of correlation 3.2 Simple linear regression 3.3 Week 1 - Test yourself mcqs", " Chapter 3 Simple linear regression 3.1 Recap of correlation At its core, correlation provides a measure of how data points on two variables are related to one another. Positive Correlation: (top left figure) As one variable increases, the other also increases. Negative Correlation: (top right figure) As one variable increases, the other decreases. No Correlation: (bottom figure) There is no discernible pattern in the relationship between the two variables. The strength and direction of the correlation are represented by the correlation coefficient, typically denoted as \\(r\\). The value of \\(r\\) ranges from -1 to +1. An \\(r\\) value closer to +1 indicates a stronger positive correlation. An \\(r\\) value closer to -1 indicates a stronger negative correlation. An \\(r\\) value closer to 0 suggests little to no correlation. 3.1.1 Correlation as a Statistical Test Going beyond visualisation of relationships, correlation can also be used as a formal statistical test to determine if there's a significant linear relationship between two variables. When conducting a correlation test, the p-value informs us about the significance of our observed correlation coefficient (\\(r\\)). If the p-value is below a predetermined threshold (commonly 0.05), we infer that the correlation in our sample is likely not due to random chance, and thus, there's a statistically significant relationship between the two variables. Reminder: what is a p-value Surface Level Explanation: A p-value is a number between 0 and 1 that tells us if the result of an experiment is likely due to chance or if there's something more going on. A small p-value (typically less than 0.05) suggests that the result is significant and not just a random occurrence. Intermediate Explanation: A p-value represents the probability of observing the data (or something more extreme) given that a specific null hypothesis is true. If we have a p-value less than a pre-decided threshold (like 0.05), we reject the null hypothesis in favour of the alternative hypothesis. This suggests that our observed data is unlikely under the assumption of the null hypothesis. However, a smaller p-value does not necessarily mean a result is \"meaningful\"; it just indicates it is statistically significant. In-Depth Explanation: Mathematically, the p-value is the probability of observing data as extreme as, or more extreme than, the observed data under the assumption that the null hypothesis is true. This is not a direct measure of the probability that either hypothesis is true. Instead, it's a measure of the extremity of the data relative to a specific model. Lower p-values suggest that the observed data are less likely under the null hypothesis, leading us to reject then null in favour of alternative hypothesis. However, it's crucial to understand that a p-value doesn't measure the size of an effect or the practical significance of a result. Furthermore, while a threshold of 0.05 is common, it's arbitrary and must be chosen with context and caution. Lastly, it's essential to remember the p-value is contingent on the correctness of the underlying statistical model and the level to which the data meet the statistical assumptions. Misunderstandings and misuse of p-values have led to various controversies in the scientific community (see the replication crisis). See Lakens (2022) Improving Your Statistical Inferences for a comprehensive overview of p-values. 3.1.2 Statistical assumptions of correlation In frequentist statistical analysis, we often aim to use parametric tests when the statistical assumptions underlying these tests are met, as these tests can be more sensitive to smaller effect sizes. For correlation analysis, Pearson's correlation is the most commonly used parametric test and Spearman's Rank correlation is the most commonly used non-parametric test. Assumption checks for correlation analysis In my experience of teaching regression, the assumption checks are often the part of statistics that confuse students the most. For now, I've put them safely away in this box, so as not to scare you off in the first week :-) We will do a fully refresh of the theory behind assumption check aspect next week when we cover multiple regression. For correlation analyse the the following assumptions should be checked and assessed: Linearity: Both variables should have a linear relationship, which means that when you plot them on a scatterplot, the distribution of data points should roughly form a straight line. Normality of Residuals: The data pairs should be normally distributed, meaning both variables being analysed should be approximately normally distributed when considered together. We do this by checking the normality of the residuals. Homoscedasticity: The spread (variance) of the residuals are constant across all values of the variable on the x-axis. i.e. data points shouldn't start of clustered and the widen out (like a funnel) further along in the correlation. Absence of Outliers: Outliers can disproportionately influence the correlation coefficient. So, it's important to check for and consider the impact of any outliers in the data. Outliers can be checked through the use of a boxplot or calculated manually. 3.2 Simple linear regression The content in the sections above, likely reflects the extent to which you explored correlation in your first year here with us. Regression analysis is simply an extension of this knowledge. Central to this understanding is the concept of the \"line of best fit\". While you might have previously used this line as a visual representation of the relationship between two variables, in linear regression, it takes on a more pivotal role. 3.2.1 The line of best fit In the scatter plot below, you'll see data points representing the hours of sleep and test performance for various participants. Notice how the data points are positively correlated? We can capture this trend by drawing a straight line through the data. This line, which we call the \"line of best fit,\" gives us a simplified representation of the relationship between hours of sleep and test performance. The optimal line of best fit is the one that minimises the total distance between itself and all the individual data points on the plot. The Concept of Residuals Even with the best possible straight line drawn through our data points, it's rare that the line will pass exactly through every point. The vertical distance between each data point and our line is called a \"residual\". For every data point, the residual is the difference between its actual value and what our line predicts the value should be. If our line of best fit is doing its job well, these residuals will be quite small, indicating that our predictions are close to the actual data. However, if the residuals are large, it suggests that our line might not be capturing the relationship between the two variables adequately. We talk more about residuals, in the context of our assumption checks, in the next chapter. The purpose of the line of best fit is to model the relationship between test performance and number of hours slept. What does this model suggest the performance on the test will be if a participant gets 7 hours sleep? 100110120130140 Hint Find the point for 7 hours on the x-axis. Draw your finger up to the blueline. Draw your finger across to the y-axis. Take that number. That is the test score that our model suggest for a participant that has 7 hours of sleep. 3.2.2 The equation of a line Every straight line on a scatter plot can be written as a simple formula: \\[ Y = mX + c \\] Where: \\(Y\\) is the dependent variable. \\(X\\) is the independent variable. \\(c\\) is where the line intersects the Y-axis, representing the predicted test performance when no hours of sleep are had. \\(m\\) is the slope of the line, indicating the predicted change in test performance for each additional hour of sleep. To make the formula for our previous scatter plot all we need is to work out how much the blue line \"goes up\" by how far it \"goes along\", and where the line crosses our y-axis. The below scatter plot is the same data but with each axes going to zero this time. This allows us to get our \\(c\\) value, 50, and by taking any two points on the line we can work out our \\(m\\) value, in this case 10 (60/6). This give us the following equation of our line: \\[ y = 10x + 50 \\] Look back at the question from before. All we need to do now is sub in the number 7 and we get our predicted test score: (10*7) + 50 = 120 3.2.3 Is this a good model? \"All models are wrong, but some are useful.\" - George E. P. Box This oft-cited quote sums up the a central truth in statistics and data modelling: no model can capture the full intricacy and unpredictability of real-world phenomena. However, that doesn't diminish the value of models. When a model simplifies complex systems and highlights important relationships, it can offer invaluable insights and guide decision-making. The line on our sleep and test score scatter plot (above) would appear to be quite representative of our data and therefore is perhaps fairly a good model (so long as our sample is representative of the population we want to use or model on in the future). However, if we try and use the relationship between participants height and their test scores as out model, this is likely to be a fairly poor model: The model derived from this data, \\(y = -0.05x + 77.89\\), would actually give an answer close to 70 no matter an individuals height. In other words, whether you're taller or shorter, the model essentially shrugs and predicts something close to 70 (the mean of the dataset). I would not describe this as a very useful model, as in, I would not spend an evening on the rack to try and make myself taller for the test. While we can see it on the scatter plot, none of the numbers in our equations allow us to gauge how 'useful' or fitting the sleep model is compared to the height model. To understand model fit we need the final new concept for this week, shared variance 3.2.4 Shared Variance When examining the relationship between two variables, the proportion of variation in one variable that can be predicted or explained by the other variable is known as the shared variance . In correlational research, this concept is crucial as it tells us how well one variable explains the variation in the other variable. In other word how useful would it be if we used a scores from one variable to predict scores on the other variable. A good way to think about this visually is with a Venn diagram. This value of shared variance is derived by taking the square of \\(r\\), the correlation coefficient, giving us our \\(R^2\\) value for the model. Take for instance, for our sleep model, if we run a pearsons correlation test on the data we find the relationship has a correlation coefficient of \\(r\\) = 0.94, squaring this give us a \\(R^2\\) of 0.89 between our predictor (study hours) and the outcome (test scores). Another way to talk about \\(R^2\\) is to say that 89% of the variance in test scores can be predicted or explained by the number of hours studied. The remaining 11% of the variance is due to factors not included in our model or random error. When we say two variables share a certain percentage of variance, it's a indication of the strength and utility of the relationship. However, while a high shared variance can be promising, it's essential to remember that correlation does not imply causation. Other underlying factors, third variables, or even coincidences could create correlations. In practical terms, understanding shared variance is critical for researchers. When a significant shared variance exists, the predictor variable becomes valuable in understanding, predicting, or even potentially influencing the outcome variable. However, the unexplained variance might also prompt researchers to consider additional predictors or factors that weren't initially in the model. Next week we'll start looking at multiple regression modelling, which as the name suggest involves multiple variables sharing variance with a dependent variable. 3.3 Week 1 - Test yourself mcqs 1. Which of the following best describes correlation? The probability of one event occurring given another has occurred. The relationship between the means of two groups. The strength and direction of a linear relationship between two variables. A statistical test to determine significance. 2. A negative correlation coefficient between two variables indicates: there's no relationship between the two variables. as one variable increases, the other decreases. both variables increase together. the data is scattered without any clear trend. 3. Which statistical method is used to predict the value of one variable based on the value of another variable? T-test. ANOVA. Simple linear regression. Chi-square test. 4. In the context of a correlation test, the p-value informs us: about the strength of the correlation. about the slope of the line of best fit. whether the observed correlation is likely due to random chance. about the effect size of the relationship. 5. What does the \"line of best fit\" represent in the scatter plot? The line that maximizes the total distance between itself and the data points. The line that minimizes the total distance between itself and the data points. The line that represents the mean of the data points. The line that represents the mode of the data points. 6. In the formula \\(Y=mX+c\\), what does \\(c\\) represent? The slope of the line. The predicted change in test performance for each additional hour of sleep. Where the line intersects the Y-axis. The independent variable. 7. What is the shared variance in a simple linear regression? The proportion of variation in one variable that can be predicted by another variable. The difference in variance between two variables. The total variance of both variables. The square root of the variance of both variables. 8. How is the shared variance value derived? By taking the cube of the correlation coefficient. By taking the square root of the correlation coefficient. By dividing the correlation coefficient by 2. By taking the square of the correlation coefficient. "],["jasp-workshop---simple-linear-regression.html", "Chapter 4 JASP Workshop - Simple linear Regression 4.1 Simple linear regression - example analysis 4.2 \"Where to click\" guide - simple linear regression analysis 4.3 JASP lab exercises 4.4 APA style guide", " Chapter 4 JASP Workshop - Simple linear Regression JASP is a free, open-source statistical software package with a user-friendly, point-and-click interface suitable for research in psychology. It offers a wide range of statistical analyses, from basic descriptive statistics to advanced methods like regression and ANOVA. JASP is available on most of the university computers, however, we recommend that you also install a personal version for your own laptop or desktop computer so that you can continue learning outside of class time. JASP can be downloaded here: www.jasp-stats.org/ 4.1 Simple linear regression - example analysis In this video I go through the analysis and answers for Simple Linear Regression Exercise 1. If you would like to follow along (or have a go at the exercise yourself first) the question sheet and data can be found in the Week 1 module area for NS5108. 4.2 \"Where to click\" guide - simple linear regression analysis The following is a step by step guide for performing a correlation and simple linear regression analysis in JASP. Watch the video above for more context related to these steps. 4.2.1 Correlation Open JASP. Load the Data: click on the File tab at the top left and select Open. Then navigate to the folder containing your data file and open it. Visualise the data: Identify the two variables you want to run your correlation on. Click Descriptives and move the two variables over to the Variables box. Below, in the Customizable plots drop down tick the Scatter plots. You may want to click none for the graphs above and to the right (to make your plot clearer). Also change that regression line (line of best fit) to linear. Identify outliers: Also in the customizable plots drop down, select boxplots. Tick label outliers if they show any dots outside of the whiskers for the box plots. Note and interpret any outliers. Calculate correlation coefficient: click Regression -&gt; Correlation from the top bar. Pick the variables you are interested in running a correlation on. Extract and interpret the Pearson's \\(r\\) and p-value. 4.2.2 Simple linear regression Check normality (of residuals): Click Regression -&gt; Linear Regression from the top bar. Pick your criterion variable (the variable you are aiming to predict) and place this in the Dependent variable box. Pick your predictor and place that in the Covariates box. In the Plots drop down select Residuals histogram. Visually interpret this histogram for an indication of this assumption check. Check Homoscedasticity: Also in the Plots drop down, select Residuals vs. predicted. Visually interpret this plot to assess Homoscedasticity. Extract the \\(R^2\\) value: This should be in the first table of the Linear Regression section. Determine the significance of the model: This can be obtained though the ANOVA table. F(df,df)=F-value, p=p-value. Extract values for your model regression equation: H1 intercept = \\(c\\), H1 variable = \\(m\\) 4.3 JASP lab exercises There are two additional exercises on Moodle. Each exercise question sheet comes with a dataset and answers sheet. Work through the question and then check your answers with the answer sheet. 4.4 APA style guide Here is an example of a hypothesis for a correlational analysis: Null Hypothesis (H0): There is no correlation between participants' age and their scores on a cognitive ability test. Alternative Hypothesis (H1): There is a correlation between participants' age and their scores on a cognitive ability test. Here is an example of a correlation analysis reported in APA style: A Pearson correlation was conducted to assess the relationship between participants' age and their scores on a cognitive ability test. Assumptions checks were performed to ensure no violation of the assumptions of normality, linearity, and homoscedasticity. There was a significant negative correlation between age and cognitive ability scores, r(98) = -.45, p &lt; .001, with older participants tending to have lower scores on the cognitive ability test. As such, the null hypothesis can be rejected. Here is an example of a hypothesis for a simple linear regression analysis: Null Hypothesis (H0): The number of study hours is not a significant predictor of test scores. Alternative Hypothesis (H1): The number of study hours is a significant predictor of test scores. Here is an example of a simple linear regression analysis reported in APA style: A simple linear regression was conducted to predict test scores based on the number of study hours. The assumptions of linearity, independence, and normality were checked and met. The regression equation was found to be: Test Score = 50.2 + 6.7 * Study Hours. The results indicated that there was a significant relationship between the number of study hours and test scores, F(1, 98) = 34.5, p &lt; .001. As such, the null hypothesis can be rejected The R² value was .26, indicating that approximately 26% of the variance in test scores can be explained by the number of study hours. For each additional hour of study, there was an increase of 6.7 points in the test score. The intercept value of 50.2 indicates that a student who does not study at all (0 hours) is expected to score 50.2 points on the test. "],["multiple-regression.html", "Chapter 5 Multiple regression 5.1 Shared variance with multiple variables 5.2 The Regression Equation 5.3 Interpreting a regression model 5.4 Building our regression equation from the model 5.5 So what? 5.6 Week 2 - Test yourself mcqs", " Chapter 5 Multiple regression Multiple regression is a method that helps us predict an outcome variable based on several independent variables. It is a technique that tells us which variables are the most important for predicting an outcome, and how such predictor variables interact. By building a prediction model, multiple regression allows us to understand how variables are connected and helps us make powerful data-informed decisions. 5.1 Shared variance with multiple variables In the previous chapter on simple linear regression, we suggested that one predictor variable (exercise) shared variance with another variable (well-being). Visually, this can be represented using a Venn diagram with two circles: one for exercise and one for well-being. The overlap between these two circles represents the shared variance, or the extent to which variations in exercise can explain variations in well-being. This shared variance can be represented numerically as the square of the correlation coefficient between the two variables. In this hypothetical example exercise is correlated with well-being with a correlation coefficient of r=0.307 (i.e. a moderate-positive correlation) and, as such, it can be said that exercise explains 9.4% of the variance in well-being (\\(R^2\\)=0.094) In a multiple regression analysis, we use two or more independent variables to predict the value of a dependent variable. In doing so, each independent variable may share some variance with the dependent variable, but also shares variance among the other independent variables. Understanding this shared variance is crucial for interpreting the results of a multiple regression analysis. For instance, let's say we're using three independent variables to predict well-being: exercise, age, and income. Each of these variables could contribute to the variance of well-being. When we add the variables one-by-one, the overlap of each independent variable circle with the well-being circle represents the unique shared variance each variable contributes to well-being. When adding our samples age data, to help us explain well-being, we see significant correlations for the three different relationships. 1. a positive correlation exists between exercise and well-being, 2. a negative correlations exist between age and well-being 3. a negative correlation between age and exercise. Why are the \\(R^2\\) positive for negative correlations? Because the \\(R^2\\) is derived from squaring an r value, and when you square any negative value, it loses its negative sign. Both age and exercise now explain some of the variability in well-being, and since they are also related to each other, some of the variance they explain overlaps. Finally, when we add income, we find that the variable shares variance with both age and well-being. Income therefore adds more variance explained to our model while also overlapping with some of the variance explained by age The outcome of this is that each variable that we add here allowed us to better explain our concept of well-being. And as such, if we were to know a person's amount of exercise, age, and income we could predict their well-being to a greater degree than if we just know their amount of exercise. Extra detail on overlapping variance It's important to understand that the Venn diagrams in this guide offer a simplified representation of variance explained by predictor variables in a regression model. In reality, when two predictor variables have overlapping variance, this shared variance can contribute more to the model than what the individual variables might suggest. This occurs due to interaction effects, where the combined influence of two predictors can explain more variance in the dependent variable than the sum of their individual effects. When looking at the Venn diagrams, remember that this additional explained variance from interactions isn't explicitly depicted, but it's crucial to consider when interpreting regression results. This is why, as you'll see later on, the final \\(R^2\\) is greater than the sum of all the simple (univariate) regressions. 5.2 The Regression Equation As you've probably realised, using Venn diagrams to illustrate the relationships among multiple variables can get quite confusing, especially when there are numerous variables that all have some degree of correlation with each other. That's where the regression equation comes in handy. The regression equation is a mathematical representation of the relationships among the variables in a multiple regression analysis. It shows how the dependent variable is predicted by the independent variables. In its abstract form, the regression equation can be written as: \\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_kx_k + \\varepsilon \\] Here, \\(y\\) is the dependent variable, \\(x_1\\), \\(x_2\\),...,\\(x_k\\) are the independent variables (i.e. the amount of exercise, age, and income of the participants), \\(\\beta_0\\) is the intercept, \\(\\beta_1\\), \\(\\beta_2\\),...,\\(\\beta_k\\) are the coefficients for the independent variables, and \\(\\varepsilon\\) is the error term, representing the unexplained variability in the dependent variable (i.e. the left over white space in the Venn diagram figure above). The intercept \\(\\beta_0\\) represents the value of the dependent variable when all independent variables are zero. This \\(\\beta_0\\) is equivalent to the intercept, \\(c\\), value in the equation from last week. The coefficients (all the remaining \\(\\beta\\) values) represent the average change in the dependent variable for a one-unit change in the respective independent variable, holding all other variables constant. Just like the gradient of the line \\(m\\) value in the equation from last week. The \"...\" and the \\(k\\)'s are just a way of saying \"add in as many beta values as you have variables in your model\". Running our regression analysis allowed us to substituent numbers for all of the coefficients to the degree that all we are left with are \\(x\\)'s, that we can use to sub in our data. 5.3 Interpreting a regression model I will explain the steps for running a regression model, in full, in this weeks JASP workshop. For now, I want us just to focus on the interpretation. Here is the JASP output for a model that uses the variables exercise, age and income to predict well-being scores. Let's break this down by table. 5.3.1 Descriptive statistics The descriptive statistics are useful for context. This detail isn't given here but I can tell you that in this dataset (N=1000), the well-being score is out of 100, the exercise variable is based off of number of hours of exercise a week, income is in £GBP, and age is in years. 5.3.2 Model Summary The Model Summary table provides an overview of the overall performance of the regression model. It contains key statistics that helps us assess how well the model fits the data. The table contains two rows \\(H_0\\) and \\(H_1\\). \\(H_0\\) refers to the null model, a model that assumes that none of the predictors in the model have an effect on the dependent variable, meaning that the coefficients for all predictors are equal to zero. \\(H_1\\) is our alternative model, the model containing our variables. This is the line that interests us. The main details of interest are: The Adjusted \\(R^2\\) Value: This is a modified version of \\(R^2\\) that takes into account the number of independent variables in the model. It allows us to say how much variance in our dependent variable is explained by our predictor variables. RMSE: RMSE stands for Root Mean Square Error. It is a measure of the differences between values predicted by a model and the values actually observed. RMSE is a measure of how spread out the residuals are. In other words, it tells you how concentrated the data is around the line of best fit. It is used as our \\(\\varepsilon\\) in the regression model. We will often take the Adjusted \\(R^2\\) for multiple regression and the \\(R^2\\) only for simple regression (regression with just one predictor variable). For our well-being model, the Adjusted \\(R^2\\) of 0.469 tells us that our three variables together explain 46.9% of the variability in well-being. Which is pretty good. 5.3.3 ANOVA The ANOVA table provides information about the overall significance of the model. It shows whether the model is better at predicting the dependent variable than a model with no independent variables (i.e., the null model). The findings from this table will need to be reported in our write-up, however, to interpret it, all we actually need is the p-value. A significant finding on this test tells us that our model is significantly better than the null model. What is an ANOVA ANOVA stands for ANalysis Of VAriance. We'll come back to ANOVA in week 4. For now, just think of it as a slightly fancier t-test, a test that compares across groups (rather than correlates). In this case it is comparing the null model to the alternative model and seeing if they are significantly different. 5.3.4 Coefficients Finally, to understand the role of each variable in the model we need to look at the coefficients table. This table provides detailed information about the individual variables in the model. It includes statistics for each independent variable and the intercept for the model. There are two columns that are key to our understanding of the model. The Unstandardized column and the p column. Starting with the p column, this is the probability of obtaining a t-value as extreme as the one observed if the null hypothesis is true. A small p-value indicates that the variable plays a statistically significant role in our model. It could be the case that we include a variable that is not related to our dependent variable or the variance it explains is accounted for by other variables in the model. In those cases, the p-value would be above 0.05. A p-value of &lt;.001 for all three of our variables indicates that they are all relevant to include in our model. The values in our Unstandardized column correspond to the \\(\\beta\\) values in our regression equation. They play the same role as the gradient of the line of best fit from simple regression last week. They represent the absolute change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant. In our model, this table tells us that: For every 1 hour increase of exercise a week, well-being increases by 1.88 points. For every 1 year of ageing, well-being decreases by 1.09 points. For every £1 of income, well-being increased by 0.001 points. That last finding is not as intuitive as it could be, so it makes sense to convert income into values of 1000's of pounds (i.e. if someone earns £32500 a year they can be said to earn 32.5 thousand pounds a year). To do this all I need to do is divide each of our income values by 1000 and run the model again. Doing so gives us this: For every £1000 of income, well-being increases by 1.35 points. 5.4 Building our regression equation from the model From this we can now start building out our regression equation. We have three variables predicting well-being so all we need to do is fill in the \\(\\beta\\) values and the \\(\\varepsilon\\) in the following equation. \\[ Wellbeing = \\beta_0 + \\beta_1income + \\beta_2age + \\beta_3exercise + \\varepsilon \\] \\(\\beta_0\\) = The unstandardized (beta) value for the intercept of \\(H_1\\) = 35.25 \\(\\beta_1\\) - \\(\\beta_3\\) = the unstandardized (beta) value of the three variables = 1.35, -1.09 and 1.88 \\(\\varepsilon\\) = the Root Mean Square Error (RMSE) of \\(H_1\\) = 15.7 This give us the following regression equation: \\[ Wellbeing = 35.25 + 1.35income -1.09age + 1.88exercise + 15.7\\] From this equation, if we knew an individual's income, age and how much exercise they engaged in per week we could estimate their well-being with a fairly high degree of accuracy. Alongside the equation it is important to note our Adjusted \\(R^2\\) of 0.469, and indicate that this means that over half of the variability in well-being is not explained by the variables we have included in the model. 5.5 So what? What is the point of doing all this work? Well, if I'm wanting to increase the well-being of my local community this finding might put me off putting money into community exercise classes. While there all likely other benefits of such an intervention, there are only so many hours in a week and, based on this data, a 1.88 point increase in well-being (on a scale out of 100) for an extra hour of exercise might not be the most cost-effective way of spending my limited pool of money. Then again, it's not like we can stop someone from ageing, or increase their income. Perhaps further research that compares the effects of, for example, exercise, mindfulness and diet, while controlling for demographic factors, would be useful to inform what type of intervention might be more cost-effective. 5.6 Week 2 - Test yourself mcqs What is multiple regression? Using a single categorical variable to predict the value of a dependent variable. A method that helps predict an outcome variable based on one independent variable. A method that helps predict an outcome variable based on several independent variables. A measure of the association between two variables. What is the regression equation? A graphical representation of the relationships among the variables. A mathematical calculation of the shared variance. A mathematical representation of the relationships among the variables in a multiple regression analysis. An empirical observation of the relationships among the variables. In the regression equation, what do the  values represent? The dependent variable. The coefficients for the independent variables. The error term. The independent variables. What do the values in the Unstandardized column in the coefficients table represent? the shared variance between the variables. the average change in the dependent variable for a one-unit change in the respective independent variable, holding all other variables constant. the correlation coefficient between the variables. the absolute value of the correlation coefficient between the variables. What does the Adjusted \\(R^2\\) value tell us in a multiple regression analysis? The strength of the linear relationship between the dependent variable and each independent variable. The shared variance among the independent variables. The percentage of variability in the dependent variable that is explained by the predictor variables. The effect size of the independent variables on the dependent variable. "],["statistical-assumption-checks-for-multiple-regression.html", "Chapter 6 Statistical assumption checks for multiple regression 6.1 Check for Linearity 6.2 Check for Multicollinearity 6.3 Check for Outliers 6.4 Checks of residuals", " Chapter 6 Statistical assumption checks for multiple regression At this point in your studies, it's important for you to understand how to assess and report the statistical assumptions underlying regression analysis. However, it's not until more advanced stages in your career that you'll learn how to address such violations of assumptions. For now, just interpret and report these assumptions. If the results deviate greatly from the assumptions, then it's important to report such deviations and state that findings \"should be taken with caution\". Here are the key assumptions in multiple regression and how to check for them: 6.1 Check for Linearity The assumption of linearity states that the relationship between the independent and dependent variables is linear. You can check this visually by examining scatter plots of the independent variables against the dependent variable. 6.2 Check for Multicollinearity Multicollinearity occurs when two or more independent variables in a regression model are closely related, making it difficult to disentangle their individual effects on the dependent variable. This can lead to unstable coefficient estimates, as small changes in the data can result in significant changes in the estimates. You can assess multicollinearity by creating a correlation matrix of the independent variables and looking for variables with a correlation coefficient greater than 0.7 or less than -0.7. However, this is not a strict rule, just an indication of a strong correlation. In the table above, suppose we use these five personality constructs to predict a behaviour, such as recidivism. Checking the correlation coefficients, the largest value is -0.368, indicating that multicollinearity is not likely an issue in this case. Be mindful not to confuse correlation coefficients with p-values, as it's an easy mistake to make. While multicollinearity can make interpretation challenging, it's not necessarily a deal-breaker. In some cases, the focus may be on predicting the outcome rather than interpreting the individual contributions of each predictor. For a more in-depth discussion on multicollinearity and its implications see Vatcheva et al (2016). 6.3 Check for Outliers Outliers are data points that differ significantly from other observations. They can be detected through various methods. In a boxplot, outliers are typically displayed as individual points outside the whiskers, which extend from the first and third quartiles (Q1 and Q3) to Q1 - 1.5 * IQR (interquartile range) and Q3 + 1.5 * IQR, respectively (see this link for a full explanation of how read a boxplot). JASP can automatically identify outliers in boxplots, making it easier to spot them. However, addressing outliers is often a contentious issue because their treatment can have a significant impact on the results of a statistical analysis. The decision to keep or remove outliers depends on multiple factors. First, it's important to consider the context of your data. Outliers could be the result of measurement errors, data entry errors, or other random anomalies. However, they could also represent genuine extreme values that offer valuable insights. For example, in a study on wealth distribution, billionaires would be outliers but they are a real-world phenomenon that might be important to the research. Secondly, outliers can heavily influence other data assumptions, such as linearity, normality of residuals, and homoscedasticity. It is crucial to evaluate whether outliers are affecting these assumptions and to justify their removal or retention. This, in fact, can be away of addressing the violations of assumptions listed in this chapter. Thirdly, outliers can disproportionately affect key statistical measures like the mean and standard deviation. It's very easy to fall into the trap of removing outliers until a significant results appears. This kind of practice may fall into that grey area of academic misconduct that we discussed in first year, known as Questionable Research Practices. Ultimately, the decision to handle outliers involves a balance of statistical judgment and domain expertise. It's necessary to ensure that any decision involves transparently reporting and justifying your decisions. 6.4 Checks of residuals The last two assumption checks require an examination of the residual. Residuals are a fundamental concept in regression analysis, representing the difference between observed and predicted values of the dependent variable. In a simple regression, we visualised the predicted value with our line of best fit, and the observed value are just the data point in our relationship. For each observation, a residual is calculated by subtracting the predicted value from the observed value. In simple linear regression, where there is one independent variable, the residual for each observation is the distance from the data point to the regression line. In multiple regression, it is the distance from the observation to the regression plane. The goal of regression analysis is to minimize the residuals, or in other words, to make the predicted values as close as possible to the observed values. Residuals can be used to evaluate the fit of a regression model, to detect outliers, and to check for any violations of the assumptions of regression. 6.4.1 Check for Normality of Residuals The assumption of normality of residuals is important for hypothesis testing in linear regression. It ensures that the standard errors of the coefficients are unbiased and that the significant testing for the coefficients are valid. This, in turn, helps us make valid inferences about the relationships between the variables. In contrast, checking for normality of the individual variables is not required for linear regression. Instead, the focus is on the distribution of the residuals, which should be normally distributed if the model's assumptions are met. The residuals represent the differences between the observed and predicted values of the dependent variable, and their distribution tells us about the model's fit and the relationships between the variables. If you're working on a program where you can calculate the residuals, then you can check the normality of the residuals using the Shaperio Wilks test or by assessing the skewness and kurtosis. In JASP the only means of checking the normality of residuals is through the normality of residuals plot option. 6.4.2 Check for Homoscedasticity Homoscedasticity, the word that strikes fear into undergrad (and most postgrad) psychology students throughout the land, is not as complicated as it sounds. \"Homos\": Greek for \"same\" or \"equal.\" In the context of homoscedasticity, it refers to the equality or sameness of the variances. \"Skedasis\": Greek word related to the concept of dispersion or spreading out. In statistics, it is associated with the variance or scatter of data points. The scatter plots below demonstrate how such data might look. Heteroscedasticity (i.e. different \"spreadness\") means that the line of best fit becomes less reliable at different points in the model. In this context, what we're looking for an equal distribution of data points along the line of best fit, avoiding a funnel-shaped pattern. When dealing with regression analyses that involve more than two predictor variables, it's often not feasible to visualise the data. In such cases, a plot of predicted variables against residuals can be used to check for homoscedasticity. In this plot, we are, again, looking for the absence of a funnel shape in the points. This check can also be performed using z-scores of the predicted values against the residuals; a level line in this context would indicate homoscedasticity (a uniform spread of the residuals). This method is used in software like SPSS for checking homoscedasticity, so you may see some other guides talking about this if you read wider on the topic. "],["jasp-workshop---multiple-regression.html", "Chapter 7 JASP Workshop - Multiple regression 7.1 Multiple regression - Example analysis 7.2 \"Where to click\" guide - Multiple Regression 7.3 JASP lab exercises 7.4 APA Style Guide for Multiple Regression", " Chapter 7 JASP Workshop - Multiple regression JASP is a free, open-source statistical software package with a user-friendly, point-and-click interface suitable for research in psychology. It offers a wide range of statistical analyses, from basic descriptive statistics to advanced methods like regression and ANOVA. JASP is available on most of the university computers, however, we recommend that you also install a personal version for your own laptop or desktop computer so that you can continue learning outside of class time. JASP can be downloaded here: www.jasp-stats.org/ 7.1 Multiple regression - Example analysis In this video I go through the analysis and answers for Multiple Regression Exercise 1. If you would like to follow along (or have a go at the exercise yourself first) the question sheet and data can be found in the Week 2 module area for NS5108. 7.2 \"Where to click\" guide - Multiple Regression Open JASP. Load the Data: click on the File tab at the top left and select Open. Then navigate to the folder containing your data file and open it. Identify which of your variable will be your predictors and which will be the dependent variable (the variable you are aiming to predict). Check linear relationships between predictor variables: Click Regression -&gt; Correlation from the top bar. Move your predictor variables and your dependent variable over to the Variables Box. Click the Scatter plots options. Visually interpret the Correlation plots to determine if all the predictor variables have a linear relationship to the dependent variable. Check for multicollinearity: Within the same results section from step 3 check the correlation table. Interpret if the correlation coefficients between your predictor variables is large enough to indicate an issue with multicollinearity. Check normality (of residuals): Click Regression -&gt; Linear Regression from the top bar. Pick your dependent variable and place this in the Dependent variable box. Pick your predictor variables and place them in the Covariates box. In the Plots drop down select Residuals histogram. Visually interpret this histogram for an indication of this assumption check. Check Homoscedasticity: Also in the Plots drop down, select Residuals vs. predicted. Visually interpret this plot to assess Homoscedasticity. Extract the Adjusted \\(R^2\\) value: This should be in the first table of the Linear Regression section. Determine the significance of the model: This can be obtained though the ANOVA table. F(df,df)=F-value, p=p-value. Extract values for your model regression equation: This will be the unstandardized values in the coefficients table. See below for how to report your findings. 7.3 JASP lab exercises There are two additional exercises on Moodle. Each exercise question sheet comes with a dataset and answers sheet. Work through the question and then check your answers with the answer sheet. 7.4 APA Style Guide for Multiple Regression 7.4.1 Hypothesis for Multiple Regression Analysis Null Hypothesis (H0): Neither self-esteem nor study hours are significant predictors of academic performance. Alternative Hypothesis (H1): At least one of self-esteem or study hours is a significant predictor of academic performance. 7.4.2 Reporting Multiple Regression in APA Style A multiple linear regression was conducted to explore the impact of self-esteem and study hours on academic performance. Assumptions of linearity, independence, multicollinearity, and normality were checked and met. The multiple regression model significantly predicted academic performance, F(2, 97) = 36.8, p &lt; .001, \\(R^2\\) = .43. The regression equation was found to be: Academic Performance = 45.2 + 4.1 * Self-Esteem + 5.2 * Study Hours. Both self-esteem and study hours significantly added to the prediction of academic performance. Specifically, self-esteem was a significant predictor, t(97) = 2.9, p = .005, and contributed to an increase of 4.1 points for every unit increase in self-esteem. Study hours were also a significant predictor, t(97) = 5.1, p &lt; .001, and contributed to an increase of 5.2 points for every additional hour spent studying. Overall, the model explained approximately 43% of the variance in academic performance (R² = .43). As such, the null hypothesis can be rejected, indicating that at least one of the predictors significantly impacts academic performance. "],["logistic-regression.html", "Chapter 8 Logistic regression 8.1 Modelling a logistic function 8.2 Understanding odds ratios in the context of logistic regression", " Chapter 8 Logistic regression [Chapter currently under construction] In this chapter we'll look at our final regression technique, logistic regression. Unlike linear regression, which predicts a continuous outcome variable based on one or more predictor variables, logistic regression is used when the outcome variable is categorical. The most common form of logistic regression is binary logistic regression, where the outcome is limited to two categories such as Yes or No, Diagnosis or No Diagnosis, or 1 or 0. For instance, imagine you're interested in exploring factors that predict whether individuals are likely to have high blood pressure. While linear regression could help you predict something continuous like systolic blood pressure measurments, logistic regression would help you predict whether someone is likely to have a diagnosis of hypertention (high blood pressure) or not. Logistic regression uses a mathematical function that transforms linear input into a probability between 0 and 1. The logistic function has an \"S\" shape, allowing it to smoothly transition between the two extremes. 8.1 Modelling a logistic function The S-curve in logistic regression is essentially plotting the probability of an event occurring across the range of a predictor variable. In the example below I have plotted stress level (on a scale between 0 and 50) against the probability of an individual relapsing for a particular damaging behaviour (i.e, drug taking, smoking etc). The curve that we add to such a relationship works similarly to the line of best fit between the datapoints that we first used in the simple regression chapters. If we want to work out the chance of relaps for an individual that scores a 30 on the stress variable all we need to do is trace the point from 30 to the line and then read off the corresponding y-axis point. In this case the value is around 0.6 indicating that, according to out model, a scores of 30 on the scale indicates that an individual has a 85% estimated probability of experiencing a relapse. 8.2 Understanding odds ratios in the context of logistic regression In the previous section we talked about the probability of an event (probabilities being between 0 and 1) however we can also talk about the same data in terms of odds, particularly how the odds of an relaps event increase as a function of an increase in the stress variable. Odds ratios are calculated from the coefficients (\\(\\beta\\)) in the logistic regression model (JASP calculates them automatically if you tick the odds ration button so I wont go into the maths here). An odds ratio greater than 1 indicates that as the predictor variable increases, the odds of the outcome occurring also increase. Conversely, an odds ratio less than 1 indicates that as the predictor increases, the odds of the outcome occurring decrease. In the case of our current model our odds ratio is 1.29 indicating that for every unit increase in stress the chance of relaps increases by 29%. Possible data set: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0255683#sec019 [Chapter currently under construction] "],["further-reading.html", "Chapter 9 further reading", " Chapter 9 further reading Beyond multiple linear regression: https://bookdown.org/roback/bookdown-BeyondMLR/ Introduction to modern statistics: https://openintro-ims.netlify.app/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
